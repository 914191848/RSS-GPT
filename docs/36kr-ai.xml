<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>36氪 - 人工智能·AI</title>
<link>https://www.36kr.com/motif/327686782977</link>

<item>
<title>《时代》人工智能百人榜（四）：思想家</title>
<link>https://www.36kr.com/p/2469707773106305</link>
<guid>https://www.36kr.com/p/2469707773106305</guid>
<content:encoded><![CDATA[

<blockquote><p>神译局是36氪旗下编译团队，关注科技、商业、职场、生活等领域，重点介绍国外的新技术、新观点、新风向。</p></blockquote><p>编者按：人工智能的独特之处既最令人恐惧也最值得庆祝——一些技能堪比我们人类，然后还能更进一步，做到人类做不到的事情。模仿人类行为已成为人工智能的决定性特征。然而，机器学习和大语言模型的每一次进步背后实际上都是人——这里面既有经常被忽视的，让大语言模型使用起来更安全的人类劳动，又有就在什么时候以及如何最好地使用这项技术方面做出关键决定的个人。本文综合了各方推荐和建议，将数百项提名汇总到一起，最终形成了这份百人榜名单。从很多方面来说，这 100 人构成了推动人工智能发展的关系网络与权力中心。他们是竞争对手、监管者、科学家、艺术家、倡导者、以及高管——属于既互相竞争又共同合作的人类，他们的洞察力、欲望与缺陷将塑造一项影响力与日俱增的技术的发展方向。文章来自编译，篇幅关系，我们分四部分刊出，此为第四部分。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231011/v2_f7807969d9cf4f9c9cce4e98aeec1978@1694_oswg1262095oswg2000oswg1125_img_jpg?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><h2>思想家</h2><h3>杰弗里·辛顿：多伦多大学名誉教授</h3><p>今年2月，杰弗里·辛顿 (Geoffrey Hinton)，过去 50 年来最有影响力的人工智能研究人员之一，经历了一次“缓慢的顿悟”。</p><p>76 岁的辛顿在 2013 年加入谷歌之前，一直致力于主要在学术圈建立模拟人脑的人工智能系统。他一直都认为，人脑比他和其他人正在开发的机器都要好，通过自己做的东西变得更像大脑，它们就会得到改善。但在今年2月，他意识到“我们现在拥有的数字智能可能已经好过大脑了。只不过规模还没那么大。”</p><p>世界各地的开发者目前正竞相开发最大型的人工智能系统。考虑到目前人工智能公司扩大模型规模的速度，人工智能系统实现100 万亿连接（约与人脑神经元之间的连接数量相同）可能只需要不到五年的时间。</p><p>对此感到担忧的辛顿今年 5 月辞去了谷歌副总裁兼工程研究员的职务，并接受了一系列采访。他在采访中解释说，自己离开是为了可以自由地讨论人工智能的危险，他还谈到了对帮助将该技术引入人工智能领域感到遗憾。他担心一旦人工智能系统的规模扩大到人类大脑的那样会发生的事情，也对人类可能被他帮助创造的技术消灭的前景感到忧心。辛顿说： “这些东西会变得比我们更加聪明并接管一切。如果你想知道那是什么感觉，去问问小鸡吧。”</p><p>在英国出生和长大的辛顿系出名门，其亲戚包括数学家玛丽·埃弗里斯·布尔（Mary Everest Boole）以及逻辑学家乔治·布尔（George Boole），这两人的工作对现代计算机科学至关重要；还有外科医生詹姆斯·辛顿（James Hinton）；以及测量员乔治·埃佛勒斯（George Everest，珠穆朗玛峰的英文名用的就是他的名字。）</p><p>人类大脑一直让辛顿感到着迷。作为一名剑桥大学本科生，他试过一系列的学科——生理学、物理学、哲学——然后在 1970 年获得了实验心理学学位。在开始攻读博士学位之前，他曾短暂担任过一阵木匠。 1972 年，他在爱丁堡大学获得了人工智能博士学位，这是当时英国人工智能领域唯一的研究生项目。</p><p>20 世纪 70 年代，在未能兑现其战后承诺后，人工智能经历了一段热度退却时期，也就是现在所谓的 “人工智能寒冬”。在这个当时不流行的领域，辛顿开始追逐一个不受欢迎的想法：一中莫发放人脑结构，叫做神经网络的人工智能系统。他的论文导师每周都敦促他改变主意。每次他都会回答：“再给我六个月，我会证明它是有效的。”</p><p>修完博士学位后，辛顿移居到美国，他的研究也获得了更多资金。他在美国各地的大学任教，发表了开创性的研究成果，并因此获得了 2018 年图灵奖，最后在多伦多大学获得了计算机科学教授职位。多伦多现在已成为辛顿的大本营；他出差的频率相对较低，因为背部问题导致他无法坐下。在开车去旅行时，他会躺在后座上；吃饭时他需要跪在桌子前，“就像圣坛上的和尚一样”。</p><p>2012 年，辛顿和他的两名研究生亚历克斯·克里泽夫斯基（Alex Krizhevsky） 以及伊尔亚·苏茨克维（Ilya Sutskever，现任 OpenAI 首席科学家）去参加 ImageNet，这是一项看谁开发的图像识别人工智能系统最准确的年度竞赛，他们统治了这项竞赛——这有力地证明了神经网络已经成熟。辛顿的坚持得到了回报。</p><p>他和他的两个学生开始收到科技巨头的丰厚offer。他们成立了一家叫做 DNN-research 的空壳公司来拍卖自己的专业知识，有四家科技公司，包括谷歌、微软、百度以及 DeepMind，报价数千万美元要收购这家公司。一周后，辛顿选择了谷歌而不是最后的投标人百度。 2013 年，他加入了 Google Brain，今年 5 月，他离开了这支顶尖的机器学习团队。</p><p>辛顿对神经网络的开发和普及发挥了重要作用。神经网络是目前占主导地位的人工智能开发范式，可以通过摄取和处理大量数据，从而推动图像识别、语言理解和自动驾驶汽车的进步。他的工作有可能加速他所担心的未来的到来，也就是人工智能变成超人，给人类带来灾难性的后果。辛顿表示，“我找借口安慰自己：就算我不做，别人也会做。”</p><p>辛顿不知道该怎么防止超人的人工智能系统接管世界。他说，如果还有什么希望的话，只能把希望寄托在下一代身上，并指出他觉得自己已经太老了，无没法继续给研究做出贡献。很多科学家在职业生涯后期都转向政策工作，但他拒绝了谷歌让他在该公司担任此类职务的提议。 他说：“我从来都不擅长这个或对此感兴趣。我是一名科学家。”</p><p>相反，辛顿在过去几个月一直扮演敲钟人的角色——他可以像任何人一样以通俗易懂的方式解释人工智能的技术细节，并花大量时间接受采访以提高公众意识。他还与政策制定者交谈，包括英国首相办公室官员、加拿大总理贾斯汀·特鲁多、欧盟委员会执行副主席玛格丽特·维斯塔格以及美国参议员伯尼·桑德斯与乔恩·奥索夫等。</p><p>辛顿表示，虽然他现在理论上已经理解了人工智能的风险，但感情上还没有跟上。 “我们作为顶尖智能的位置将被取代，这个想法很难让你接受。”</p><p>但目前，他从自己的另一位亲戚那里得到了启发：他的表亲寒春（Joan Hinton）是少数参与了曼哈顿计划的女科学家之一。在自己帮助制造的核武器被投放到广岛和长崎后，寒春成为了一名和平活动家。 1948年，她移居中国，她余生的大部分时间都在农场从事机械工作。辛顿自己的退休计划没那么喧嚣，但同样充满田园气息：他打算重新拾起木工活，并走走远路。</p><h3>李飞飞：斯坦福大学教授</h3><p>李飞飞指出，人工智能不会是技术改变世界的第一次，而且每次的改变都是为了变得更好。 “如果我们……把自己传送到历史上的任何一刻——比如火被发现的那一刻，蒸汽机被制造处理的那一刻，或者发明电力的那一刻——我认为话题都将十分相似：技术是把双刃剑。技术赋予我们力量，但这种力量也会带来危险。我认为人工智能也一样。”</p><p>对于自己所描述的希望与危险，李飞飞比大多数人都要了解。她的研究为当今运营的图像识别人工智能系统奠定了基础，并扩大了人工智能在医疗保健领域的应用。作为 AI4ALL（一家总部位于美国的非营利组织，旨在提高人工智能的多样性和包容性）的联合创始人，她一直是人工智能多样性的倡导者。</p><p>李飞飞出生于中国成都，15 岁时移居美国，后在普林斯顿大学学习物理和计算机科学，并在 2017 年获得加州理工学院电气工程博士学位。2006 年，李飞飞开始做带文本描述的图像数据库 ImageNet。至 2009 年止，李飞飞和她的团队在众包的帮助下，已经标记了 320 万张图像。一年后，她们举办了一场竞赛，目的是想看看谁能设计出识别图像内容最准确的人工智能系统。通过为研究人员提供一个共同的基准，李飞飞加速了人工智能图像识别系统的发展。</p><p>除了 2017 年至 2018 年在谷歌工作了一段时间外，李飞飞的职业生涯一直在学术界度过。最近，随着人工智能开发者不断用更强大的计算能力来训练自己的系统，学术界在筹措训练最强大的人工智能系统所需的巨额资金方面已经开始捉襟见肘。李飞飞表示： “我担心在人工智能领域，学术圈与产业圈正在形成全球性的资源鸿沟”。</p><p>2023 年 6 月，在与拜登总统会面时，李飞飞呼吁要抱持“登月心态”，主张政府在投资上要有雄心，要确保利用人工智能为公共利益服务。作为美国国家人工智能研究资源（NAIRR）任务组的成员，李飞飞尤其呼吁政府要提供人工智能的算力资源。她与工作组其他成员的努力似乎正在取得成果。 今年7 月，美国国会人工智能核心小组的领导层提出了一项法案，要建立 NAIRR，为研究人员提供安全开发人工智能所需的资源。在与孩子们一起观看完《奥本海默》之后，身为斯坦福以人为本人工智能研究院（Stanford Institute for Human-Centered AI）联合主任的李飞飞对这部电影于当下的相似之处感到震惊。 “我对科学家的责任感产生了非常非常强烈的共鸣。我们都是世界公民。”</p><h3>阿贝巴·比尔哈内：认知科学家</h3><p>阿贝巴·比尔哈内（Abeba Birhane） 是一名训练有素的认知科学家，当她注意到一项几乎没人在做的重要任务时，她开始走上了人工智能研究的道路。人工智能模型要接受数据集（文本与图像的集合）的训练，这些数据集是从互联网收集而来，规模正变得越来越大。但比尔哈内意识到，随着这些数据集从数百万条数据激增至数十亿条，几乎没人系统性地检查过其中是否存在有害材料，而后者可能导致人工智能在结构上存在种族主义、性别歧视与其他偏见。</p><p>比尔哈内与一小群研究人员一起开创了一个新学科：对可公开访问的人工智能训练数据集进行审计。这项工作会很繁重。 比尔哈内现在是 Mozilla 基金会AI Accountability的高级顾问，也是都柏林三一学院的兼职助理教授。 “我以前很喜欢到咖啡馆工作，现在没办法了，我的屏幕对于工作来说已经不安全了。”</p><p>在最近一篇正在接受同行评审的论文中，比尔哈内和她的合著者得出了一个令人震惊的结论：用更大的数据集训练的人工智能模型更有可能表现出有害的偏见和刻板印象。 比尔哈内表示：“我们笨想验证这样一个假设：随着规模的扩大，问题就会消失”。但她们的研究表明事实恰恰相反。 “我们发现，随着数据集规模的扩大，仇恨内容也会增多。”</p><h3>谢恩·列格：谷歌DeepMind联合创始人，通用人工智能首席科学家</h3><p>当顶级人工智能实验室 DeepMind 联合创始人谢恩·列格（Shane Legg）去面试求职者时，他希望能够确定他们知道自己要做什么。 DeepMind 首席运营官莉拉·易卜拉欣 (Lila Ibrahim) 表示，鉴于该公司正在开发的技术存在风险，与列格的谈话让她对孩子们的未来感到担忧。</p><p>列格自DeepMind成立以来一直是这里的首席科学家，今年4月，在DeepMind与Google Brain合并组建成Google DeepMind之后，又成为了新组织的通用人工智能首席科学家。他说，他经常会直白地谈到通用人工智能（AGI，一种几乎可以完成人类能做的任何认知任务的人工智能）什么时候会到来以及 AGI 可能带来的风险，以“看看他们对此有何反应。因为很多人认为这种事情完全是疯了。我想看看他们在思考技术水平超前的东西时自在程度如何……我认为这是一个重要品质。”</p><p>今天，很多人开始接受列格二十多年来一直关注的想法。 2011 年，他在博客网站 LessWrong 上接受采访时估计，到 2028 年，做出人类水平的机器智地可能性为50%。列格表示，其实自己在二十多年前当软件工程师时就做出了这一预测。这位工程师当时看了雷·库兹韦尔（Ray Kurzweil）的《心灵机器时代 —当计算机超越人脑》（The Age of Spiritual machines）之后，此后就一支没有改变过主意。直到最近，大多数人工智能研究人员还对他的预测不屑一顾，其中包括图灵奖获得者杰弗里·辛顿（Geoffrey Hinton）以及约书亚·本希奥（Yoshua Bengio）。但今年早些时候，这两人在这个问题上突然改变了主意，并（非常惶恐地）预测与人类水平相当的人工智能将在未来 5 到 20 年内被开发出来。49 岁的莱格表示： “他们都曾觉得我的预测太过疯狂。现在他们都不这么想了。”</p><p>列格对自己的预测非常认真，因此他决定重返校园学习更多有关人工智能的知识 — 2003 年，他开始攻读博士学位。他到瑞士卢加诺的达勒莫勒人工智能研究所工作，并因《机器超级智能》这篇论文而获得了著名奖项。 （ 2019 年，列格还把自己的导师Marcus Hutter 招进了DeepMind，担任高级研究科学家。）</p><p>2009年，在伦敦大学学院担任博士后研究员期间，列格结识了研究员同事戴密斯·哈萨比斯（Demis Hassabis）。2010 年，两人与哈萨比斯的儿时好友兼进步活动家穆斯塔法·苏莱曼 (Mustafa Suleyman)&nbsp; 一共创立了 DeepMind，其使命是通过开发 AGI 来解决智能问题，并利用它来解决人类的问题。 （“AGI”这个词是 1997 年物理学家 Mark Gubrud 先用起来的，但在 2002 年，列格独立提出了这个说法，并在他的前任老板 Ben Goertzel 的帮助下推广开来。）</p><p>此后，低调的列格开始领导DeepMind的AGI技术安全团队——这个团队在努力确保强大的人工智呢系统一旦开发出来，就会按照其创建者的意图行事，并防止自行形成有害目标的人工智能系统造成灾难。 列格估计，他和其他人有 70% 的机会到 2028 年解决这个问题。他说：“我觉得这是可行的。未必有我们想象的那么难，而且事后看来，似乎应该是相当明显的。” 列格认为，早期研究人员预见到的许多困难，比如确保人工智能系统理解人类的价值观，已经通过此后完成的工作得到解决，但很多人工智能悲观主义者不能与时俱进地改变自己的观点。</p><p>但在假设中地 AGI 实现之前很久，谷歌 DeepMind 就需要确保其人工智能系统地正常运行。据报道，该公司将于今年秋天发布迄今为止最大的人工智能模型 Gemini。谷歌 DeepMind 首席执行官哈萨比斯正在领导这项工作，甚至已经退出江湖地谷歌联合创始人谢尔盖·布林都为此重新出山。但列格表示，谷歌 DeepMind 用于确保 Gemini 行为的做法并没有什么特别之处。 “跟其他大模型以及开发者一样，我们也采用了一系列地对齐技术或这些技术的变体，而不是什么特别的技术。再过一两代，我们可能需要一些更有趣的对齐技术。”</p><p>除了在 DeepMind 从事人工智能安全方面的工作外，列格还建立了一个 AGI 社区。这个社区有约 600 名成员，约占 DeepMind 员工总数的 25%。群组有内部的聊天渠道；在会议上，大家会提出新想法或聆听外部演讲者的发言。 列格表示：“DeepMind 有很多人对 AGI 充满热枕”。</p><p>列格无疑是其中之一。他说，他不知道如果我们在超级人工智能到来后仍能幸存下来的话，生活会变成什么样子。 列格说：“我们讨论的是拥有超越人类智慧的事物，设想运用其智慧让世界变得更美好、更道德。仅凭我们人类智慧，很难知道会发生什么。”</p><p>与此同时，人工智能政策研究所（AIPI）7月份进行的民意调查发现，62%的美国人对人工智能感到担忧，而感到兴奋的只有21%。 AIPI 联合创始人兼执行董事丹尼尔·科尔森 (Daniel Colson) 表示，社交媒体的负面影响日益导致美国公众“质疑科学技术进步默认就是对社会有利的看法”。他认为，开发更强大的人工智能系统所带来的风险（列格本人也承认）很大，给停止开发提供了理由。</p><p>列格认为我们可以把事情做对。他说： “如果它能让世界变得更美好、更道德的话，那就很令人兴奋。我认为这个世界有很多问题可以通过做出一个非常有能力、有道德的智能系统来解决。这个世界是可能变得更加美好的。”</p><h3>鲁曼·乔杜里：Humane Intelligence CEO ，创始人</h3><p>今年8 月初，约 4000 名黑客聚集在拉斯维加斯，他们的目标是攻击 OpenAI、Google 与 Anthropic 的聊天机器人。能够说服人工智能违反自身规则的黑客——比如让聊天机器人给出炭疽的配方，或者散布种族主义言论——将获得积分奖励。获胜者发现了多个漏洞，其中一个聊天机器人还泄露了被告知要保密的信用卡号。</p><p>这场活动的组织者之一是人工智能伦理学家、Humane Intelligence 的创始人鲁曼·乔杜里（Rumman Chowdhury）。Humane Intelligence 是一家专门研究所谓的人工智能系统红队测试的非营利组织。这种做法借鉴了黑客文化，后者通过对计算机程序进行压力测试来识别安全缺陷而获得奖励是很常见的事情。其想法是，通过激励很多人尽量尝试去攻击聊天机器人等形式的人工智能，开发者得以发现并修复问题。因为如果这些问题是在发布后被发现的话，危险性可能会高得多。拜登政府是拉斯维加斯这场活动的重要支持者。白宫在一份声明中表示：“这项独立活动将为研究人员级公众提供有关这些模型影响的关键信息，并让人工智能公司与开发者能够采取措施解决这些模型存在的问题。”</p><p>乔杜里比大多数人都要了解人工智能的危险。在埃隆·马斯克解雇她之前，她一直是 Twitter 机器学习道德团队的负责人。 ChatGPT 发布后，她询问该机器人自己的信息。结果机器说她是一位收藏了很多鞋子的网红。乔杜里表示： “问题不仅仅在于信息的错误，还在于它十分的性别化。”</p><p><strong>问：关于目前人工智能的应用方式，你最担心什么？</strong></p><p>鲁曼·乔杜里：会导致权力和财富越来越集中到越来越少数人手里。一是它正在颠覆某些产业与民生。其次，这些人并不能反映大多数人的意见、观点、需求和愿望。但他们却要把自己的技术强加给全世界。他们的财富实际上是建立在用我们所有人免费奉献出来的东西之上的。比方说，我们在 Reddit 上发布的内容、我们在互联网上发布的自己的照片、我们在社交媒体上跟朋友的对话、我们在网上发布的书评等。他们免费抓取了这些内容，然后转而向我们收钱，同时也剥夺了很多人的生计。</p><p><strong>关于人工智能，你希望更多人了解到什么？</strong></p><p>人工智能不是魔法，只是数学，然后放进代码里。首先，几乎所有与人工智能有关的问题过去都处理过类似的问题。其次，大家以为程序员或人工智能开发者很神奇或具有独特能力，以为他们要比其他人聪明，这也是错得离谱的。这些人给自己所开发的技术营造了一种神秘感，其唯一目的就是排除他人。其三是，大家都不敢批评人工智能，不敢质疑，不敢问它是不是在做对的事。他们认为人工智能比领域专家还要擅长做决策。</p><p><strong>就连一些专家对人工智能也存在哪些误解？</strong></p><p>对知觉的理解。有很多人，甚至包括专家，也会将模仿聊天功能的用户设计当作是知觉，这个很让我感到吃惊。居然有这么多人被一个自然且易于使用的聊天界面所欺骗，就因为它能编出看似易读的语言，就以为它具备了感觉！颗我们甚至连人类的意识从何而来都还没弄清楚。</p><h3>曾毅：中国科学院教授</h3><p>大二那年，曾毅上人工智能入门课。第一堂课上，教授就完整放映了史蒂文·斯皮尔伯格2001年的那部电影《人工智能》（A.I. Artificial Intelligence）。</p><p>电影鲁曼有一幕是两名研究人员在讨论模拟人脑来造出一个懂得爱的机器人。曾毅对此伸手鼓舞。 曾毅说：“我这辈子就想干这个。也就是说，造一个会爱人类的机器人”。 41 岁的中国科学院教授曾毅一直在致力于“类脑智能”的开发——设计出尽可能与人类大脑相似的人工智能系统——并希望它们能够拥有道德感。</p><p>2016年左右，曾毅开始更加关注人工智能系统带来的风险，他开始把更多时间花在与政策制定者合作，制定有利于人工智能发展的规则上。三年后，曾毅指导团队撰写了《人工智能北京共识》。同时他还 “通过新一代人工智能治理委员会深度参与了政策制定”。</p><p>曾毅还推动进一步加强国际合作。他说： “我觉得我有责任让全世界知道中国的科学家和政策制定者确实有类似的想法”。他帮助联合国教科文组织制定了《人工智能伦理问题建议书》，并参与了许多非正式、非官方的外交举措，比如跨文化人工智能伦理与治理国际研讨会（International Workshop on Cross-Cultural AI Ethics and Governance），最近还在联合国安理会的一次会议上发表了讲话。</p><p>紧张的地缘政治气候限制了中美之间的合作，但曾毅认为，两国在对待人工智能风险的态度方面有很多共同点。最近的一项民意调查发现，62% 的美国选民担心人工智能，而对此感到兴奋的只有 21%。作为他在北京的远期人工智能中心(Center for Long-term AI)的工作的一部分，曾毅对中国公众进行了调查，发现91%的受访者支持对人工智能模型实施强制性的安全和伦理框架。他说：“我们别无选择，必须合作”。</p><h3>蒂姆尼特·格布鲁：Distributed AI Research Institute 创始人，执行主任</h3><p>蒂姆尼特·格布鲁（Timnit Gebru））与人共同撰写了最近记忆中最具影响力的人工智能伦理论文之一。那篇期刊文章认为，大语言模型存在如此的偏见并非偶然，而是有意选择将速度置于安全之上的结果。在格布鲁拒绝将自己的名字从独立发表的论文中删除的要求后， 2020 年，她失去了谷歌道德人工智能团队联合负责人的工作。 （格布鲁说自己是被解雇的；谷歌则说她是辞职的。）</p><p>发生了这些事情之后，格布鲁已成为负责任的人工智能领域的火炬手。作为Distributed AI Research Institute（DAIR）的创始人及执行董事，她为跨学科的人工智能研究打造出一片空间，而这在大型科技公司当中是很少见的。迄今为止，DAIR 的研究与社区建设齐头并进的混合模式主要集中在两条平行轨道上。一方面，格布鲁和她的同事质疑科技行业要依赖低薪、不稳定的工人，其中很多来自于南半球国家。 格布鲁说：“很多人都想把机器想象成是有知觉的，并且这个过程没有人类的参与。他们想掩盖正在发生的事情，这是其中的举措之一。”</p><p>另一方面，格布鲁致力于研究某些打算开发通用人工智能的技术人员的意识形态根源。格布鲁和她的同事埃米尔·托雷斯（Émile P. Torres）用 TESCREAL 代指这些人信奉的一长串晦涩的“主义”，这些意识形态不仅与优生学等被揭穿的伪科学有令人厌恶的联系，而且还让其追随者产生一种倾向，那就是只要能让目标（比方说，人类安全地创造人工智能）实现的可能性稍微大一点，就可以采取任何手段（造成严重不平等、贫困，甚至更糟）。 格布鲁说：“年轻的计算机科学专业的学生可能觉得自己必须遵循这条轨迹”。她指的是科技公司进一步集中财富和权力，榨取自然资源与劳动力，并泛化基于监控的商业模式。 “重要的是要弄清楚：他们的意识形态基础是什么？在了解之后再去设问，‘如果采用不同的意识形态基础呢？在这种情况下，技术将如何发展？”</p><h3>凯特·克劳福德：南加州大学Annenberg学院教授，微软研究院首席研究员</h3><p>在学者凯特·克劳福德（Kate Crawford）来说，人工智能并不是某种抽象的、未来主义的思想实验：而是一系列冷酷的物理事实。在过去 20 年的时间里，克劳福德研究了大规模数据系统对环境的影响，以及它们是如何影响我们的社会和政治体系的。现年 50 岁的克劳福德居住在纽约，曾撰写过书籍，与人共同创立了 AI Now Institute来开展有关科技行业权力集中的研究，并为世界各地的政策制定者提供建议。她跟人合作的艺术作品《人工智能系统解剖》探索了亚马逊 Echo 智能音箱的生命周期，甚至还到现代艺术博物馆展出过。</p><p><strong>问：你写过一本关于人工智能对环境影响的书，叫做《Atlas of AI》。这本书的主旨是什么？</strong></p><p>凯特·克劳福德：认为人工智能是虚无缥缈的，是飘在云端的数学算法这种看法绝对不是事实。事实上，其唯一的工作方式是析取大量数据、人力与自然资源，包括能源、水以及矿物。这本书其实讲的是人工智能是 21 世纪的采掘业。</p><p>在出现了生成式人工智能之后，这种情况甚至更加严重。数据量增加了。隐藏的人类劳动量，尤其是在从人类反馈中学习的强化学习（RLHF）方面，也在增长。生成式人工智能所消耗的能源和水量是传统人工智能的 1000 到 5000 倍。</p><p><strong>在探索数据集如何感知世界的研究项目Knowing Machines当中，你为什么要关注训练数据？</strong></p><p>训练数据层是表征（以及未表征）世界方式的基本构建块。这是构思故事的字母表。</p><p>在很多情况下，我们已经到达了人工智能史的这个阶段，你只要抓取整个互联网就可以说，“好吧，这就是世界。”但如果你声称互联网就是人类文化，那就很危险了。真实的世界比互联网呈现给你的世界要更加多样化。所以我们确实已经到了需要更多关注、更多监管，以及对如何训练人工智能来表征世界要有更多的批判意识的地步了。</p><p>从根本上说，大家没法理解它会如何改变我们看待和理解世界的方式。这是一个很基本的事情，其影响远超深度伪造问题或其对劳动力的影响。</p><h3>普什米特·哥利：Google DeepMind 研究副总裁</h3><p>普什米特·哥利（Pushmeet Kohli）认为自己本质上属于乐观或谨慎乐观的那种人。对于谷歌 DeepMind 的 AI for Science 项目（一个利用人工智能解决科学重大挑战的项目）及Responsible and Reliable AI团队（确保 DeepMind 的人工智能系统不会脱轨）的领导者来说，这样的气质再合适不过。</p><p>哥利在喜马拉雅山脚下的印度德拉敦长大，后来移居到英国学习。他在微软工作了近 11 年，最终在微软的Cognition Group（这个小组的目标是开发出能够执行人类几乎所有类型任务的人工智能系统）当起了研究总监。2017年，他加盟DeepMind，并很快组建其Safe and Reliable AI团队（后来更名了，今年4月，DeepMind已经与谷歌的另一支人工智能团队合并）。 哥利表示，DeepMind 联合创始人兼首席通用人工智能科学家谢恩·列格“从第零天开始”就一直致力于安全问题，但他的团队试图解决“近期部署 ML [机器学习] 模型带来的安全问题。”</p><p>哥利拒绝在所领导的两支团队当中做出更喜欢哪一个的选择二，但说到AI for Science 的工作时，他的眼睛亮了。 AlphaFold 是该团队迄今为止最成功的成果，已被超过 100 万研究人员使用，并且可以在几秒钟内根据蛋白质的氨基酸结构预测出结构，相比之下，之前完成这项任务需要花费数月或数年的时间。得以更好地了解蛋白质结构将加速药物发现，并可能为进一步的科学突破铺平道路。</p><p>最近，哥利的 AI for Science 团队又发布了 AlphaTensor，这是一个建立在 AlphaZero 基础上的人工智能系统，后者在包括围棋在内的一系列游戏当中表现出了非凡性能，并且可以发现新颖算法。</p><p>他认为，通过提高我们对世界的理解，人工智能最终解决的问题将多于所产生的问题。哥利说，比方说，多年来，科学家们一直在研究一些看似微不足道的问题，比如“葡萄酒是否有益健康”。他认为，人工智能也将帮助人类应对类似葡萄酒之争的问题，只不过那些将是更复杂的挑战，如气候变化和流行病等， “是行星级的问题”。</p><h3>伊尔亚·苏茨克维：OpenAI联合创始人，首席科学家</h3><p>伊利亚·苏茨克维 (Ilya Sutskever) 表示，一个不太聪明的人确保更聪明、更强大的人按照自己的利益行事是有先例的。这个先例就是人类婴儿。 OpenAI 首席科学家苏茨克维尔说“我们知道这是可能的。父母非常关心孩子的福祉。这是可以做到的。但这种胚教用的是什么机制呢？”</p><p>思考这个问题的人是业界最著名的技术思想家之一。在 2015 年作为创始成员加入 OpenAI 之前，苏茨克维尔就已经因为推动了计算机视觉与机器翻译领域的突破而闻名。OpenAI的第一步举措就是从谷歌挖走他；如果没有他，公司随后推出的一长串创新也许就会大不相同。 苏茨克维尔的名字出现在催生了 ChatGPT 以及图像生成器 DALL-E 等产品的数十篇研究论文之中。但在与苏茨克维尔交谈过后，你很快就会感觉到他觉得自己最重要的工作就在眼前。</p><p>今年7 月，OpenAI 宣布 37 岁的苏茨克维将担任其新的 Superalignment 团队的联合领导。这支团队的任务是解决如何确保超级智能人工智能符合人类利益的技术挑战。除了这项工作以外，公司还有一项短期的研究工作，针对的是当前或不久的将来所开发的较弱人工智能系统进行对齐，这两项工作是独立的，会并行进行。</p><p>在苏茨克维尔看来，理解如何将某些价值观强加到显著超越人类的系统上这项任务至关重要。他说： “到头来人工智能系统会将变得非常非常非常有能力，非常非常非常强大。到时候我们将没法理解它们。它们会比我们聪明得多。到那时候，胚教必须非常牢固才行，这一点绝对至关重要，这样它们对我们的感觉就会像我们对婴儿的感觉一样。”</p><h3>崔艺珍：华盛顿大学教授</h3><p>《纽约客》刊登过一幅漫画，描绘的是世界末日时一对夫妇面对面坐在荒地的废墟上。哪个标题更好：“哦，好吧，更糟的情况我们也活过来了”，还是“我想见到其他人”？</p><p>你的猜测要好过 ChatGPT 的。去年，华盛顿大学计算机科学教授、2022 年著名的麦克阿瑟“天才”奖获得者崔艺珍（Yejin Choi）与人合著了一篇获奖论文，里面测试了一系列先进人工智能系统能不能猜出《纽约客》漫画标题大赛的获奖作品，能不能解释出获奖作品的有趣之处在哪里。按照崔艺珍的说法，漫画标题作者的工作是安全的——至少目前是这样。</p><p>崔艺珍的研究重点是人类智能与 ChatGPT 等人工智能的众多不同之处。 “计算器算得比我更好更快，但这并不意味着计算器在智力的其他维度上优于我们任何人。”</p><p>崔艺珍出生在韩国，2000 年移居到美国，为微软工作，她职业生涯的大部分时间都在研究人工智能系统是否能够培养出常识以及幽默感。最近，46 岁的崔艺珍对开发理解社会和道德规范的人工智能系统产生了兴趣。 她说：“这源于我对公平以及多样性的兴趣”。但崔艺珍很快意识到，道德规范也与对齐相关，也就是确保人工智能系统按照其创造者的意图行事的问题。那些关心对齐的人往往会担心流氓人工智能可能会制定出有害的目标，并最终在追求这些目标的过程中杀死人类。但崔艺珍说，这样做显然违反了道德规范。不过她又表示，了解我们如何形成自己的道德感并不容易。 “人类如何获得这个东西是很神秘的。”</p><p>崔艺珍说，首先，人类要靠指导来辨别是非。从某种意义上说，很多大语言模型（比如为 OpenAI 的 ChatGPT 提供支撑的模型）就是这么训练的，利用了一个所谓的人类反馈强化学习 (RLHF)的过程。但崔艺珍表示，精心设计的指令可能会导致人工智能系统给出有问题的响应，而且当前的训练模型事实是“拼凑而成或属于大规模的打地鼠”，这一点应该引导我们继续寻找“一个更好、更强大、更简单的解决方案。”</p><p>崔艺珍现在把很多时间花在思考向人工智能系统传授道德价值观时缺失的部分。但是，她警告说，就算这个问题得到解决，要解决的问题还有很多。 她说：“对齐假设有一个可以优化的数学目标。但我不觉得人类社会是这样的，因为我们彼此之间是如此的不同。” 崔艺珍认为，考虑到文化规范的广泛性——即便在同一社区内，代际之间也持有截然不同的价值观——所以没法找到一种正确的解决方案加以优化。 “我们得设法弄清楚如何支持不同个人所拥有的这些多元化的价值观。”</p><h3>杨立昆：Meta 首席人工智能科学家</h3><p>杨立昆（Yann LeCun） 一直以来都是按照自己的节奏在前进。 20 世纪 80 年代，这位法国计算机科学家假设可以设计人工神经网络来模仿人类大脑，之后在几十年的时间里，他的想法被很多人嘲笑成异想天开。但由于这个领域的技术突破，杨立昆的想法将为当前的生成人工智能革命奠定基础。</p><p>如今，现任 Meta 首席人工智能科学家级纽约大学计算机科学教授的杨立昆仍会发表大胆、有争议的声明，与他不认同的人争辩。杨立昆说：“我是可以保持沉默，但这不是我的风格”。</p><p>比方说，杨立昆驳斥了人工智能会导致灭绝的恐惧，称这种观点很“荒谬”，跟“世界末日崇拜”类似，甚至一度与深度学习的其他两位先驱，2018年与他一道拿到图灵奖的杰弗里·辛顿 (Geoffrey Hinton) 与约书亚·本吉奥 (Yoshua Bengio) 发生争执。他认为，当前对大语言模型 (LLM)以及ChatGPT滋生的热潮是一种误导，很快就会陷入死胡同。他还极力捍卫人工智能在打击 Facebook 上仇恨言论方面的有效性。</p><p><strong>问：你已经见证了人工智能的炒作周期。可不可以将这一刻与之前的几十年对比一下？</strong></p><p>杨立昆：这个炒作周期的主要区别在于大家有东西可以玩。我认为这可以帮助激发很多人的想象力。</p><p>毫无疑问，这个领域的人（包括我在内）都对大语言模型的效果感到惊讶。但它离完美还差得远。通过让这些系统变得更大利用更多的数据，这样的训练方式并不能让它们达到人类智能。我现在是想用“等一下，游戏不会到此结束”这样的话来让大家稍微冷静一下。如果我们想象我们正走在通往真正智能机器的高速公路上，想想会很有趣，但其实我们是走在出口匝道上。</p><p><strong>今年早些时候，Meta 决定把自己的人工智能技术作为开源软件赠送出去，这引起了包括谷歌以及 OpenAI 在内的众多实体的激烈批评。斯坦福大学的一位研究人员甚至把这种行为比作“给杂货店里的每个人提供了一枚手榴弹”。他们的担忧是否不是毫无根据？</strong></p><p>开源生成模型已经出现好几年了，我们还没有看到这些东西被大规模用到大家谈论的邪恶用途上，比如大量生成虚假信息、网络攻击或设计致命病原体。我觉得这些场景就像007电影里面的那些场景。有些人就是以妄想为生的。</p><p>归根到底，你是不是相信整个社会还是会以一种绝对积极的方式去使用技术，哪怕存在某些邪恶的用途？印刷机也会让许多怀有恶意的人传播虚假信息，双向的互联网也一样，人工智能系统亦如此。但它给社会带来的好处实在是太大了，你可能愿意承受这些风险。</p><p>当然，对于那些相信寸止生死存亡风险的人来说，这种是没法做权衡取舍的。但那需要机器取得极大飞跃。当前的人工智能系统当然不存在生存风险。</p><p><strong>你说人工智能末日论很“荒谬”。你为什么如此肯定这项技术不会威胁世界？</strong></p><p>这个世界的那些埃利泽·尤德科夫斯基们说：“如果你犯了一点小错误，只要打开这个超级智能系统，它就会立即接管世界并逃脱我们的控制。”但这样式行不通的。这就好像你在 1960 年的时候说：“我们要造第一架横跨大西洋的喷气式飞机。我们不会事先进行测试。我们会一造好飞机就让一千人坐上去，驾驶它飞越大西洋，希望它不会爆炸。”当然了，这是极其愚蠢的。</p><p>客机花了几十年的时间才变得超级安全：经过了数十年的微调与精心设计。如果你试图解释怎么让涡轮喷气发动机变得安全，你就得研究非常非常复杂的机械工程、热力学以及各种没人能理解的东西。你不需要很熟练就能想象出数百种涡轮喷气发动机会如何出事的情形。但实际上，设计一款可靠的产品需要真正的才华与专业知识。 [编者注：达到目前航空安全水平需要不断试错，这个过程多年来已经导致数千名乘客死亡。]</p><p>所以人工智能也是一样的情况。很多人说人工智能不安全。这些人很幼稚。他们不懂。这是一个我们甚至还没有开始着手解决的复杂工程问题，因为我们甚至还没有一个好的超级智能人工智能系统设计。</p><p>现在，很多人工智能研究人员自己都没法想象如何确保这些系统的安全，比方说杰弗里·辛顿和约书亚·本吉奥。但这个问题我已经思考很多年了。我认为可以通过设计它们必须遵守一定数量的目标来确保它们的安全。你得把这些目标硬写进去，这样从构造上就保证了系统在完成任务的过程中不会产生不遵守护栏的输出。</p><p>于是就引出了设计这些护栏的问题。但这并没有大家想象的那么困难，因为这跟设计法律有点类似。</p><p><strong>你觉得是什么原因导致你的观点跟杰弗里·辛顿与约书亚·本吉奥的存在那么大的分歧？</strong></p><p>辛顿认为大语言模型比我想象的更聪明，而且在它们如何让我们达到人类水平的人工智能的问题上，他比我要更乐观一些。所以他突然间意识到，“我们需要担心超级智能机器。如果你有一个更聪明的实体的话，它就会想要接管世界。”</p><p>我认为这是不对的。聪明和想要接管之间没有任何关联。哪怕是在人类内部，也不是最聪明的人就一定想当领导。事实上，大多数情况恰恰相反。</p><p>统治欲望确实是有等级组织和社会性的物种的特质。这确实是人性的结果，事实上也也是进化将我们塑造成这样的。但红毛猩猩并不是一个社会物种，也没有统治谁的欲望。它们不需要。所以我们可以像猩猩一样聪明，但没有任何统治的欲望。</p><p><strong>你对目前 Facebook 上人工智能系统标记仇恨言论的正确率感到满意吗？</strong></p><p>离完美还差得远。任何违反内容政策的帖子类型获得通过都属于失败。但它正在取得很大的进步，这完全归功于人工智能。</p><p><strong>尽管大家不认可了这么多年，但你对神经网络的看法是正确的，这一点会不会让你对自己当前一系列有争议的观点更有信心？</strong></p><p>我想我一直都比较固执己见，不怕站出来讲话。我是一名科学家。我可以保持沉默，但这不是我的风格。</p><h3>黛博拉·拉吉：Mozilla基金会研究员</h3><p>2017 年，在机器学习公司 Clarifai 实习时，黛博拉·拉吉（Inioluwa Deborah Raji）有了一个令人担忧的发现。</p><p>27 岁的拉吉正在帮助这家初创公司训练一个内容审核模型，目的是要过滤掉露骨的图像，但她注意到该模型把包含有色人种的内容标记成不露骨的比例特别高。模型训练所用的数据缺乏多样性，而产品也反映了这一点。换句话说，这个程序正对真实世界进行过滤，让它变得更加白人化。</p><p>她了解到这种缺乏多样性并不是意外，而是行业常态。 “当我说我们需要更多样化的数据时，我得到的回应是，数据本来都已经很难拿到了，为什么还要考虑做出更复杂的选择呢？”</p><p>这段经历促使拉吉将她的注意力从创业世界转向人工智能研究，她开始关注人工智能公司如何确保自家的模型不会造成不当伤害，尤其是对在开发过程中可能被忽视的人群造成的伤害。 她说：“自我看来很显然，这个领域的人甚至都没意识到这是个问题”。</p><p>拉吉的工作重点从此变成了设计方法从内外部对开发人工智能系统的公司进行审计。她与谷歌的道德人工智能团队合作，为人工智能系统引入了更全面的内部评估流程。她还与算法正义联盟（Algorithmic Justice League）合作，为其性别阴影（Gender Shades，评估由 IBM、微软和 Face++ 开发的人工智能驱动的性别分类工具的准确性）审计项目制定了“外部审计”策略。</p><p>虽然许多关于监管人工智能的讨论都设想这种技术会在遥远的未来才使用，但拉吉表示，近期也需要关注。她指出，当公司在发布人工智能系统之前未能对其进行正确评估时，是会造成潜在影响的。算法可能会识别错嫌疑人，导致抓错人，或导致对有色人种的住房歧视长期存在。 拉吉说：“现实情况是，很多部署都已经开始了，而且执行起来相当随意”。</p><p>拉吉指出，在缺乏政府监管的情况下，目前得由开发者对自家产品及其可能造成的危害提供透明的评估，尽管者也许并不是最可靠的来源。 她说：“现在公司的产品是基于一个理想基准来评估的。从隐私到如实透露系统对特定用户的有效情况，基准都没有对产品做出修改要求。”</p><p>通过与 Mozilla 基金会（一家专注于互联网保护的全球非营利组织）合作，她一直致力于寻找开源的审计工具，从而在更广范围内实施之前，让（从政府官员到可能受系统影响的人的）任何利益相关者更好地理解产品，对其发起挑战。</p><p>她说：“一旦你从根本上开始质疑这些说法时，你会发现水面下的问题要复杂得多。这就像打开了潘多拉魔盒。”</p><h3>马克思·泰格马克：2023 年人工智能领域 100 名最有影响力的人物</h3><p>今年 3 月，生命未来研究所（Future of Life Institute）发表了一封公开信，警告人工智能的最新进展对“社会和人类造成巨大风险”，并敦促人工智能实验室将训练比 GPT-4 还要强大的系统暂停。这封信获得了超过 30000 人的联署，其中包括该研究所的顾问埃隆·马斯克、苹果联合创始人史蒂夫·沃兹尼亚克以及作家尤瓦尔·诺亚·哈拉里（Yuval Noah Harari）等知名人士，从而帮助公众提高对人工智能安全问题的意识。</p><p>马克斯·泰格马克 (Max Tegmark) 是瑞典裔美国物理学家，也是这家总部位于麻省剑桥市的研究所的联合创始人，他把大量时间花在警告该组织所谓的“极端大规模风险”，比如核战争的影响以及无视通用人工智能或超级人工智能的威胁上。今年1月该研究所曾推出了一部虚构短片《Artificial Escalation》，里面探讨了美国和中国在核指挥系统中使用人工智能产生的灾难性影响，影片引起了大家的争议。</p><p>泰格马克专注于人工智能、物理学与神经科学的交叉领域，他是麻省理工学院物理学教授，但也在他的祖国瑞典学习过经济学。他在答问中介绍了《Artificial Escalation》，并分享了他对人工智能监管的必要性以及当前人工智能商业模式的道德规范的想法。</p><p><strong>问：生命未来研究所最近发布了一部虚构短片，里面探讨了美国和中国将人工智能纳入各自核指挥、控制级通信系统的场景，结果是灾难性的。为什么要制作这部电影？为什么是现在？</strong></p><p>马克斯·泰格马克：我们制作这部电影是为了说明人工智能推动危机升级的速度其实要快得多，比人类独立做出评估、表明意图和缓和升级的速度要快，从而大大增加了核战争的风险。全球领导人目前正在思考人工智能对国际和平与安全构成威胁的不同方式，这是主要方式之一。</p><p><strong>有观点认为可以给军队设置足够的护栏，让人工智能无法决定人的生死，你怎么看？</strong></p><p>首先，对于一项迫使我们的对手采取此类护栏的国际条约，美国政府目前是反对的。其次，上述护栏对于解决网络漏洞、认知不确定性或其他关键风险因素没有起到任何作用。最后，正如我们在电影里面所看到那样，名义上有人类参与决策并不意味着我们能保持控制。</p><p><strong>政府可以采取哪些措施来监管人工智能？</strong></p><p>最好的行动方针是效仿生物技术，确保有潜在危险的产品要获得在人工智能[版] 的FDA 批准。超过60%的美国人支持这种做法。</p><p><strong>对于未来人工智能的采用，你最担心的是什么？</strong></p><p>人类会在未来几十年内灭绝。掀起超人类人工智能竞赛的首席执行官们最近签署了一份声明，警告他们正在尝试开发的产品可能会导致人类灭绝。他们不清楚这些人工智能的工作机制，也没有令人满意的解决方案来保证它们的安全。</p><p><strong>关于人工智能，有哪件事情是你希望能有更多人了解的？</strong></p><p>变化的节奏。人工智能的发展太快了，快到很多多专家预计它会在几年内在大多数任务的表现上超越人类。大家需要明白，如果我们现在不采取行动的话，那就太晚了。</p><p><strong>我们仍然需要解决的关键伦理或哲学问题是什么？</strong></p><p>少数未经选举产生的科技领袖让其他人类冒如此巨大的风险，这符合道德吗？</p><p><strong>当前的人工智能与你五年前的预测有何不同？</strong></p><p>能力提高得比我预期的还要快，监管的进展比我预期的还要慢。</p><p><strong>你觉得哪部科幻小说或电影最能代表人工智能的未来？</strong></p><p>亚当·麦凯（Adam McKay）的《不要抬头》。这部电影很好地展现了我们现在所处的位置，以及我们将来要去到哪里（如果小行星代表超级智能的话）。</p><h3>约书亚·本希奥：蒙特利尔学习算法研究所科学主任</h3><p>约书亚·本希奥（Yoshua Bengio） 是过去三十年最重要的人工智能研究人员之一，对于人工智能可以做什么，他的了解程度比大多数人都要好。尽管如此，在邂逅了 OpenAI&nbsp; 2022 年 11 月发布的人工智能聊天机器人 ChatGPT 时，本希奥还剩产生了“本能”反应。他用了一整个冬天以及春天的大部分时间，才开始从理性和感性上调整到位，接受了人工智能超越人类速度的新认识。</p><p>本吉奥在今年8月下旬的一次演讲中说道：“在心理上意识到你一直在为之努力的事业，你以为对社会、人类、科学来说是件好事，但实际上可能却是灾难性的，这一点是非常具有挑战性的。这就像你以为自己一辈子都在行善，然后有人[告诉你]你其实一直在制造一颗会杀死所有人的炸弹。”</p><p>今年 3 月，59 岁的本吉奥公开谈起了人工智能带来的风险，就在他的导师杰弗里·辛顿（2018 年与他一起获得了著名的图灵奖）离开谷歌的几周前，他发出了警报。在蒙特利尔大学担任教授已有三十多年的本吉奥说： “辛顿跟我在没有相互沟通的情况下得出了相同，或者说非常相似的结论”。 本吉奥和辛顿分别做出预测，在未来 5-20 年内的某个时候，将会开发出在所有任务上都优于人类的人工智能。</p><p>本吉奥所在的团队取得了多项基础性突破，从而为人工智能近期的快速发展奠定了基础。 2003 年，本吉奥证明神经网络可以通过预测下一个单词来学习人类的语言模式（比方说自动更正），这为现代大语言模型奠定了基础。 2014 年，本吉奥与伊恩·古德费洛（Ian Goodfellow）合作，提出了一种训练人工智能的方法，让两个人工智能相互竞争，一个生成内容，另一个判断其质量。 2018 年，本吉奥帮助形成了注意力适应神经网络的概念，通过关注最相关的部分来理解社交网络等高度关联的数据。</p><p>与大多数同行不同，本吉奥一直不为产业界的更高薪水或更多的计算资源所动。他解释道： “我本质上是一名学者——我希望在我所做的事情上保持自由”。</p><p>现在，本吉奥打算利用他非凡的智力，认真思考当前的形势，他说，“人类似乎陷入了绝望”。</p><p>本吉奥表示，未来，人工智能可能会胜过我们。但在那之前情况可能会变得很棘手。 本吉奥说：“近在咫尺的一件就是干预选举。下次美国大选可能就会发生这种事情。”今年，他前往美国国会就人工智能的危险作证，并撰写了有关人工智能政策和治理的论文。他打算开始将工作转向对人工智能安全技术的研究。</p><p>本吉奥说：“不管是哪种情况，真正的问题是，我能做些什么来降低这些风险？我没法将其归零。目前还不清楚是不是有人可以做到。但如果我们能够将坏事发生的概率降低 10 倍，那就去做吧。”</p><h3>艾米莉·本德：华盛顿大学教授</h3><p>艾米莉·本德（Emily M. Bender） 并不认为自己是人工智能研究员。这位华盛顿大学教授首先是一位语言学家。但她对大语言模型危险性的敏锐研究以及对人工智能炒作周期的严厉盘问让她成为本行业火力最强大的批评者之一。</p><p>早在 ChatGPT 出现之前，Bender 就已经是机器学习神话的戳破者，她揭穿了关于人工智能功能的过度承诺，并挑战这些系统是智能的想法。她说： “你不能指望机器学习系统能够学会训练数据没有的东西。否则你就是在期待魔法出现。”</p><p>在 2020 年一篇有先见之明的论文里，本德与她的合著者将大语言模型比作是在水下电缆旁，监听位于不同荒岛的两个人通话的章鱼。如果章鱼花费了大量时间偷听他们之间的对话，也许就能很好地预测每一方的反应，然后就可以切断电缆，并冒充对话的其中一方。但它没法理解自己所说的话或听到的话——如果其中一位岛民需要帮助对方抵御熊的袭击的话，问题就大了。本德认为，“人们以为它能够理解，这是很危险的。”</p><p>本德继续探讨了大语言模型的直接风险：在气候危机时期，它们会是能源消耗大户。它们加剧了偏见，并自信地将谎言视为事实，污染了信息的生态体系。而且它们在解析英语以外的语言方面表现通常要差得多，当这些模型被应用到紧急响应系统等关键基础设施之中时，非英语使用者将面临风险。本德敦促这个领域的研究人员至少要告知他们用来开发模型的语言名称，以免给人留下错误且可能有害的印象，即这些系统对于非英语世界同样有效。本德的声音铿锵有力，事实上，它现在被称为“本德原则”。</p><p>虽然本德不是政策专家，但在过去一年的时间里，她的工作帮助立法者形成对人工智能偏见和监管必要性的看法。越来越多的危言耸听者警告说人工智能很快就会变得复杂到将取代我们所有人，她是这种声音重要的反对者。在本德看来，真正的风险在于相信这些系统更像我们人类而不是章鱼。</p><h3>玛格丽特·米切尔：Hugging Face 首席人工智能伦理科学家</h3><p>玛格丽特·米切尔 (Margaret Mitchell) 与蒂姆尼特·格布鲁 (Timnit Gebru) 曾共同领导过谷歌人工智能道德团队三年。但两人在 2020 年发表了一篇论文，里面提出大语言模型加剧了社会不平等，部分是因为企业决定将规模置于安全之上，米切尔和格布鲁表示自己因此被迫离职。 （谷歌称米切尔是因为违反行为准则而被解雇的，格布鲁则是自己辞职的。）米切尔此后变成了一名主要批评者，职责人工智能公司缺乏多样性和包容性，从而对这些公司所开发的技术的质量产生负面影响。现在，米切尔是面向开发者的人工智能初创企业 Hugging Face 的首席人工智能伦理科学家。她的公众重点是确保开源人工智能带来的好处尽可能的多，并尽可能地减少危害。</p><p><strong>问：人工智能让你彻夜难眠的原因是什么？</strong></p><p>玛格丽特·米切尔：这与技术本身无关，而与创造技术的文化有关。这个圈子存在一种狭隘的思维方式，不成比例地将女性和有色人种排除在外。这意味着技术地每一个发展方向都没有充分考虑到人类价值观的多样性。因此，虽然一方面他们开始声称他们想要开发出符合人类价值观的系统（顺便说一句，这儿是好事，女性和有色人种讨论这个问题已经有 20 年了），但却是口惠而实不至，他们并不理解什么是价值多元化，不知道往一个系统注入价值意味着什么，并且只能以过于简单的数学方式来做这件事。因此，部分推出这项技术的公司显然没法反思他们是如何制造了自己公开表示想要避免或减轻的问题的，这实在是令人愤怒。我晚上睡觉时的感受不是担心，而是愤怒。</p><p><strong>说到人工智能的风险和危害，你是否担心不断渗透会成为大语言模型、diffusion模型以及其他形式的人工智能所造成危险的主要部分？人工智能带来的危险有多少来自于权力集中在少数人手中，又有多少来自于赋予了不大但很坏的行为者更多的权力？</strong></p><p>这是一个非常常见的误解。不管是封闭系统还是开放系统，坏人都会干坏事。一般来说，有时候最有效的攻击是在封闭系统内进行的，因为攻击大众会更容易。[开放系统中]的安全漏洞会不断有人寻找并做出修复。我同意你的观点，坏人可以用开源软件来做出坏东西。这确实是的。我只是不确定净伤害是坏人造成的。这有很多原因。不良行为的载体是通过社交媒体平台、新闻网站或类似的东西。因此，在控制坏人方面，我认为应该继续致力于保障分发途径。但总的来说，这些（可能会生成恶意材料的）开源模型用小型个人计算机就可以跑，但规模不大。要想大规模部署，你得利用其他服务，而这其他服务就是限制因素。</p><p><strong>如果说现在大家对人工智能的讨论方式有什么可以改变的话，会是哪一种？</strong></p><p>我希望大家不要滥用多义性。因此，机器学习里面的“学习”的意思跟人类智能里面的“学习”含义有所不同。但在讨论的时候“学习”被滥用了，他们假装这是一码事。有人滥用术语来误导。公众感到困惑。作为一个真正理解这门语言的人，目睹这一切实在是令人愤怒。</p><h3>保罗·沙瑞尔：新美国安全中心执行副主任</h3><p>2007 年，保罗·沙瑞尔 (Paul Scharre) 随美国陆军游骑兵部队前往伊拉克，途中他遭遇了公路炸弹。 “我本以为会看到拆弹人员（技术员）穿着大大的拆弹服出来，就像电影《拆弹部队》的情形那样。结果他们弄了一个小机器人来拆除炸弹。我脑子里的那盏灯一下子亮了。”</p><p>几年过去了，进入美国国防部工作的沙瑞尔多次出访伊拉克和阿富汗之后，他的那盏灯依然亮着。他首先想要探索的事情之一是用机器人技术来拉开军事人员与潜在威胁之间的距离。后来，他带领一个工作组起草了制定美国国防部自动武器系统政策的命令。</p><p>自 2013 年离开美国国防部以来，沙瑞尔一直在军事事务智库新美国安全中心（Center for a New American Security）工作，一开始担任研究员，最近担任副主任兼研究主任。他继续关注着人工智能与军事的交叉领域，会经常关注该领域的行动。2018年，他出版了自己的第一本书《无人军队》（Army of None: Autonomous Weapons and the Future of War）。这本书重点关注无人机等武器，这是当时人工智能与军事结合的核心问题。</p><p>尽管担心强大的人工智能系统落入对手手中，但沙瑞尔也担心涉及强大人工智能系统的事故。 “我担心会发生行业事故。我也担心会发生人工智能武器化形式的事故——各国都已经将人工智能引入到军队之中。”</p><p>他承认，在国土安全界这种观点属于少数，因为“国家安全机构往往优先考虑保持竞争的领先地位”。</p><p>沙瑞尔担心，无论是国家层面的监管还是国际层面的条约缺乏协调，都可能导致出现危险的军备竞赛。 “未来可能会出现一些我们根本就不应该做出来的人工智能系统。具体什么样的系统我们还不知道。但有些系统也许就是危险到不该开发出来或部署出去的。”</p><h3>简·雷克：OpenAI Superalignment联合负责人</h3><p>人工智能系统现在的反应还很迟钝，敏锐的用户通常可以判断出它们的输出是否存在潜在危害。比方说，如果一位首席执行官要求 GPT-4给她提供实现公司利润最大化的方法建议，她是很有可能判断出 GPT-4 的回答会不会导致公司破产的。</p><p>但随着人工智能系统的能力变得越来越强大，它们的输出可能会变得复杂到人类无法评估的地步。对于那些致力于对齐的人来说，这一点尤其令人担忧——这个研究领域致力于确保人工智能系统按照创造者的意图行事——因为他们担心人工智能系统可能会制定出对人类而言可能是灾难性的目标。比如说，如果 GPT-6 被问到如何实现公司利润最大化，它很可能会生成一个首席执行官无法评估的极其复杂的计划。如果她决定执行这项计划，她的利润可能会飙升……或者 GPT-6 可能会操纵她，为自己的议程谋取巨大权力。在按下回车键之前，她可能根本没法知道会产生什么结果。</p><p>36 岁的雷克是顶级人工智能实验室 OpenAI&nbsp; Superalignment 团队的联合负责人，他希望自己的工作能让真相更容易被揭开。</p><p>自从看了雷·库兹韦尔（Ray Kurzweil）与埃利泽·尤德科斯基（Eliezer Yudkowsky）的作品以来，雷克花了十多年的时间去思考对齐问题。在与开创性的对齐研究人员，如曾任澳大利亚国立大学教授、现任 DeepMind 高级研究员的马库斯·赫特（Marcus Hutter），《超级智能》（Superintelligence）一书的作者尼克·博斯特罗姆（Nick Bostrom），以及 DeepMind 的谢恩·列格开展了密切合作之后，雷克于 2021 年加入了 OpenAI。</p><p>现在，雷克正在参与迄今为止最雄心勃勃的对齐努力之一。今年7月， Superalignment 团队宣布，他们给自己留出了四年的时间窗口期，“以确保比人类聪明得多的人工智能系统能遵循人类意图”。为了帮助他们完成这项任务，他们被分配了 OpenAI 稀缺且昂贵的计算资源的20%，还有共同领导该团队的 OpenAI 首席科学家伊尔亚·苏茨克维的强大脑力支持。</p><p>雷克认为，这些潜在的人工智能系统可以从多方面提供帮助。比方说，它们可以帮助检查构成人工智能系统世界模型的大量数学表示，即便是开发人工智能模型的科学家都很难解释这些数学表示。今年 5 月，OpenAI 朝着这个方向迈出了早期一步，他们发表了一篇论文，里面解释了 GPT-4 试图解释 GPT-2里面每个神经元的用途。尽管结果好坏参半，但还是取得了一些成功——GPT-4 发现了一个似乎与“对加拿大人、地点与实体的引用”相对应的神经元。</p><p>也许是因为雷克致力于对齐的实际解决方案而不是思想实验，所以他比许多致力于预防人工智能相关灾难的人要乐观一些——他更愿意强调人类能动性之大。</p><p>雷克说：“还有很多事情悬而未决。人类对所发生的事情拥有很大的所有权，我们应该努力让它走正路。从某种意义上说，[避免人工智能灾难]是每个人的责任，因为这可能需要很多人共同努力，并在正确的时间做出正确的决定。”</p><h3>保罗·克里斯蒂亚诺：对齐研究中心创始人</h3><p>希腊神话中的迈达斯国王希望自己碰到的一切都能变成金子。他的愿望得到了实现，但这个恩赐很快就变成了诅咒，甚至连他吃的东西和他的女儿都被变成了金子。</p><p>十年前，很多关于人工智能世界末日的思想实验都设想过迈达斯国王的情形。在这些思想实验里，人类会告诉人工智能系统该做什么，而人工智能则会最大限度地、一字不差地执行，从而导致灾难发生。比方说，一个人工智能系统被告知要最大限度地提高回形针工厂的产量，然后，它就会把地球上的所有原子，包括构成人体的原子，都变成回形针。</p><p>那些致力于对齐（确保人工智能系统按照创造者的意图行事）的人不再担心这一点。研究人员现在可以训练人工智能系统通过迭代地方式学习难以阐明的目标，方法是让人类根据人工智能给出的响应的帮助程度对它们进行排名，并让人工智能系统学习生成它预测会被评为帮助作用尽可能大的结果。通过这种方法，人类不必说出他们希望人工智能做什么，只需要告诉人工智能它是否做到了即可。</p><p>这种技术被称为基于人类反馈的强化学习（RLHF）。保罗·克里斯蒂亚诺 (Paul Christiano) 是其主要架构师之一。 作为对齐领域最受尊敬的研究人员之一，克里斯蒂亚诺于 2017 年加入了 OpenAI。四年后，他离职并成立了对齐研究中心 （ARC，一家位于加州伯克利的非营利性研究组织），开展理论对齐研究并开发技术来测试人工智能模型是否具有危险能力。当 OpenAI 与 Anthropic 想知道是否应该发布自己的模型时，他们会去问 ARC。</p><p><strong>问：能否描述一下 RLHF技术的发展？</strong></p><p>保罗·克里斯蒂亚诺：先介绍一下背景。在我加入 OpenAI 之前，有两条相关线索需要注意。一是我已经思考对齐已经有很长一段时间了，我一直尝试理解合理的对齐解决方案是什么样的。 RLHF 是一个非常早期且自然的步骤。</p><p>我认为第二条需要注意的线索是，一群人通常在更简单的环境下致力于从人类那里学习价值观。有一群人，尤其是机器人领域的那些人，正在思考如何学习难以指定的任务的奖励函数。</p><p>[OpenAI] 的第一个项目试图进入某些深度 RL [强化学习] 效果很好的领域。模拟机器人任务和玩游戏是深度强化学习最成功的两个领域。因此，我们刚刚在这两个领域做了一个项目，结果表明人类定义的模糊目标是可以学习的。这个项目做得相当不错，然后下一步就是尝试把它适配到更有趣或更现实的模型 - 尝试用到语言模型上。这是与训练 GPT-1 并行进行的，或者时间稍早一些。</p><p><strong>2021 年你为什么要离开 OpenAI？</strong></p><p>我修完了学习理论的博士学位。我天然的比较优势肯定是做理论研究。我之所以在这四年从事实证研究，很大程度上是因为关于对齐的实证研究还不够充分，而且似乎在 OpenAI 这里，我可以帮助去实现一些该有的非常基本的东西。</p><p>就具体时间而言，大约在同一时间内，跟我合作的一帮人离开并成立了 Anthropic。因此，这在一定程度上增加了与 OpenAI 以外人员合作的动力，并减少了与 OpenAI 的人沟通的动力。我稍微考虑过政策影响力问题。但最重要的事情肯定是想回到理论研究上。</p><p><strong>为什么让更多的人跳出企业实验室有利于政策影响力？</strong></p><p>对实验室施加外部压力，让对方实施负责任的政策非常重要。我认为，无论是对于实验室施加压力而言，还是对于作为与世界其他地方互动而言，拥有不被看作是实验室的人是很有价值的。</p><p>模型评估旨在了解人工智能系统可以做什么（能力），以及它们是否按照开发者的预期工作（对齐）。 对齐研究中心对OpenAI与Anthropic均进行了模型评估。这些是怎么做出来的？是通过你跟这些公司的个人关系来实现的吗？</p><p>进行评估的理由非常充分。所以与我有任何关系无关，是这些实验室很乐意做的事情。我与 OpenAI 和 Anthropic 的人关系是不错，这对我很有帮助，所以这件事做起来特别容易。我还认为这两家可能是对进行此类评估最感兴趣的组织。</p><p><strong>你在对齐社区的时间比大多数人都长。对齐社区是不是可以通过一些方式上的改变，从而在其寻求解决的问题上取得更好的进展？</strong></p><p>实际上，最重要的事情似乎是引入更多关心对齐的人。早在 2012 年，只有当你对人类的长远未来真正着迷，或者对人工智能异常兴奋的时候，你才会考虑这个问题，那时候的情况是说得过去的。那时候还没看到当今世界出现的这些我们所担心的风险，但我认为给我们今天的处境与未来的风险之间划清界限要容易得多。</p><p>从社会影响的角度来看，让更多的人思考这个问题也是很有意义的。各国都应该积极关注这件事，更广泛的社会也应该积极关注。聪明人对这些问题越来越感兴趣，并且除了机器学习圈子与湾区以外，它正在拓展到新的方向。</p><h3>卡利卡·巴里：微软印度研究院首席研究院</h3><p>25 年前，当卡利卡·巴里（Kalika Bali）告诉其他技术专家她想用全世界最边缘化的语言工作时，他们劝这样只会边缘化自己。 来自印度的巴里无视这个建议：“我就觉得我周围的人没法获得技术，于是我坚持了下来——现在我非常乐观。”</p><p>在大型科技公司的研发子公司微软研究院，巴里正在努力确保人工智能的爆发能够包容边缘化语言。在盖茨基金会的一个项目力，她正在帮助用五种印度语言（总共有超过 10 亿人使用）建立“性别意向性”数据集。通过确保这些数据集不包含有从互联网上抓取人工智能训练数据时经常出现的性别偏见，巴里希望不仅能让更多的人接触技术，而且能够以一种不带偏见的公平的方式与技术互动。这个项目有发挥更大影响力的机会：它可以帮助巴里及其同事开发一个有望消除现有人工智能数据集的性别偏见的工具。</p><p>在微软研究院的另一个项目里面，巴里正在研究代码混合（code mixing）——这种做法在多元文化社区当中很常见，将两种语言集成到一起——以确保人工智能工具也能满足这些社区的需求。 巴里说：“我认为应该有一个语言不再成为技术障碍的世界。这样每个人不管说什么语言就都可以使用技术了。”</p><h3>斯图尔特·拉塞尔：加州大学伯克利分校教授</h3><p>在 2019 年出版的非小说类著作《人类兼容》(Human Compatible) 里，计算机科学家斯图尔特·拉塞尔 (Stuart Russell)将人类在不考虑后果的情况下开发人工智能的尝试比作高级外星文明与人类之间进行电子邮件交换。外星人给人类发电子邮件说：“警告：我们会在 30 到 50 年内抵达地球。”他们会收到自动回复：“人类目前不在办公室。我们回来后会回复你的消息的。”</p><p>出生于英国的拉塞尔在获得博士学位之前曾在牛津大学学习物理学。在斯坦福大学，他职业生涯的前半段都是在加速外星人的到来。作为加州大学伯克利分校电气工程与计算机科学教授，拉塞尔对这个领域做出了基础性的贡献。他还帮助其他人进入该领域并做出了自己的贡献。他与彼得·诺米格（Peter Norvig）一起撰写了权威著作《人工智能：现代方法》（Artificial Intelligence: A Modern Approach），据称该书已被 134 个国家的 1547 所学院和大学使用。</p><p>但大约在十年前，拉塞尔开始思考一个问题：如果我们成功了怎么办？ 2013 年在收到一封电子邮件之后，拉塞尔成为禁止致命自主武器系统直言不讳的倡导者，他担心一旦这些系统可以廉价生产，形成军团，就会被用来针对不同肤色或政治立场的人群。 2016年，他在伯克利成立了人类兼容人工智能研究中心（Center for Human-Compatible Artificial Intelligence），其主要关注点是“确保人工智能系统对人类有益”。</p><p>人工智能的快速发展让他更加悲观。 “重要的是哪个先到？是先实现真正的通用人工智能，还是先想办法保证它的安全？现在感觉前者发生的可能性更大了。这可能会带来很大的问题。”</p><p>但总的来说，拉塞尔感觉比十年前更加乐观。最近的进展以及顶尖科学家表达出来的担忧，比如他与其他 30000 多人在 今年3 月联署的呼吁暂停大型人工智能实验的公开信，“让政府接受了这样一个信息：关于安全，我们得做点什么。”</p><p>外星人到达的时间可能比我们之前想象的还要早。但拉塞尔对大家对这封公开信的反应感到振奋。 他说：“3月底时，人类又回到办公室了。”</p><h3>阿尔温德·纳拉亚南&amp;萨亚什·卡普尔：普林斯顿大学教授&amp;博士生</h3><p>关于人工智能的讨论可能充满了夸张的成分。那些试图兜售产品和服务的人会吹嘘人工智能系统已经能做到的事情。那些担心人工智能风险的人常常出于对尚不存在的能力的恐惧而发表看法。那些试图获得投资的人对人工智能将带来的好处赞不绝口。</p><p>2019年，普林斯顿大学计算机科学教授阿尔温德·纳拉亚南（Arvind Narayanan）做了题为“如何识破人工智能骗局”的演讲。这次演讲迅速走红：幻灯片被下载了数万次，他的推文被数百万人浏览。觉察到大家兴趣的纳拉亚南开始与他的博士生萨亚什·卡普尔 (Sayash Kapoor) 合作，写一本关于这个主题的书，该书将于 2024 年出版。</p><p><strong>问：为什么人工智能骗局如此盛行？</strong></p><p>纳拉亚南：我认为有很多因素。我首先思考的是，不要关注这些骗局的供给侧，而是要关注其需求侧。如果需求存在，自然就会有供给。如果大家不买账，那就算有人想炒也炒不起来。</p><p>那么需求从何而来？我们在书里提出的论点是，坏掉的人工智能对于坏掉的机构非常有吸引力。人力资源部门之所以迫切需要人工智能来预测哪些候选人会表现出色，是因为每个空缺职位都会收到数百甚至数千份申请。以人们希望的方式彻底做出评估的想法是行不通的。这是因为招聘流程本身似乎已经坏掉了。</p><p>我要指出的另一点是，因为从很多方面来说[机器学习]都是十分棘手的技术，它非常非常容易自欺欺人。所以很多炒作的人不仅是在愚弄别人，更是在愚弄自己。</p><p><strong>对于人工智能可能对人类构成生存风险的观点，你们有何看法？对齐是我们应该担心的问题吗？</strong></p><p>纳拉亚南：当然可能会带来生存风险。但我们对很多观点是强烈反对的。有种看法认为可以用发生概率来指导政策。但当我看了这些概率估计背后的方法之后，我觉得都是胡说八道。关于人工智能的影响，一项被广泛引用调查称，50% 的人工智能研究人员认为，生存风险至少有 10% 的可能性。但这份调查的回复率非常低，而且属于自我选择——认真对待这一风险的人会选择作答。所以存在很大的选择偏差。</p><p>也许我们应该认真对待生存风险，我对此并不否认。但正在提议的那些干预措施——要么我们应该找到一些灵丹妙药式的技术突破，要么我们应该放慢这项技术的发展速度，要么得禁止这项技术，要么将其限制在极少数公司手中——所有这些措施确实都是存在问题的。我认为对齐不会来自某种灵丹妙药式的技术解决方案，而是来自于研究坏人会如何利用人工智能来伤害他人或我们的社会，然后对所有这些攻击面进行防御。</p><h3>沙基尔·穆罕默德：Google DeepMind研究总监 &amp; Deep Learning Indaba 创始人</h3><p>自沙基尔·穆罕默德（Shakir Mohamed） 2007 年进入剑桥大学学习机器学习以来，人工智能领域已经取得了长足的进步。</p><p>2013 年，穆罕默德加入了当时位于伦敦的一家叫做 DeepMind 的小型初创公司。在那里，他开创了部分生成人工智能模型的早期研究。他的论文被引用次数达数千次，现在，他是 Google DeepMind 的研究总监。但穆罕默德的热情不仅仅局限于技术研究。穆罕默德在南非长大，他在职业生涯早期就注意到来自自己的大陆研究人员稀缺。 2017 年，他与人共同创立了 Deep Learning Indaba，旨在加强整个非洲的机器学习与人工智能。</p><p>Indaba 是讨论严肃话题的聚会，Deep Learning Indaba汇聚了非洲领先得研究人员，每年都会提供交流和学习得机会。它还为在 36 个不同国家举办的小型 IndabaX 会议提供支撑，并通过为非洲大陆最具影响力的机器学习论文提供奖项来激励研究。</p><p>因为这些会议而走到一起的一些人最终共同创立了 Masakhane 与 LelapaAI 等初创企业，开发出专门为非洲语言设计的自然语言处理软件。在东欧和拉丁美洲，机器学习研究人员也受到了 Indaba 的启发，纷纷创办自己的会议和活动，以增加中低收入国家的代表性。</p><p>穆罕默德说：“我个人的使命是致力于具有社会目的的机器学习”。他用一个例子来警告说，如果没有世界各地人民的适当代表，像人工智能这样的新技术很容易造成伤害：研究人员曾用创新的预报技术来预测 1998 年秘鲁的厄尔尼诺天气系统。但这些信息经常被渔业界误解，有时甚至被忽视掉，部分是因为科学家不知道如何跟渔民沟通。穆罕默德希望开展把当地社区的需求放在优先地位得人工智能研究项目。 他说：“这是推动我工作的价值观之一”。</p><p><strong>延伸阅读：</strong><br /><a href="https://36kr.com/p/preview/yDtih5nSwPud7LGQCxo8jPJz_giqS10tgFoci7MRqyvDIuAbrXb2KJ_eEITw0Dvg" rel="noopener noreferrer" target="_blank">《时代》人工智能百人榜（一）：领袖</a></p><p><a href="https://36kr.com/p/preview/lEp34b2Kvr0Qun8J8PYNVQlpXpC4pe2wfHgffEghiZ0Wk4TOWhbyoVx3pQfLbqaQ" rel="noopener noreferrer" target="_blank">《时代》人工智能百人榜（二）：创新者</a></p><p><a href="https://36kr.com/p/preview/HsrH3vPTWNS1KI5jYDw60JW7e4g4JLw5O-V1SnWhFftR0wpRilQloBMqaa5pm7kz" rel="noopener noreferrer" target="_blank">《时代》人工智能百人榜（三）：塑造者</a></p><p>译者：boxi。</p>
]]></content:encoded>
<pubDate>Fri, 27 Oct 2023 12:00:40 GMT</pubDate>
<pubDate>Fri, 27 Oct 2023 12:00:40 GMT</pubDate>
</item>
<item>
<title>《时代》人工智能百人榜（三）：塑造者</title>
<link>https://www.36kr.com/p/2466467309328514</link>
<guid>https://www.36kr.com/p/2466467309328514</guid>
<content:encoded><![CDATA[

<blockquote><p>神译局是36氪旗下编译团队，关注科技、商业、职场、生活等领域，重点介绍国外的新技术、新观点、新风向。</p></blockquote><p>编者按：人工智能的独特之处既最令人恐惧也最值得庆祝——一些技能堪比我们人类，然后还能更进一步，做到人类做不到的事情。模仿人类行为已成为人工智能的决定性特征。然而，机器学习和大语言模型的每一次进步背后实际上都是人——这里面既有经常被忽视的，让大语言模型使用起来更安全的人类劳动，又有就在什么时候以及如何最好地使用这项技术方面做出关键决定的个人。本文综合了各方推荐和建议，将数百项提名汇总到一起，最终形成了这份百人榜名单。从很多方面来说，这 100 人构成了推动人工智能发展的关系网络与权力中心。他们是竞争对手、监管者、科学家、艺术家、倡导者、以及高管——属于既互相竞争又共同合作的人类，他们的洞察力、欲望与缺陷将塑造一项影响力与日俱增的技术的发展方向。文章来自编译，篇幅关系，我们分四部分刊出，此为第三部分。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231011/v2_32537966f53b46669a32f8e363448f67@1694_oswg1262095oswg2000oswg1125_img_jpg?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><h2>塑造者</h2><h3>阿隆德拉·尼尔森：高等研究院人工智能组顾问</h3><p>当拜登政府去年被赋予应对快速发展的生成式人工智能的任务时，阿隆德拉·尼尔森（Alondra Nelson）率先采取了行动。去年10月发布的《人工智能权利法案蓝图》（AI Bill of Rights）就是在身为白宫科技政策办公室主任的尼尔森的管理下出台的。这项法案没有法律约束力，也缺乏强制执行手段，但它为人工智能开发者以及政策制定者确立了一个框架，她希望两者都能遵循这个框架，从而确保人工智能成为造福公众的力量。法案写道：“在美国创新力量的推动下，这些工具具备重塑社会方方面面，并让每个人的生活变得更美好的的潜力”。尼尔森希望通过这个框架能促使美国国会尽快起草和通过人工智能立法。</p><p>尼尔森还希望这份73页的文件能促使国会尽快起草和通过人工智能立法。她说：“情况已经非常紧迫。不久前，因为没能及时采取行动，社交媒体的监管已经给过我们警示。”她希望美国国会能吸取教训，在人工智能监管的问题上不要再重蹈覆辙。</p><p>进入白宫前的尼尔森拥有一份出色的履历：哥伦比亚大学及耶鲁大学教授、非营利性的社会科学研究委员会（Social Science Research Council）主席兼首席执行官，多本关于遗传学、种族与医疗歧视的书籍作者。她对这份人工智能蓝图同样严谨，十分注重细节，她和她的团队在一年的时间里曾与行业参与者、学者、高中生以及教师进行过广泛交谈。通过这些广泛的对话，她为行业参与者确定了一系列最佳实践，包括红队演练（red teaming，在公开部署人工智能系统之前对其进行压力测试）以及持续审计等。尼尔森说，这些最佳实践都是为了确保人工智能真正为公众服务，而不仅仅是为饥渴的科技行业带来福音。</p><p>好莱坞最近正在发起罢工，部分是因为人工智能将来有可能取代编剧与演员在电影制作当中所扮演的角色。类似的斗争可能还会在许多行业展开。但尼尔森警告说，大家对机器会抢走人类工作的那些最严重的担忧往往被夸大了。她以放射学为例，2016 年，人工智能先驱杰弗里·辛顿（ Geoffrey Hinton ）曾预测放射科会在五年内被人工智能取代。但现在，世界各地都存在放射科的劳动力短缺问题。</p><p>尼尔森说：“每每出现这类宣言，就是需要思考我们想要什么样的社会的时候。我经常反对用既成事实的方法来思考工作和人工智能之间的关系。我们是可以采取一些措施来降低其破坏性的。”</p><p>尼尔森已于今年二月离开白宫，但仍担任多个有影响力的职位。作为美国进步中心（Center for American Progress）的研究员，她为美国各州议员和国会议员提供有关人工智能政策的建议。作为高等研究院（Institute for Advanced Study）人工智能工作组的成员，她在行业、政策制定者和民间社会之间发挥联络作用。</p><p>尼尔森表示，鉴于欧美等地区即将举行选举，人工智能的监管问题变得更加紧迫。人工智能已经被用来针对政客的深度伪造视频，并且被游说团体当作武器更大规模地传播谎言。她警告说： “虚假信息与错误信息会会变得更加严重。我们必须系好安全带，尽量减轻其中的一些问题。”</p><h3>伊恩·霍加斯：英国前沿人工智能工作组组长</h3><p>作为一名前科技企业家与投资者，伊恩·霍加斯 (Ian Hogarth) 靠押注机器学习公司发家致富。他投资了 50 多家人工智能公司，并在过去六年每年都会与人合作发布“人工智能现状”报告。 “我与建立人工智能公司的人呆在一起让我逐渐了解了世界的发展方向，但公众却不太知情。”</p><p>今年 4 月，霍加斯为《金融时报》撰写的一篇文章广为流传。他在文中指出，企业竞相开发“上帝般的”人工智能会带来风险，在最坏的情况下，“可能会导致人类被淘汰或毁灭”。两个月后，英国政府宣布任命这位 41 岁的英国人为英国新的人工智能安全计划（现在叫做前沿人工智能工作组）的负责人。霍加斯表示，这项对人工智能安全的 1 亿英镑投资是英国更广泛计划的一部分，旨在将自己打造成全球人工智能使用规范以及人工智能系统监管的领导者。今年 11 月，该国将举办首届人工智能峰会。届时国际政策的制定者、顶级企业的首席执行官以及安全研究人员将济济一堂。</p><p>霍加斯指导下的英国人工智能特别工作组的目标是在应该政府内部建立起目前只能在产业界进行的安全研究能力。霍加斯表示，该工作组是“全球范围内在国家层面解决人工智能安全问题最重要的努力。”</p><p>霍加斯表示，该工作组将优先考虑近期风险的安全研究。 他举例说：“大家投入了大量的资本和精力来制造更强大的编码工具。这些工具会增强开发出各种网络攻击的潜力。这个问题的风险正日益增加，也是我们要研究的问题。”它还表示，考虑到人工智能让设计和合成危险病原体变得更容易的风险，生物安全是另一个重点领域。</p><p>虽然 1 亿英镑听起来不少，但与领先的人工智能公司的预算相比，还是小巫见大巫。 今年4月，OpenAI 在最新一轮融资中又筹集了 3 亿美元，令其总估值达到 287 亿美元。 2022 年，谷歌在研发上的支出为 395 亿美元。全球领先的机器学习研究人员的年薪可达数百万美元。霍加斯承认，在训练“基础”人工智能模型的能力方面，英国不可能与科技巨头竞争——“光是一次训练就得花费 1 亿美元”——但他相信，这样一笔政府预算仍有可能给安全研究做出有意义的贡献， 因为这类研究的“资本密集程度要低得多”。</p><p>安全研究仍然主要在人工智能公司内部进行的另一个原因，是人工智能实验室往往会保护其最强大模型的机密“权重”与训练数据集，部分是因为这些属于很容易复制的商业秘密，但也是因为确实担心扩散造成的危险。为了开展与行业研究人员相同的水准的安全工作，霍加斯的工作组需要确保能安全访问那些数据。但目前还不清楚该工作组能否拿到。今年 6 月，英国首相里希·苏纳克宣布， OpenAI、谷歌 DeepMind 以及 Anthropic这三家全球领先的人工智能实验室已承诺让英国“尽早或优先访问”其模型，以用于安全研究目的。但两个月后霍加斯在接受采访时拒绝透露英国政府已获得了何种类型的访问权限（如果有的话）的详细信息。</p><p>不过，霍加斯表示，在吸引人才方面，预算以及大型科技公司的竞争都不是障碍。 他说：“有一个研究人员群体一直在等待提供此类公共服务的机会”。值得注意的是，霍加斯承担这项工作是不领薪水的，因为他认为这是一项“重要的公共服务”。霍加斯表示，较低的工资并没有阻止许多出色的机器学习研究人员加入工作组。</p><h3>梅雷迪思·惠特克：Signal 总裁</h3><p>大多数人把这波人工智能炒作周期的开始日期定为 2022 年底，那是 OpenAI 发布 ChatGPT 的日子。但在梅雷迪思·惠特克 (Meredith Whittaker) 看了，这一切要从 2013 年左右开始算起。那时候，她还在谷歌工作，得以从内部见证了公司将重心转向机器学习、收购 DeepMind 并为工程师提供重新培训的课程。惠特克控制着一大笔钱，有人向建议她建立一个机器学习系统来预测未来的种族灭绝。就在那一刻，她得到了一个启示： “我当时在想，你该怎么定义种族灭绝？你该怎么告诉系统种族灭绝是什么样的？”她想象用草率的数据喂给这样一个所谓的“智能”人工智能可能会带来的危害。 “我当时想，一旦系统产生影响时会发生什么？”</p><p>那时候正值爱德华·斯诺登(Edward Snowden)披露美国政府在收集新兴的科技公司产生的大量互联网使用数据，这让惠特克掉进了一个兔子洞。她开始学习人工智能，并很快意识到这个东西依赖于海量的数据和计算能力，这些只有最富有的科技公司才能获得。她说：“在某一刻我认识到，监控商业模式的赢家通过资源整合，把数据密集型、计算密集型的技术重新包装成‘智能’，这样他们就可以进入任何想进入的市场”。</p><p>在谷歌 13 年职业生涯即将结束时，惠特克帮助领导了大规模的内部抗议活动，反对涉嫌工作场所性骚扰，以及谷歌为美国军方提供的人工智能工作。尽管谷歌在这两个问题上都同意了员工的要求，但惠特克还是在 2019 年辞职了，她说该公司对她的行动主义进行了报复，并迫使她放弃与人工智能道德相关的责任。她后来到美国联邦贸易委员会任职，为主席莉娜·汗（Lina Khan）就企业权力集中与人工智能危害之间的联系提供咨询。 2022 年，她成为了负责监管加密的私密聊天应用 Signal的基金会主席。</p><p>这项工作不是你想象的那个样子，其实它与人工智能世界关系很大。科技公司（通常在未经其创建者同意的情况下）从互联网上抓取数十亿张图像和文本，靠收集的大量数据来训练自己的人工智能系统。这可能包括你的个人资料图片、餐厅评论、推文或对维基百科的编辑。还包括数千名出版作家的书籍以及无数专业与业余艺术家的作品。 惠特克说：“数亿人的意义建构、创造性的劳动与工作被剥夺、聚合，并用来创建破坏了我们生计以及我们为彼此建立的相互理解的生态体系的系统，这根本是不道德的”。她补充道，今天的人工智能“依赖于各种免费的公共劳动力，数十亿小时的时间现在集中在少数公司手中，然后这些公司又以某种方式将其洗白为‘智能’，这进一步加剧了信息和权力的不对称。”</p><p>Signal 的加密聊天服务可以防止文本和数据被预期接收者以外的任何人拦截，这是朝着替代系统迈出的一小步，但具有象征意义。惠特克表示：“隐私问题与人工智能问题的维恩图是一个圆圈。Signal 正在科技行业生态体系内反对这个体系。它试图做出某个破坏数据管道的东西。”</p><h3>詹姆斯·马尼卡：谷歌高级副总裁</h3><p>像谷歌负责研究、技术与社会的高级副总裁詹姆斯·马尼卡（James Manyika）那样拥有广泛的人脉网络或人工智能经验的人工智能领导者很少。这位牛津大学人工智能博士曾担任奥巴马政府的技术顾问，并且是美国宇航局喷气推进实验室的访问科学家，麦肯锡的顾问。现在，58 岁的马尼卡还担任着负责制定美国人工智能监管战略的美国国家人工智能咨询委员会（National AI Advisory Committee）的副主席。</p><p>马尼卡表示：“我的职业生涯以及我所做的各种不同的事情有一条主题线，一方面是如何利用技术的可能性来造福世界各地从东帕洛阿尔托到津巴布韦的人们，但同时，也要认真思考这些技术对社会的影响如何。”</p><p>在谷歌，马尼卡对公司的人工智能研究和产品拥有广泛权限，涉及到气候变化、医疗保健、娱乐等领域。其中一个人工智能项目是追踪从美国加州到澳大利亚的野火。另一个预测脆弱地区洪水风险的Flood Hub，今年该项目已扩大到为80个国家和数亿人提供服务。</p><p>尽管马尼卡对人工智能的能力充满热情，但他也对存在的风险提出了警告。今年5月，人工智能安全中心（Center for AI Safety）就人工智能的风险发明了声明，称人类存在“因人工智能而灭绝”的可能性。马尼卡也签署了这项声明，他表示，自己在罗德西亚（现在的津巴布韦）类似种族隔离的制度下长大的经历对他如何对待人工智能以及确定应该防止哪些情况产生了“深远的影响”。</p><p>马尼卡表示，他的首要任务是发展产品的安全性，并对其局限性保持透明——即即便这意味着谷歌有时候会落后于其他人工智能竞争对手。 他说：“有人走得早一点，有人走得晚一点，有人会走得快一点。但对我来说，唯一要比的是把事情做对。”</p><h3>杰克·克拉克：Anthropic 联合创始人，政策负责人</h3><p>今年夏天，杰克·克拉克 (Jack Clark) 在Anthropic 的几位同事的陪同下，前往旧金山的一家电影院观看《奥本海默》。 “在他们谈论是否要用炸弹点燃气氛的场景中，有很多我称之为紧张的笑声，”克拉克说。</p><p>克拉克是这家全球领先的人工智能实验室之一的联合创始人，同时也是公司政策的负责人，他正在努力解决人工智能会如何颠覆地缘政治力量和人类机构以及如何应对的问题。在联合国安理会的演讲以及与全球领导人的讨论中，克拉克主张各国政府应剥夺目前人工智能公司手上拥有的大部分决策权，并交到最终对政府负责的机构手中。他说，之前的努力（比如 Meta 的 Facebook 监督委员会）因为缺乏行动力而失败了。 “在我看来，感觉就像你需要的[监督机构]是你害怕的东西。基本上，这意味着你需要让政府形成真正的硬能力。”</p><p>克里斯托弗·诺兰 (Christopher Nolan) 的那部传记电影讲述了自身存在缺陷的原子弹制造者罗伯特·奥本海默 (J. Robert Oppenheimer)。战后他对军备控制方面付出的努力以失败告终，这一点引起了克拉克的共鸣。他说： “《奥本海默》这部电影我真正喜欢的地方在于它揭示了政治内斗的规模之庞大。技术专家花了很多时间去思考这个东西的设计和开发，之后大家之所以愿意听你的，是因为你是建造者。但《奥本海默》给人的其中一个教训是：你是可以造出原子弹，但你可能会输掉更大的政治游戏。其后果可能是别人会以你未必喜欢的疯狂方式使用原子弹。”</p><h3>安娜·马坎朱：OpenAI负责全球事务的副总裁</h3><p>未来几年，不管全球各地出现什么样的人工智能监管政策，安娜·马坎朱 (Anna Makanju) 很有可能都会在其中留下自己的印记。 47 岁的马坎朱是 OpenAI 负责全球事务副总裁。这家公司已经是人工智能领域的领导者，并自我定位为业界善意监管的最重要推动者之一。在过去一年的时间里，马坎朱与 OpenAI 首席执行官山姆·阿尔特曼（Sam Altman）一直在世界各地旅行，与领导人会面，并就如何应对这项新兴技术提供建议。马坎朱说：“既要确保创新仍然可行，又要有确保创新顺利进行所需的护栏，每个人都在努力实现这种平衡。”</p><p>2021 年 9 月加入 OpenAI的马坎朱拥有丰富的政策经验可供借鉴。她曾在奥巴马政府期间担任副总统乔·拜登的政策顾问，还曾在联合国、北约和 Facebook 工作过。尽管 OpenAI 声称支持给人工智能领域的护栏，但关于 OpenAI 对监管的遵从意愿如何的问题仍然存在。马坎朱在接受采访时回应了这些批评，并谈到了前进的道路。</p><p><strong>问：在与世界各地人工智能领导人会面的过程中，你了解到了什么？</strong></p><p>马坎朱：其实大家对这个问题的想法很相似，这一点很令人吃惊。基本上大家对人工智能带来的社会利益最大化并确保有护栏都感到十分兴奋。不过对于这意味着什么或应该怎么做大家有不同的看法。</p><p><strong>科技与加密货币领域的很多领导者都在努力讨好政府，好让他们基本上能够进行自我监管。为什么我们应该相信 OpenAI 与阿尔特曼呼吁进行监管是出于真心的？</strong></p><p>让监管确保这项技术真正惠及每个人是我们履行使命的重要组成部分。我们的做法，以及我在约两年前接受这份工作时的想法，就是尽可能地开展协作。</p><p>山姆·阿尔特曼向国会作证也许是最明显的例子，但我们从一开始就是这样做的。我们一直在向监管机构与政策制定者表示：“这就是我们所认为的技术的发展方向。我们真心希望成为帮助你尽可能了解这项技术的合作伙伴，从而帮助为这项技术建立规则和基础设施，确保人工智能让所有人都能受益。”</p><p><strong>为什么Anthropic&nbsp;要与谷歌、微软一起帮助成立前沿模型论坛（Frontier Model Forum）？</strong></p><p>对于整个行业的许多特定风险以及常见的安全干预措施，目前我们甚至还缺乏共同语言。所有的实验室都在做这方面事情。对于安全最佳实践，政策制定者要求我们采用行业级的做法，而我们确实在利用最好的技术专业知识来确定什么是最佳方法。</p><h3>奥马尔·阿尔·奥拉马：阿联酋人工智能国务大臣</h3><p>2017 年，奥马尔·阿尔·奥拉马 (Omar Al Olama) 走马上任，成为全球第一位负责人工智能的部长。这位33 岁的阿联酋人工智能国务大臣表示：“得有人从整体得视角去审视人工智能及其在政府的应用，并确保不同机构之间至少存在某种形式的协调。”</p><p>这个领域最有影响力的玩家——美国、欧盟和中国——尚未注意到这个来自海湾地区得榜样。人工智能通常属于数字或科技部长的职权范围（欧盟就是这样），到目前为止，全球关于人工智能及其监管的对话一直都是支离破碎的。尽管阿尔·奥拉马表示，采用不同的做法对于避免陷入群体思维是“健康的”，但各国必须愿意在一个包容性的多边论坛中共同努力。</p><p>他警告说：“建立孤岛，尤其是像人工智能这样深奥的技术形成孤岛，是自找麻烦。”</p><p><strong>问：你是阿联酋首任人工智能大臣，你能否先谈谈这个角色是怎么产生的？</strong></p><p>奥拉马：据我们所知，这不是什么新技术，它已经有 50 多年的历史。但社交媒体、自动驾驶汽车的出现，以及能够完成曾经看似不可能的事情，这些向我们展示了这项技术的发展轨迹。阿联酋的领导层相信，我们不需要等着这项技术作为其他人决策的副产品来到我们身边，然后对本质上陌生的东西做出反应，而是可以成为准备最充分的国家，去迎接人工智能的积极面与消极面。这就是设立这个角色的原因。</p><p>一开始我认为阿联酋可以做很多事情，而且可以自己做。不过，时至今日，我 100% 确定你没法在孤岛上管理这项技术。你不能自己一个人做。你必须跟其他人一起做。我们必须用一种非政治性的、真正全球性的方式来做这件事。因此，需要签署一项把所有人纳入在内的全球条约。</p><p><strong>人工智能的主要参与者在监管方面采取了不同的方法。你对此有何看法？你认为阿联酋扮演了什么样的角色？</strong></p><p>事实上，我认为大家采取不同做法是非常健康的。欧盟走自己的路，中国走自己的路，美国走自己路，这是非常健康的。因为我觉得没人第一次就能做对。但是，如果我们有不同的模式，我们可以从中抽取一些元素并模仿有效的做法，同时确保我们可以研究所有不同的模式，而不是陷入到群体思维，朝着一个方向发展。</p><p>阿联酋处理和管理人工智能的模式也很有趣：我们知道自己的独特性。我们是一个中等规模的国家；我们投资人工智能已经有好几年了。所以我们在这个领域比很多其他国家是领先的。我们没法跟中国和美国竞争；我认为我们永远都不会有跟别人争的想法。我们的工作首先是成为人工智能的推动者，并利用我们的优势来支持所有参与者。因此，当我们的政策制定以及政府很灵活并且能够迅速采取行动时，我们希望确保这一点，也就是只要有人想在监管环境下部署人工智能时，都能先想到阿联酋，之后，我们还可以将自己的发现输出到全世界的其他地方。因此，从这个意义上说，我们在这项技术方面与那些大玩家形成了互补。</p><p><strong>说到人工智能带来的机遇和挑战，各国需要讨论的最重要的优先事项是什么？这些对话的紧迫性如何？</strong></p><p>人工智能不是一种技术，每种技术都有不同的用途。自动驾驶汽车与大语言模型有很大不同。在治理人工智能或展望人工智能的未来时，第一个问题是大家实际上把它们全部看作了一项技术，并认为我们对所有问题的答案都是一样的。</p><p>我们需要确定的第二样东西是人工智能的颠覆能力——不管是积极的还是消极的。这种能力因国家而异。比方说，阿联酋推自动驾驶汽车会带来积极影响而不是消极影响，因为我们的基础设施非常先进而且是新的。相比之下，比方说，一个拥有超过 100 万卡车司机且基础设施尚未准备好承载此类技术的国家的情况则不一样。</p><p>与此同时，对于即将举行选举的国家来说，传播虚假信息的大语言模型和技术会是我们首先组要解决的问题。第一，我们需要设置一定的护栏。我们还需要就这方面的研究进行全球对话，并进行全球性的脉搏检查，这项研究正在进行中，而且需要是非政治性的。因此，中国科学家需要与美国科学家交谈，美国科学家需要与阿联酋科学家交谈。它需要是一个类似于联合国的机构，我们要了解边界在哪里以及能力怎样。因为我们今天面临的一个问题是，与政府和善意的参与者相比，恶意的参与者总是更加敏捷，总是处在最前沿。</p><p><strong>政府设立专门的人工智能部长有多重要？</strong></p><p>人工智能技术的影响太深远了，以至于我们其实可以以史为鉴来理解为什么我们需要为其设立一个部门。人类过去靠煤炭和木火获取能源时还没有能源部。等到确保能源的生产和能源的分配变得至关重要时，各国政府都任命了自己的能源部长。电信领域也发生了同样的情况。我确信人工智能也将一样。</p><h3>凯莉·麦克南：艺术家</h3><p>去年，视觉艺术家凯莉·麦克南 (Kelly McKernan) 开始在 Twitter 上看到一些看似自己艺术作品但其实不是的图像。 麦克南说：“我感觉那些就好像是我脑海里面未尚未完成的草图，我甚至都还没画出来。这确实很令人不安。”</p><p>37 岁的麦克南很快发现，全球各地的人都在用自己的名字作为关键词来提示人工智能的文本生成图像模型，比方说 Midjourney 以及 Stable Diffusion。这些工具可以马上创作出麦克南那种梦幻、科幻风格的图像。光是在 Midjourney 的Discord 服务器上，就有超过 11000 张按照麦克南风格创作的图像 ——但所有这些都艺术家未经本人同意或输入。</p><p>随着人工智能图像生成工具的流行，麦克南还发现自己的自由职业机会正在慢慢消失。此前，麦克南每个月都能拿得到几分零工，比如为自出版作家创作书籍封面，或为纳什维尔当地音乐家创作专辑封面。但这些机会正在枯竭，而收入似乎流向了利用艺术家的创作训练模型的人工智能公司那里。艺术家表示：“有人不用雇我，而是打开程序，只用我的名字就能模仿出足够接近、足够好的东西，而价格却只用一点点，知道这个会让人抓狂。”</p><p>于是，今年一月，麦克南与艺术家莎拉·安德森 (Sarah Andersen) 以及卡拉·奥尔蒂斯 (Karla Ortiz) 一起加入了一场针对 Midjourney、Stability AI 与 DeviantArt 的集体诉讼。艺术家们指控后者侵犯版权并要求为自己的作品付费。他们也不是唯一对人工智能这个新领域发起法律斗争的人。 Getty Images 也起诉 Stable Diffusion 的“无耻侵权”。 萨拉·斯尔夫曼（Sarah Silverman） 率先对 ChatGPT 开发商 OpenAI 提起诉讼，称其在未经同意、标注或补偿的情况下用她的书来训练人工智能模型。控辩双方在法庭上将面临激烈斗争，一名法官已经对麦克南案的合理性表示怀疑。但如果他们获胜的话，结果可能对保护整个创意阶层遭受灭顶之灾至关重要。</p><p>麦克南说：“这些公司同无偿使用我们的劳动获取了巨额利润。概念艺术家、插画家、平面设计师、库存艺术家被裁员，被那些声称要转向人工智能的公司解雇。这是一场生存危机。”</p><h3>埃里克·施密特：Schmidt Futures联合创始人</h3><p>2001 年至 2011 年间，埃里克·施密特 (Eric Schmidt)&nbsp; 曾担任谷歌的首席执行官，过去这十年里，作为硅谷与行动迟缓的美国国家安全界之间的联络人，他一直推动要提高人工智能研发的紧迫性。他是聚焦科技的慈善项目Schmidt Futures的联合创始人，最近还担任了美国国家人工智能安全委员会（National Security Commission on Artificial Intelligence）的主席。</p><p>施密特认为美国在与中国的人工智能竞赛岁处于领先但地位并不稳定，并介绍了他自己的生活当中是使用人工智能工具的，还解释了我们尚未未这项技术改变战争、科学、政治与社会本身做好准备的原因。</p><p><strong>人工智能的现状与你五年前的预测有何不同？</strong></p><p>埃里克·施密特：我认为科技行业很擅长预测宏大主题，但很不擅长准确预测那是如何发生的。问题是，接下来的发现是什么？我曾经说过，[通用智能]可以在 20 年内实现。现在已经过了10年。现在我所看到的创新水平比我这辈子见过的都要强几个数量级。我经历过分时、 PC 行业、web革命、Unix 革命、Linux、Facebook 以及 Google。但这个的增长速度比所有这些的增长总和还要快。</p><p><strong>人工智能对政治以及即将到来的 2024 年的选举有何影响？</strong></p><p>我认为 2024 年的选举会非常艰难。社交媒体公司还没有做好准备。假视频、假图片、语音播报泛滥——我觉得我们还没有做好准备。解决方案很简单。你需要知道用户是谁，内容来自哪里。然后就可以弄清楚谁往平台塞了这些可怕的东西。</p><p><strong>你在生活当中是怎么使用人工智能工具的？</strong></p><p>我必须给拜登总统写一份关于人工智能的备忘录，但我写得不太好。所以我先写份草稿，然后把它发给 GPT-4，让GPT-4 重写。我的提示是“重写这份备忘录，不要改变里面的数学。”我意识到我想要一个能让我所做的一切都变得更好的系统：如果我想唱歌，它能让我唱得更好；如果我想写东西，它能让我写得更好；如果我想选择读点什么或其他娱乐方式，我希望它能给出推荐。我想要一个非常成熟的个人数字伴侣，但必须是由我控制的。比方说，收到冗长的备忘录后，我希望它能对内容做出基本总结：“这部分是重复的，你已经读过这个，你不需要再读一遍。”这个对我来说价值相当高……只用让我把正在做的事情做得更好。</p><p><strong>就人工智能能力而言，美国相对于中国处于什么水平？</strong></p><p>三年来第第一次来到中国，我的判断是在这些技术上目前他们落后美国两到三年。他们已经表明了要追赶上来的强烈目标。如果我们停止奔跑，他们就会追上来，然后我们就会非常不高兴。</p><h3>玛格丽特·维斯塔格：欧盟委员会执行副主席</h3><p>作为欧洲的数字沙皇，玛格丽特·维斯塔格（Margrethe Vestager）享有盛誉。对于全球最大科技巨头违反反垄断规则的行为，这位 55 岁的丹麦人罕见地表现出要进行调查甚至罚款的意愿。现在，她把目光投向了一个更大的挑战：保护欧洲的价值观免受人工智能的风险。</p><p>在维斯塔格看来，人工智能具有巨大潜力，可以释放新一轮生产力浪潮，并为气候变化和医疗保健等社会挑战提供解决方案。但也存在重大风险，尤其是人工智能驱动的虚假信息的兴起可能会导致人们“完全不再相信任何事情。这并不是说他们会相信一些令人愤慨或虚假的事情。更重要的是：你不再相信你所看到的东西，因为它可能是假的。”</p><p>为此，过去两年，维斯塔格牵头制定了《欧盟人工智能法案》。这项法案如果获得通过的话，将成为全球首部全面的人工智能法律。维斯塔格表示，欧盟提议的规则将禁止在公共场合将人工智能用于有争议的用途，比方说社会评分以及面部识别，此举是遵循“基于风险的方法”，确保其使用符合欧洲的价值观，同时仍鼓励创新。</p><p>虽然欧盟希望在今年年底前就立法达成最终协议，但维斯塔格表示，根本而言人工智能监管属于全球事务，并呼吁制定“国际行为准则”以跟上其快速发展的步伐。欧盟的人工智能法也许是维斯塔格对人工智能做出的最后贡献。 目前她正在集中精力竞选欧洲投资银行主席。如果竞选成功的话，她将于 2024 年 1 月就任新职务。</p><h3>安娜·埃舒：美国国会议员</h3><p>1992 年，加州民主党众议员安娜·埃舒 (Anna Eshoo) 首次当选国会议员时，互联网才刚刚成为主流。那时候就算立法者听说过人工智能，那也不是什么积极的事情。 埃舒表示：“当时这还很难接受人工智能。只要一提到这个东西，就会觉得很威胁性，而且会破坏就业机会。”</p><p>三十年后，人工智能似乎成为人人都可以谈论的话题，而在国会山这里，现年 80 岁的 埃舒往往是这一话题的引领者。自今年1月份以来，她就一直担任美国国会人工智能小组（Congressional AI Caucus）的联合主席，这是一个跨党派团体，致力于教育政策制定者了解人工智能对技术、经济和社会的影响。她表示，她的主要重点是召集专家，帮助政策制定者制定美国监管人工智能的路线图，今年 5 月，她还听取了 OpenAI 首席执行官山姆·阿尔特曼、Anthropic 联合创始人杰克·克拉克等科技领袖所做的简报。</p><p>埃舒还一直在制定监管人工智能的法律。人工智能 ChatGPT 的一夜成名对政策制定者产生量很大的触动，掀起了一股起草新人工智能法律的狂热，其中包括她与众议员刘云平（Ted Lieu）以及肯·巴克（Ken Buck）今年 6 月共同提出的要成立一个监管人工智能的国家委员会的议案。拟议中的这家“公共可信机构”将汇集来自民间社会、政府、行业和劳工组织的专家，其任务是提出建议并制定人工智能监管的全面框架。</p><p>她今年还提出立法，要美国战略准备和响应管理局研究人工智能对美国生物安全造成的潜在威胁，其中包括恶意行为者如何利用人工智能来开发 SARS-CoV 或埃博拉病毒等新型病原体等。另外，她还打算提出一项建立美国国家人工智能研究资源的法案，目标是提供开发安全和值得信赖的人工智能所需的数据和工具，并要求大型社交媒体平台对自家算法极化导致线下暴力的有害内容负责。</p><p>人工智能监管仍处于早期阶段，但埃舒认为立法者已就制定政策框架达成共识：“人工智能前景广阔，我们希望创新及其承诺得以实现，但我们也需要解决专家们提出的风险问题。”</p><h3>理查德·马腾格：内容审核员联盟组织者</h3><p>与 ChatGPT 得互动会让人感觉很神奇。这个聊天机器人很友好、乐于助人。如果你曾试过让它生成暴力、仇恨或露骨的色情内容，你就会想让它上钩并不容易。</p><p>这并非偶然。时间倒回到ChatGPT 一炮走红的前一年，肯尼亚得一支团队就被征召来帮助解决它的缺陷了。问题是这个：OpenAI 的大语言模型经常会出现种族主义等不当类型的内容。 （这是接受来自互联网的大量文本训练的副作用，因为这个地方往往存在令人讨厌的东西。）为了帮助 OpenAI 解决这个问题，外包公司 Sama 的理查德·马腾格（Richard Mathenge） 和他的数十名同事用了几个月的时间审查了有害内容的例子（比如仇恨言论、暴力内容等）并对其进行分类，去训练 ChatGPT 避免鹦鹉学舌。这项时薪不到 2 美元的工作造成了伤害。38 岁的马滕格说： “我们要应对严重的心理创伤”。</p><p>今年 5 月，作为 150 名非洲人工智能工作者的一员，马滕格投票建立了非洲的第一个内容审核员联盟，这是一项跨公司的努力，旨在为肯尼亚（此类外包工作的中心）替科技巨头打工的工作者赢得更好的工作条件。 今年7 月，马滕格与 ChatGPT 的三名前同事向肯尼亚议会提交了一份请愿书，呼吁立法者调查大型科技公司在该国的外包做法，并立法实施救济。马森格说：“我们有义务接触议会，”因为人工智能背后的人类工作者“被当作垃圾”。</p><h3>斯内哈·雷瓦努尔： Encode Justice 创始人，负责人</h3><p>今年早些时候，斯内哈·雷瓦努尔（Sneha Revanur） 开始注意到自己的朋友圈出现了一个新趋势：“就像 Google 已经成为大家普遍接受的动词一样，ChatGPT 也逐渐成为我们的日常词汇。”那时候她还是一名大学新生，她注意到，不管是起草给教授的电子邮件，还是撰写分手的短信，她的同龄人似乎都借助了这个聊天机器人的帮忙。</p><p>在雷瓦努尔看来，Z 世代（一般是指1997 年至 2012 年间出生的人）采用生成式人工智能工具的速度如此迅速并不奇怪，18 岁的她属于“从出生的第一天起”就沉浸在技术之中的一代人。她们在监管方面也有发言权才说得过去。</p><p>雷瓦努尔对监管人工智能的兴趣始于 2020 年。那一年，她成立了 Encode Justice，这是一个由年轻人领导、专注于人工智能的民间社会组织，旨在动员她家乡加州的年轻一代反对 25 号提案，这是一项旨在用基于风险的算法取代现金保释金的提案。该提案被否决后，这家组织继续开展工作，重点关注对同行进行人工智能政策倡导方面的教育和动员。这个组织目前在全球 30 个国家拥有 800 名年轻成员，被拿来跟之前由青年领导的气候和枪支控制运动相提并论。</p><p>她说：“我们这一代人会继承[开发者]用以极快速度所开发技术的影响。”她称美国联邦政府在控制社交媒体巨头方面的惰性对人工智能是个警示。 “即便当时对年轻人和各种社区的影响都已得到充分记录，[立法者]用了几十年的时间才认真对待社交媒体，并真正开始采取监管行动。”</p><p>在人工智能行业众人的敦促下，美国政府此次似乎行动颇为迅速。今年夏天，雷瓦努尔等人发表公开信，敦促美国国会领导人以及白宫科技政策办公室让更多年轻人加入到人工智能监督与咨询事务。不久之后，她受邀参加了美国副总统卡马拉·哈里斯主持的人工智能圆桌讨论。 雷瓦努尔说：“在人工智能监管及了解其对社会的影响方面，年轻人第一次被当作关键的利益相关者看待。我们是下一代的用户、消费者、倡导者与开发者，谈判桌上应该有我们的一席之地。”</p><h3>特利斯坦·哈里斯：Center for Humane Technology 联合创始人，执行理事</h3><p>你不大可能有在爱沙尼亚实施谋杀的打算。但如果你确实有此打算，人工智能系统肯定会让你受益，因为它能看完并理解爱沙尼亚的所有法律，并从中找出漏洞，让你脱罪。这种情形让特里斯坦·哈里斯（Tristan Harris）彻夜难眠。事实上，人工智能领域蓬勃发展的时期有很多事情让哈里斯彻夜难眠。</p><p>哈里斯是人道技术中心（Center for Humane Technology）的联合创始人，也是播客《Your Undivided Attention》的主持人，他一直把自己塑造为站在人工智能堡垒上的人物——一位致力于确保我们正确使用这一代人工智能的技术领导者，因为犯错的代价我们根本承受不起。他说： “我们必须公开对话，评估人工智能会对社会产生什么影响。因为它确实会随时随地影响到一切。”</p><p>当然，这不是我们与人工智能的第一次接触。我们的第一次接触是在本世纪的头 20 年，社交媒体与推荐引擎开始扩散，将人们的眼球和思想都引导到往往很可怕甚至是危险的内容上。哈里斯说： “这是一场通往脑干底部的竞赛。人们被引导到那些导致成瘾、孤立、心理健康问题、网络欺凌、骚扰等有害内容上。”所有这些都是 2020 年的那部纪录片《社会困境》的主题，哈里斯在里面也有露面。</p><p>这种混乱可以说是人工智能 1.0 时代造成的。到了人工智能2.0时代，危险更大。生成式人工智能，比如 ChatGPT，牵涉到从无到有地创造语言——而语言的类型有无数种。</p><p>哈里斯说：“化学是语言，生物学是语言，法律是语言，宗教是语言。我可以用天花的语言，并让它朝着更具传播性和致命性的方向发展。有人可以命令人工智能系统，‘给我写一封信，去鼓励这个孩子自杀。’人工智能可以用假新闻、假法律或假宗教文件的形式吐出语言。”</p><p>放纵的野心对这一切起到了涡轮增压的作用。开发者、公司、国家正在展开竞赛，看看谁能够开发出最好、最强大、最赚钱的系统。哈里斯说，一个贴切的比方是开发第一批核武器的竞赛。 哈里斯说：“设计师们担心，如果自己不造，那些骨子里坏得多的人就会去造。所以他们的想法是，‘让好人来造（核武器）。’”</p><p>当然，问题在于，虽然坏人无处不在，但我们都觉得自己是好人。今年 3 月，哈里斯发布了一封由包括埃隆·马斯克、苹果联合创始人史蒂夫·沃兹尼亚克、人工智能研究机构 Mila 创始人约书亚·本希奥（Yoshua Bengio）在内的众多科技领域领导人联署的公开信，呼吁所有人工智能实验室暂停工作六个月，并利用这段时间重新评估行业的方向。没人响应，不过哈里斯对此并不感到惊讶。但是，他希望这至少能让他们思考一下。</p><p>他说：“人工智能确实有很多好处，但问题、灾难性风险也形影不离。”</p><h3>乔伊·布拉姆维尼：Algorithmic Justice League 创始人，首席艺术家</h3><p>在拜登政府针对快速发展的人工智能技术努力建立护栏之际，今年6月，乔伊·布拉姆维尼（Joy Buolamwini） 与美国总统一起参加了一次闭门圆桌会议。算法正义联盟 (Algorithmic Justice League，AJL) 的这位创始人对已经在警务、教育及医疗保健用到的面部识别和生物识别技术表示担忧。</p><p>Buolamwini 是一位加纳、美国、加拿大裔计算机科学家，数字活动家，她在 2016 年创立了 AJL。这个组织的总部位于马萨诸塞州剑桥市，旨在利用研究和艺术突出人工智能的社会影响与潜在危害。</p><p>布拉姆维尼最近写了一本新书，名字叫做《Unmasking AI: My Mission to Protect What Is Human in a World of Machines》。这本书展示了种族主义、性别歧视、肤色歧视以及体能歧视如何导致许多人代表名额不足，并导致算法在创建过程中容易受到偏见的影响。</p><h3>埃利泽·尤德科斯基：Machine Intelligence Research Institute联合创始人</h3><p>今年 2 月，OpenAI 首席执行官山姆·阿尔特曼发了一张自拍照，里面还有另外两个人，分别是音乐家格莱姆斯（Grimes）以及备受争议的人工智能理论家埃利泽·尤德科斯基（Eliezer Yudkowsky）。格莱姆斯跟埃隆·马斯克有过恋爱关系，但阿尔特曼跟尤德科斯基似乎不大可能凑在一起。 阿尔特曼的公司已经开发出目前最强大、最通用的人工智能系统之一——GPT-4。而尤德科斯基这二十多年则一直在发出警告，说强大的人工智能系统可能会杀死全人类，而且可能性极大。</p><p>尤德科斯基是一位没有上过高中或大学的决策理论家，也是人工智能对齐领域的创始人之一，对齐的目标是确保人工智能系统遵循创建者的意愿，以此防止类似终结者的场景出现。他在 2000 年创立了人工智能奇点研究所（Singularity Institute for Artificial Intelligence），后来又更名为机器智能研究所 (MIRI)。同时，他还在自己于 2009 年创立的社区博客 LessWrong 上撰写了数百篇关于人工智能危险的文章。</p><p>不过，去年尤德科夫斯基却承认自己失败了。 2022 年 4 月 1 日，他宣布 MIRI 将使命改为“有尊严地死去”，并估计人类生存的机会为 0%。这不是愚人节玩笑：MIRI 都不高退休金计划了，因为，他们认为人工智能将“对人类的未来的颠覆性太大了，以至于传统退休规划已变得毫无意义。”</p><p>由于尤德科斯基认为我们没法安全地制作出人工智能系统，因此他决定的时间花在告诫不要造人工智能系统上。</p><p>从那时起，尤德科夫斯基就开始了一股媒体热潮，出现在更多的播客之中，包括由德州共和党众议员丹·克伦肖（Dan Crenshaw）主持的“Hold These Truths”之中，还在TED 上发表了自己的演讲。尽管他对关于人工智能的忧虑正在逐步进入主流稍微感到乐观，但仍相信人工智能有 99% 的可能性会消灭全人类。</p><h3>维里蒂·哈丁：剑桥大学人工智能与地缘政治项目主任</h3><p>关于新兴技术与民主关系，很少有人的了解程度能比得上维里蒂·哈丁（Verity Harding）。这位由英国政客转型而来人工智能专家十多年来一直专注于变革性技术及其对民主社会的影响。她曾担任英国前副首相尼克·克莱格（Nick Clegg）的特别顾问（致力于数字隐私问题），之后又在谷歌工作过，后来又加入到全球领先的人工智能实验室之一 DeepMind，与人共同创立了研究与道德部门，并领导制定全球政策。</p><p>她说，从政治转向科技是为了提高效率。哈丁表示：“民选代表、科技公司以及安全部门对技术的了解与理解程度存在巨大的赤字。”现在，她正在领导一场范畴更广的运动，追求“基于权利”进行人工智能治理，对象不仅包括开发人工智能的人，也包括那些受到人工智能影响最大的人。她目前是咨询公司 Formation Advisory 的创始人，也是剑桥大学人工智能与地缘政治项目的主任。这个项目旨在通过鼓励全球合作，建立新的框架，提供“人工智能军备竞赛”这种叙事的替代方案。</p><p>哈丁说：“人工智能太重要了，不能只是人工智能社区的事情。它需要更广泛的参与。”</p><p>哈丁认为，避免纯粹自上而下的监管至关重要，而且采取更具协作性和全球性的方法是可能的。部分是因为之前已经这样做过。在她即将出版的《人工智能需要你：我们如何可以改变人工智能的未来并拯救我们自己的未来》（AI Needs You: How We Can Change AI’s Future and Save Our Own）一书里，哈丁援引了早期的三场技术革命——太空竞赛、试管受精与胚胎学研究以及互联网——并介绍它们为全球范围内应对新兴的、不确定的技术所提供的经验教训。全球在1967 年达成了共识，《联合国外层空间条约》构成了当今国际空间法的基础。类似地，哈丁希望在人工智能领域也能实现类似的多边努力，尽管当前中美两国之间存在地缘政治紧张局势。</p><h3>刘云平：美国国会议员</h3><p>美国众议员刘云平 (Ted Lieu) 今年创造了历史。今年1月，他推出了首部由人工智能撰写的联邦立法。通过给 ChatGPT 提供提示，他让后者用自己的风格和声音撰写了一份全面的国会决议，表示支持国会对人工智能进行监管。结果人工智能生成了一份他甚至不需要编辑的决议。 这位加州民主党人在国会八月休会期间说道：“人工智能已经重塑了世界，就像蒸汽机重塑了社会一样。但随着人工智能取得新的进步，它将在几年内成为具有个性的超音速喷气发动机，我们需要为此做好准备。”</p><p>54 岁的刘云平很清楚：他是 535 名国会议员当中仅有的 3 名拥有计算机科学学位的人之一。他表示，作为政策制定者，监管人工智能是自己的首要任务之一，但这样做需要国会充分掌握这个话题。这就是为什么他（与众议员安娜·埃舒一起）一直在推动成立一个由人工智能专家组成的两党委员会，去研究人工智能的进展，并就如何监管人工智能提出新建议。他最近还与人共同提出了一项跨党派立法，以防止未来不管如何先进的人工智能有自行发射核武器的能力。刘云平说：“我们仍处在早期阶段。但这并不意味着就不能针对个别问题立法。”</p><p>当被问及是否担心美国加强监管会限制本国与外国公司竞争的能力时，刘云平说政策制定者需要小心谨慎。 “如果会妨碍创新，如果其他国家不这么做，我认为我们也不应该做，除非我们确实必须这么做。”</p><h3>莎拉·钱德：欧洲数字权利高级政策顾问</h3><p>今年6月，当欧盟宣布正在采取关键步骤通过《人工智能法案》（全球第一部关于该技术的重大法律之一）时，莎拉·钱德 (Sarah Chander) 就已经在考虑这项法律在保护有色人种社区方面能做到什么程度。</p><p>在 2020 年成为总部位于布鲁塞尔的欧洲数字权利 (EDRi) 的高级政策顾问之前，印度裔英国人钱德的关注焦点是国际法以及反种族主义。</p><p>现在，32 岁的钱德建议欧盟改进与人工智能、隐私与监视相关的政策和立法——近年来，随着越来越多的政府部署人工智能工具和基础设施，用来监视人口和控制边界，这个问题变得更加紧迫。</p><p>作为回应，钱德尤其关注新的《人工智能法案》，希望确保其能解决这些危害。而这项法案对世界各地的政策制定者来说可能会成为榜样，后者也正在努力解决如何对快速发展的技术设置护栏的问题。</p><p>在起草该法律的早期阶段，欧盟的重点似乎是把人工智能系统当作产品进行监管的技术要素，以及在向公众发布之前根据其对用户构成的风险对不同的人工智能应用进行分类。 钱德说：“这正是 [EDRi] 想要避免的，因为这没有考虑到在某些情况下或者针对某些社区是否应该使用这项技术。</p><p>她说：“基本上我们的主张是从纯粹的技术视角转向问责视角，这样我们就不仅可以将人工智能系统看作是基础设施或服务，而且看作是适应社会结构的整个系统”。</p><p>钱德与 EDRi 动员了 150 名律师、活动家、学者和民间社会组织组成联盟，要求通过公开数据库提高这些系统使用方式的透明度，以及为直接受影响者提供补救的法律框架。钱德表示，这项技术根本就不该用在跨境移民身上，因为这是“完全有害的，以至于没有办法改进”。</p><p>今年6 月，欧洲议会接受了 EDRi 的许多要求。这些事态发展让钱德对今秋欧盟讨论的前景感到“乐观”。 她说：“我们正在讨论全球首部人工智能法，争论会非常激烈，因为涉及众多相互竞争的利益”。</p><h3>尼娜·扬科维奇：Centre for Information Resilience 副总裁</h3><p>尼娜·扬科维奇 (Nina Jankowicz) 多年来一直就虚假信息以及假新闻的影响向世界各地的政府、组织和科技公司提供咨询，之后拜登政府任命她领导虚假信息治理委员会（Disinformation Governance Board）。这个在2022 年 4 月成立，隶属美国国土安全部的组织的目标就是打击虚假信息。但在就任几个小时后，当时 33 岁的她本人就成为了虚假信息运动的攻击目标。扬科维奇最终辞职，这个委员会在宣布成立后仅三周就解散了。</p><p>可是攻击仍在继续。今年6月，她发现了三个以她的美国官方肖像为主题的深度伪造色情视频；这些视频是在约一年前上传到网上的。扬科维奇说，这样的经历“有点超现实”，尤其是因为她当时已经怀孕六个月了。 “很明显，这是被用来训练人工智能模型的。”</p><p>这个模型针对的是女性。 2019 年的一项研究发现，网上发现的深度伪造内容当中有 96% 是色情内容，而且 100% 均未经相关女性的同意。但当扬科维奇检查那些深度伪造网站和论坛时，还注意到了其他一些令人担忧的趋势。这些视频的主角主要是那些仅仅因为从事政治或表演艺术等领域工作而“引起公众愤怒”的女性。 她说：“这对我来说是最令人心寒之处。纯属战略攻击，目的是在网上诽谤、破坏和羞辱女性。”</p><p>对她来说，这段痛苦的经历清楚地表明了一件事：“人工智能与性别及女性是敌对关系”。 《如何在网络上成为一名女性》（How to Be a Woman Online）与《如何输掉信息战》（How to Lose the Information War）一书的作者现在把大量时间花在敦促各国政府上，希望在讨论监管人工智能时，虚假信息、网络虐待以及深度伪造色情内容的传播等紧迫问题能得解决。</p><h3>瓦德瓦尼兄弟：Wadhwani AI 联合创始人</h3><p>2018 年，印度亿万富翁兄弟罗梅什·瓦德瓦尼（Romesh Wadhwani）与苏尼尔·瓦德瓦尼（Sunil Wadhwani） 开始思考如何利用人工智能来帮助解决全球发展挑战，尤其是人均每天生活费不足 5 美元国家的挑战。为了找到答案，罗梅什和苏尼尔兄弟（前者是 SAIGroup 创始人兼董事长，后者是 WISH 基金会创始人）决定联手，斥资 3000 万美元成立非营利机构 Wadhwani AI。 （迄今为止已承诺投入 6000 万美元。）</p><p>如今，这家总部位于孟买的研究所是少数几家专门致力于通过与南方国家政府合作，在医疗保健、教育和农业等领域为服务不足的社区开创可扩展的人工智能解决方案生态体系的机构之一。这项工作包括战略与国际研究中心（Center for Strategic and International Studies）合作的一项耗资 500 万美元的新项目。苏尼尔·瓦德瓦尼表示：“我们认为，在美国、中国与欧洲，人工智能正在被用来帮助富裕人群，但也许我们可以让印度成为将人工智能用于社会公益的全球领袖。”</p><p>六年过去了，他们已经有了一些解决方案。今年 4 月，该研究所宣布了一系列人工智能项目，目标是用来预测印度北部哈里亚纳邦 100 多加公共卫生机构的结核病患者的风险与死亡率。他们有一个程序利用了人工智能来解释血液检测结果，从而确定对结核病的耐药性，另一个程序则可检测超声波异常，从而预测患者疾病检测呈阳性的可能性。第三种解决方案尝试为护理人员提供决策支持，通过利用数据集来预测患者是否有可能完成治疗（基于年龄、性别、地点以及诊断和治疗开始之间的时间等指标），并与印度百万结核病患者的近半相应结果进行了对比。</p><p>该研究所还与印度政府合作推出了临床决策支持系统，帮助医生和一线工作人员根据数据集更快地进行诊断。 苏尼尔表示：“在短短 90 天内，这个系统现在每月接受咨询的数量已超过 400 万次”。</p><p>瓦德瓦尼兄弟表示，印度拥有 14 亿的多元化人口，非常适合该研究所的利他研究使命。 罗梅什·瓦德瓦尼说：“其他国家根本不具备印度所拥有的能力或机会”。</p><h3>伊尔哈姆·塔巴西：NIST新兴技术助理主任</h3><p>大约五年前，美国国家标准与技术研究所 (NIST) 开始制定一项计划，要推进值得信赖和负责任的人工智能系统的开发。电气工程师兼研究所 IT 实验室主任伊尔哈姆·塔巴西（Elham Tabassi） 提出，要把关于人工智能影响的讨论从原则转向实际的政策实施。</p><p>事实证明，她的建议非常重要。 塔巴西的团队开始围绕人工智能安全和偏见进行研究后，作为 2021 年国防授权法案 (NDAA) 的一部分，美国国会授权 NIST（美国商务部下属单位）为值得信赖的人工智能系统制订一个自愿的风险管理框架。在塔巴西领导下，2023 年 1 月，美国公布了旨在帮助人工智能用户和开发者分析和解决与人工智能系统相关的风险的最终框架，同时还提供了处理此类风险的实用指南。</p><p>塔巴西一直梦想成为一名科学家。1994 年，她移民到美国接受了研究生教育，五年后开始进入 NIST 工作，从事各种机器学习与计算机视觉项目，并应用到生物识别评估和标准上。在职业生涯早期，塔巴西是 NIST 指纹图像质量 (NFIQ) 的首席架构师。现在，这项标准已成为衡量指纹图像质量的国际标准。</p><h3>丹·亨德里克斯：人工智能安全中心执行主任</h3><p>2021 年，当时的加州大学伯克利分校计算机科学专业博士生丹·亨德里克斯 (Dan Hendrycks)在一个在线论坛上发帖，他预测埃隆·马斯克会在 2023 年重新加入到“开发安全先进人工智能的战斗”之中。 亨德里克斯是对的——2023 年 7 月，马斯克推出了 xAI。但他没有想到的是，自己会被任命为 xAI 的安全顾问。</p><p>修完博士学位后，亨德里克斯创立了人工智能安全中心 (CAIS)，这是一家位于旧金山的非营利组织，拥有十几名员工。亨德里克斯把时间投入研究确保人工智能安全的方法上，同时监督着增加做同样事情的人数的努力，并向政策制定者和公众通报这些危险。今年5月， CAIS发表了一份声明，警告人工智能带来的灭绝风险应该“与流行病与核战争等其他全社会规模的风险一起成为全球优先事项”。这份声明获得了 500 多名知名学者与行业领袖的联署。</p><p>大约在同一时间内，亨德里克斯策划和编辑的人工智能安全中心newsletter对马斯克将成立一个新的人工智能组织的报道做出了回应。该newsletter警告说，马斯克的决定可能会让人工智能开发者的争夺变得更加激烈，导致投入到确保人工智能系统安全的时间和精力减少。 亨德里克斯给 xAI 的高级员工之一 Igor Babuschkin 发了一封电子邮件，询问 xAI 打算如何实现人工智能安全，而对他的招聘流程就此开始了。 亨德里克斯认为自己在安全方面的研究生研究帮助他获得了 xAI 职位，并表示他直到面试过程后期才见到了马斯克。</p><p>现年 28 岁的亨德里克斯表示，尽管他加入了 xAI，但风险仍然存在，他在第一次与马斯克会面时提到了竞争压力：“我认为竞争压力是最大的风险因素。”马斯克“同意我们不该尝试去赢得这场灭绝的竞赛”。</p><p>他希望政府或国际机构能够解决这些问题。与此同时，鉴于马斯克拥有丰富的资源，亨德里克斯预计 xAI 会成为一家“顶级”的人工智能公司，但他宁愿就如何确保 xAI 的系统尽可能安全提供建议。</p><p>但最终，亨德里克斯认为最危险的竞争不是在公司之间进行。他预测，随着人工智能系统变得越来越强大，军队会意识到它们对国家安全的重要性。他担心，如果没有强有力的国际协调，国际军备竞赛就会随之而来。他预测，这样的军备竞赛有 80% 的可能性会导致一场毁灭全人类或大部分人类的灾难。这种威胁的紧迫性迫使他长时间工作，但他也安慰自己，自己顶多需要再工作 20 年。之后，他预测要么自己的工作将被自动化，要么就已经像其他人一样死掉了。</p><h3>杰西·惠特尔斯顿：Centre for Long-Term Resilience 人工智能政策负责人</h3><p>思考人工智能风险的人往往会陷入到这两个阵营之一：要么对人工智能的力量以及开发它的公司的意图持怀疑态度，要么相信人工智能将成为人类史上最重要的技术，而且开发它的公司是善意的。</p><p>杰西·惠特尔斯顿（Jess Whittlestone）不属于任何一个阵营。 惠特尔斯顿Center for Long Term Resilience的人工智能政策负责人。这个中心是一家总部位于英国的智库，成立于 2020 年，旨在提高全球应对极端风险的抵御能力。容易受到利益冲突的影响。她对自己的定位是一个认真对待极端风险但又不会受到此类冲突影响的人。</p><p>惠特尔斯顿先是学习了数学和哲学，之后又拿到了行为科学的博士学位。 2018 年，她开始从事人工智能政策研究。她说她“一直对我们如何在社会层面做出更好的决策感兴趣。”她也意识到究竟是谁在做出这些决定。比方说，她认为，企业往往会根据人工智能带来的好处来证明持续进行人工智能开发的合理性，但这些好处并没有得到充分证实。</p><p>在剑桥大学莱弗胡姆智能未来中心（Leverhulme Centre for the Future of Intelligence）以及存在风险研究中心（Existential Risk）担任学术职务后，惠特尔斯通又加入了Centre for Long-Term Resilience。她曾与中国人工智能研究人员一起参与非官方外交活动，并与知名人工智能公司一起制定人工智能安全政策。她投入了大量时间就制订人工智能政策应该知晓的技术细节向英国政府官员提供建议，这对于英国政府今年秋天主办的人工智能峰会尤其重要，而这场峰会可能会为未来数年人工智能政策问题的国际合作制定条款。</p><p><strong>延伸阅读：</strong><br /><a href="https://36kr.com/p/preview/yDtih5nSwPud7LGQCxo8jPJz_giqS10tgFoci7MRqyvDIuAbrXb2KJ_eEITw0Dvg" rel="noopener noreferrer" target="_blank">《时代》人工智能百人榜（一）：领袖</a></p><p><a href="https://36kr.com/p/preview/lEp34b2Kvr0Qun8J8PYNVQlpXpC4pe2wfHgffEghiZ0Wk4TOWhbyoVx3pQfLbqaQ" rel="noopener noreferrer" target="_blank">《时代》人工智能百人榜（二）：创新者</a></p><p>译者：boxi。</p>
]]></content:encoded>
<pubDate>Fri, 27 Oct 2023 11:00:50 GMT</pubDate>
<pubDate>Fri, 27 Oct 2023 11:00:50 GMT</pubDate>
</item>
<item>
<title>双十一，用AI挤干「水分」要低价</title>
<link>https://www.36kr.com/p/2492478053357701</link>
<guid>https://www.36kr.com/p/2492478053357701</guid>
<content:encoded><![CDATA[

<p>9.9的奶茶、7.9的眉笔、19.9的便当套餐……今年11.11，买买买囤囤囤的野性消费已成为过去式，大家不再困于消费主义的茧房，而是积极去寻找更高性价比的生活方式。</p><p>今年冲上热搜的话题“反向消费”，也精准阐释了这一潮流。与其说反向消费是一种消费降级，倒不如说是消费者的“智商”升级——“可以买贵的，但不能买贵了”。</p><p>在这一波反向消费的浪潮中，今年11.11，京东立下了“真便宜，闭眼买”的军令状。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_ee2e7a75d5834b768c1f36e922ce4db5@1267484143_oswg437816oswg1080oswg457_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>“真便宜”，要求渠道企业在供应链中充分挤出水分，让利给商家和消费者；“闭眼买”，则对产品的品质和商家的服务提出了更高的要求。</p><p>渠道拼低价，消费者要便宜的好产品，对零售行业的降本增效提出了更高、更迫切的要求。<strong>如果说，过去降本增效为品牌带来的是“锦上花”，那么在当下，则是品牌谋求生存的“雪中炭”。</strong></p><h2><strong>品牌核心竞争力，被大模型革新</strong></h2><p>前不久，沸沸扬扬的头部主播失言事件，将其高额佣金要求和市场支配地位暴露出来。<strong>品牌要毛利、主播要佣金、消费者要低价，品牌面临的“三角困境”急需被打破。</strong></p><p>不仅如此，那些翻红的老牌国货们，还面临新场景的挑战。650万网友在线指导活力28直播上链接，白猫找不到主播临时用玩具猫“带货”，甚至还有不少品牌因产能不足卖断货……</p><p>从藏在标签里到步入聚光灯下。<strong>以大模型为代表的人工智能技术，为国货突围打开了一扇窗：挤干运营降本增效的“最后一滴水”，实现了有品质的低价。</strong></p><p>作为防脱生发的明星产品，米诺地尔酊近几年来备受职场人追捧。随之飞速崛起的国货品牌蔓迪，2022年在大陆米诺地尔酊市场的占有率就达到了71.7%。</p><p>然而，毛发健康产品尚属于新型品类，米诺地尔酊产品对大众而言仍然理解门槛。并且，由于消费者作息时间，蔓迪的直播间在深夜时段往往涌入大量的用户咨询。</p><p>蔓迪直播负责人马凯迪告诉36氪，“即便每日基本保持12小时的多平台直播，真人主播对深夜涌入的大量咨询需求依然分身乏术。”与此同时，不少涉及医药专业的咨询问题，也对真人主播的知识储备提出了挑战。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_77c56b069c22447cb84b35290f968526@1267484143_oswg534745oswg865oswg577_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>同样的烦恼也发生在其它品类。比如对于母婴品牌，来自新手父母的咨询不仅涉及大量关于产品成分、使用方法的细节，<strong>最好客服还能提供一些情绪价值，</strong>这也导致单次咨询耗时较久。</p><p><strong>对商家而言，留住消费者，客服是一道重要关卡。</strong>专业的售前服务是引导消费者购买的前提，而良好的售后服务无疑是保持消费者满意度、忠诚度的有效举措。相应的，如果无法提供优质的售前、售后服务，将会引起流量极大的反噬。</p><p>幸运的是，大模型的落地，为品牌应对消费者愈发精细化、专业化的消费需求，给出了解法。</p><p>ChatGPT带来模型训练范式的变革，同样也是AI语义理解能力的跃迁——即便探索期还很长，但能肯定的是，大模型的语义理解能力绝不仅限于聊天吹水、写诗撰文这些风花雪月，而是推动产业变革。</p><p><strong>大模型能产生的价值，取决于落地产业的厚度。这个“厚”主要体现在两方面：一是数字化程度高，积累的高质量数据多；二是积累的场景和应用多，来源于零售、金融、物流等真实产业场景需求。</strong>前者决定了模型本身的技术能力，后者则决定了模型能创造的行业价值。</p><p>在这个意义上，零售行业既有大量的对话和内容交互场景，又具备复杂营销场景和供应链环节，是大模型走向田野的绝佳去处。</p><p><strong>而大模型为零售行业带来的价值也很明显：一是降本增效，二是营销创新。</strong>Gartner预估，到2026年，对话式人工智能将帮助客服中心降低800亿美元劳动力成本。Gartner副总裁分析师Daniel O'Donnell认为，“对话式人工智能可以让客服更有效率，同时也能改善客户体验。”</p><p>品牌的核心竞争力在于谁能把消费者照顾的更极致。<strong>在如今的大模型时代，捕获消费者“芳心”的“极致点”，靠的不仅是品牌营销中的感性表达，更是真正理解用户需求，以及以更专业的服务覆盖消费全生命周期。</strong></p><p><strong>不少从业者判断，若是智能服务无法打通平台-商家-消费者的完整消费生态，实施运营成本和营销成本的优化，商家就无法“躺着赚钱”，消费者也无法“闭眼省钱”。</strong></p><p>这意味着，今年11.11的品牌大战，在一定程度上也是服务背后的技术较量。</p><p><strong>对于商家而言，重要的不仅仅是把一个数字人搬进直播间，或是在线客服增加机器人对话功能，而是为消费者提供全场景的智能服务。</strong></p><h2><strong>京东云，从降本增效到价值创造</strong></h2><p>想要实现这个目标并不容易。</p><p>2017年以来，智能化服务模式在客服中心大规模应用。</p><p>以几年前最基础的文字客服为例，智能客服将用户输入的文本进行分词处理、统计词频，并转化为数字向量。通过比对现有语料库，机器人能够理解文本意义，最终匹配出相应的回答。</p><p>随着技术进步，算法升级到了深度学习、神经网络，乃至最新基于Transformer为基础模型的ChatGPT产品。但业内人士认为，短期内算法的提升不会给智能客服产品带来特别明显的改进，反而是<strong>数据可能成为智能服务比拼的关键。</strong></p><p>比如具体到直播场景，一名电商从业者指出，多数厂商提供的是标准化的数字人生成和部署技术。但由于没有零售行业的训练数据，也不能与商家的整个业务流程深度融合，数字人主播“并不智能”、也“无法创造新的增长点”。</p><p>大模型经历了百家争鸣的上半年，但转至应用落地，不少厂商仍然“拿着锤子找钉子”，发布十数个行业模型，从场景切入，再找用户的真需求。</p><p>而这时，京东已经开始“盖房子”。</p><p><strong>大模型，小切口。京东云在零售行业“把大模型做小”，更面向智能应用。</strong></p><p>依托自研的言犀大模型，京东云瞄准用户触达、用户服务、消费洞察、经营分析等多个细分场景，升级了言犀多模态数字人、言犀智能外呼、京小智三大营销、服务应用。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_89f325ffab194904aad31b561a6ccbc9@1267484143_oswg572401oswg801oswg454_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1" /></p><p><strong>在电商场景，京东云推出了言犀虚拟主播，为商家提供高仿真、高转化的数字人主播。支持直播脚本“一键生成”，2小时内可实现高效开播，每日运营精力投入低至30分钟不到，将人力成本节省到原来的不到1/10。</strong></p><p>除了电商直播，言犀数字人还可以在线下银行网点、机场航司、政务大厅、购物中心“上岗”，担任数字员工。通过应用言犀数字人短视频生成平台，一键自动生成新闻播报、产品培训等场景的短视频。</p><p>技术创新方面，京东云整合了多项多模态数字人交互能力。比如通过与大模型结合让数字人说话时的动作与语义匹配，让交互更加自然。</p><p>又比如数字人动态局部高清技术，利用了人类视觉感知并不均匀的特点，通过重点区域提高分辨率，其他区域适当降低分辨率，<strong>从而降低部署成本。</strong></p><p>流量面前，中小品牌的直播投入可能是一场赔本买卖。坑位费、抽佣再叠加预估的退货率，“几乎卖越多，亏越多”。中小品牌面临亏本，上了规模的品牌一旦合作便容易深度依赖头部主播。</p><p>为了摆脱这一境况，品牌们普遍已开始探索自播的路径，积极开展创新营销。据艾瑞咨询预测，2023年企业自播占整体电商直播的比例将接近50%，即将超过达人直播。</p><p>作为一个有着近40年历史的日化品牌，大宝属于较早转型的一批。</p><p>2019年，大宝试水自播。面对消费者对于日化产品精细化的选择，MCN主播上岗前都要接受大宝团队的产品培训。今年，大宝在经典SOD蜜之外，针对男性和年轻市场研发了多个新品线，随之而来的是繁杂的主播培训工作。</p><p>“数字人直播对于预算有限的国货品牌来说是高性价比的选择。我们现在‘两条腿’走路，真人直播和数字人接力上岗。”大宝京东渠道负责人袁航告诉36氪，出于对京东的信赖，2023年9月，大宝选用了京东云言犀数字人。</p><p>在数字人的配置上，初期试用的大宝并没有增添过多的个性化配置，从模版库中挑选一位符合品牌形象的主播，使用言犀训练好的知识库，一键生成直播文案——大宝数字主播“悄悄”上线了。<strong>“使用1个月直播间月销翻3倍”</strong>，袁航对“新同事”的表现大为赞叹。</p><p>对于消费者使用频次更多的在线客服，大宝交给了京小智。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_c120be2cb5b54ee7a137165d5175ee56@1267484143_oswg643516oswg803oswg454_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1" /></p><p>京小智，更像是品牌的“军师”、消费者的“助理”。京东相关人员介绍，<strong>今年在全面接入言犀大模型后，京小智实现了与订单、价保等京东域内各营销服务环节的打通，可以高效给出自动化解决方案，切实推进服务效率的提升。</strong></p><p>这是ChatGPT做不到的。</p><p>例如在价保环节，九阳借助京小智“价保申请”，自动为客户退差价，无需跳转和等待。而在运营策略制定上，商家只需输入自然语言指令，京小智就能生成数据分析报告。目前，九阳纯机回复率高达50%以上、转人工率仅为34.56%。</p><p><strong>比降本增效更进一步，是利用AI创造新的营销场景。</strong></p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_a80876decbfb4b06b74c6cc8a15bf91a@1267484143_oswg2596351oswg4550oswg2020_img_jpg?x-oss-process=image/quality,q_80/format,jpg/interlace,1" /></p><p>去年，数十万粉丝突然接到了偶像打来的节日祝福电话——幕后，这是伊利用智能外呼策划的活动。<strong>言犀智能外呼，聚焦在售前咨询、售后回访等语音通话场景。</strong>能够与操着方言，随时转化话题的客户流畅对话，可以根据每位客户信息做个性化沟通，还能听懂大量互联网新鲜名词，轻松消除与各圈层人群的交流障碍。</p><p>这一通电话让伊利成功召回了超过8万用户，整体ROI实现超10倍提升。与此同时，品牌的加粉率达到了35%。&nbsp;</p><h2><strong>修炼内功，源于产业、服务产业</strong></h2><p>“以智能技术为驱动的精细化运营，成为企业在全量竞争时代的必然选择，” 京东云智能服务产品部相关负责人曾表示。早在2013年，京东就开始发力智能服务，2020年尝试对外服务，至今，除零售外还在金融、政务、交通等多个重点行业开花结果。</p><p>拆解每一次智能服务，能看到多年的技术深耕与场景赋能：</p><p>言犀多模态数字人背后，是京东云10余年的智能对话经验和多模态交互技术的积淀；</p><p>言犀智能外呼背后，是京东云全自研语音语义技术，以及6亿用户在零售场景下产生的海量语音交互；</p><p>京小智背后，则是京东域内20多年来积累的触达、服务、洞察等全链路场景。</p><p><strong>这些，是京东云在零售场景构建行业壁垒的重要禀赋。而在言犀，这一面向产业的大模型加持下，京东云将会向品牌方释放更多技术降本与营销创新的势能。</strong></p><p>关于大模型的价值，京东曾做出定义，即“算法×算力×数据×产业厚度的平方”。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_abba1b1821104a1aa148933b30fcfb02@1267484143_oswg450283oswg865oswg302_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>澎湃的算力让言犀大模型的迭代与部署速度更快。早在2021年，京东在重庆落地了全国首个基于SuperPOD架构的超大规模计算集群——天琴α，采用原装DGX IB超高速带宽A100 GPU集群，推理提速6.2倍，推理成本节省90%。</p><p>由国内顶尖AI科学家打造的算法，构成了言犀大模型的“筋骨”。源于业务应用需求，京东早在2020年就首次提出K-PLUG模型，将领域知识注入大模型中，以提高大模型的专业性和忠实度。K-PLUG方法是基于Transformer模型架构x京东的产业知识进行的预训练、知识增强的指令精调，以及知识指导的强化学习。</p><p>庞大复杂的产业生态，服务超千万自营商品SKU、5000万工业品SKU、超800万家活跃企业客户、全国2000多条产业带，以及来自真实场景复杂、动态、鲜活的优质数据，成为言犀大模型30%数智供应链原生数据的源泉，为大模型解决真实产业问题，灌入“养料”。</p><p>与此同时，大模型只有更加普惠，智能应用的价值才能快速凸显，数据飞轮也能基于商家反馈更高效更快转动。为此，京东云把11.11的第一波优惠，给了品牌商家。</p><p><strong>提高转化率、新品推爆概率，品牌通过各种技术手段，来极致压缩运营成本和营销成本——谁能持续降本增效，挤干供应链这块海绵中的最后一滴水，谁就能在“低价”与“品质”的双向奔赴中，拔得头筹。</strong></p><p>反向消费热潮背景下，零售行业的变革才刚刚开始，大模型与智能服务的进化之路还很长，但无论如何，以技术为“锚点”夯实竞争力，推动品牌实现低价与高品质的兼得，已成为行业必经之路。</p><p>这也是京东所理解的“真便宜”：只有企业提升效率实现成本结构优化、消费者获得质优价美的商品和服务、产业更有动力拥抱数字技术，才能<strong>推动形成“企业降本增效-消费提质扩容-产业高质量发展”的产业正循环，为消费者带来长久的低价。</strong></p>
]]></content:encoded>
<pubDate>Fri, 27 Oct 2023 10:32:22 GMT</pubDate>
<pubDate>Fri, 27 Oct 2023 10:32:22 GMT</pubDate>
</item>
<item>
<title>Jasper AI 一年内估值打8折，AIGC开始降温</title>
<link>https://www.36kr.com/p/2492438785726343</link>
<guid>https://www.36kr.com/p/2492438785726343</guid>
<content:encoded><![CDATA[

<blockquote><p><strong>AI企业的破局之路在哪？</strong></p></blockquote><p>AI浪潮滚滚，但有的企业却在潮水中掉队了。</p><p>随着ChatGPT的爆火，很多投身AIGC浪潮的公司也获得资本青睐，取得非常高的估值。本以为接下来能蒸蒸日上，没想到却连连败退。</p><p>寒气似乎开始逼来。</p><p>据The Information报道，AI语音识别软件公司<strong>Deepgram</strong>裁掉了大约20%的员工，约20人，这是它今年第二次裁员。Deepgram的CEO<strong>Scott Stephenson</strong>表示，裁员主要源于融资困难。不只Deepgram一家，AIGC企业中的“明星人物”<strong>Jasper AI</strong>，也开启了裁员节奏，并且下调了收入预期。</p><p>在寒气逼人的另一边，反而是行业头部玩家<strong>OpenAI</strong>的持续火热。其不断获得资本青睐，最新估值达到860亿美元，相比今年4月几乎翻了三倍。</p><p>不过纵观整个行业，除了OpenAI，许多AIGC初创企业都开始走下坡路，一股AIGC降温潮似乎正在涌来。</p><h2><strong>01 寒意尽显的AIGC玩家</strong></h2><p>按照爽剧里的剧情，Deepgram本可以走上“人生巅峰”。</p><p>Deepgram是一家成立于2015年、位于旧金山的AI初创公司，专注于语音识别和语音文字转录工具。</p><p>去年下半年，Deepgram进行B轮融资筹集到4700万美元。再加上此前的融资，Deepgram一共筹集了8600万美元，估值达到2.67亿美元，投资者包括Madrona Venture Group、Tiger Global Management和Y Combinator等实力雄厚的明星机构。</p><p>另外，Stephenson还提到，公司刚刚经历了创业历史上“最好的一个季度”，尽管他并没有透露有多好。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_0803cdce55424d5db2ca99c4a6e392bc@000000_oswg350224oswg1080oswg720_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p class="img-desc">Deepgram&nbsp;CEO Scott Stephenson</p><p>按照这番势头，Deepgram应该不差钱，可以不断走上坡路，结果在高光时刻反而裁员，释放出萎靡信号。</p><p>Stephenson将此次裁员，归咎于利率上升，导致启动资金减少。他表示：“预计未来一年的融资环境不会变好，高利率将持续更长时间，因此必须更加保守，牺牲增长节省成本。”</p><p>在Deepgram之前，另一家AIGC公司Jasper也已经出现经营危机。</p><p>Jasper的核心产品是Jasper.AI，有些读者也许是它的用户。作为基于GPT3模型开发的AIGC产品，Jasper.AI可以被拿来生成Instagram标题、编写TikTok视频脚本、写电子邮件等等。</p><p>不仅如此，Jasper还提供了60多个适合各种使用场景的文案模板，支持超过25种语言，并且能够检查生成内容中涉及的错误。</p><p>凭借这些功能，Jasper.AI一经推出就火了，在社交媒体、跨海电商、视频制作等领域获得大量用户的青睐。2021年，Jasper收入超4000万美金，到了2022年又翻了一倍达到8000万美元。截至2022年，有超过100万人用过Jasper.AI，付费用户数量超过7万。</p><p>在Jasper.AI发布18个月后，由于产品持续火热，Jasper于2022年10月获得由Insight Partners领投的1.25亿美元A轮融资，估值也涨到15亿美元，跻身独角兽行列。这距离Jasper成立还不到两年，可谓风光一时。</p><p>然而，幸福的时光总是短暂的。</p><p>今年2月，Jasper预期全年收入为1亿4000万美元，结果到了夏天就把预期下调了30%，紧接着在7月开启了裁员。前不久，Jasper的两位联合创始人——CEO Dave Rogenmoser和CTO J.P. Morgan都在上个月辞职了。另外，前不久，据The Information报道，Jasper已经将面向员工的股票估值降低了20%。</p><p>大量信号都表明，许多被寄予厚望的AIGC明星企业，在迅速走入下坡路。</p><p>在这背后，真正的导火索究竟是什么？</p><h2><strong>02 两大“导火索”</strong></h2><p>沿着Deepgram和Jasper曲折的成长路径，可以看到两条让它们走下坡路的导火索：</p><p><strong>竞争和融资。</strong></p><p>9月24日在美国旧金山举行的YC校友会上，OpenAI创始人兼CEO<strong>Sam Altman</strong>吸引了大批观众。面对台下大量的AI创业者，Sam Altman发出警告：</p><p><strong>简单模仿ChatGPT的公司，不会有好的结果。</strong></p><p>Deepgram和Jasper虽然没有模仿ChatGPT，但二者跟OpenAI之间存在你争我斗的竞争关系。</p><p>这种竞争，让Deepgram和Jasper陷入了经营危机。</p><p>2022年9月，OpenAI推出开源的语音识别软件Whisper后，对于Deepgram打击很大。在功能体验上，Whisper并不逊色于Deepgram。更为关键的是，在Whisper推出六个月后，OpenAI开始通过API向开发者收费，并且费用比较低廉。这种“价格战”大大降低了开发者的使用门槛，让Deepgram倍感压力。</p><p>同样的，OpenAI也对Jasper形成十分大的打击。</p><p>Jasper.AI是采用付费模式的，而ChatGPT一推出就免费开放，迅速成为史上增速最快的消费级应用。并且，当时基于GPT3.5模型的ChatGPT，在许多方面的能力并不逊色于Jasper.AI，抢走了大批用户。因此，Jasper的产品一度被调侃为“换皮版GPT-3”，这就让Jasper产生了危机感。</p><p>很快，OpenAI继续迭代，GPT4的出世让Jasper.AI黯然失色，在功能上对其进行了进一步的碾压，唱衰Jasper的声音此起彼伏。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_9e4fb5c702bd4df390888f7110d04a3d@000000_oswg420375oswg1080oswg691_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p><strong>面对OpenAI的强劲实力，一众AI企业都变得后劲不足。在此之外，融资环境渐冷，也是悬在它们头上的达摩克利斯之剑。</strong></p><p>根据Crunchbase数据，2022年全球融资放缓，全球风险投资仅为4450亿元，同比下降35%。2022年美国AI领域融资数目、融资金额同比下滑，分别为下降19%、46%。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_631ae367bf264e0da2c16f0539360b6f@000000_oswg174673oswg1080oswg393_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>据Pitchbook最新数据显示，2023年第三季度，全球生成式AI领域投融资交易数量达101笔，环比下降29%，交易总额降至61亿美元，比今年一季度降低80%左右。与此同时，全球初创企业的交易总额较上年同期下降31%，至730亿美元。</p><p>这意味着，生成式AI企业的融资环境在变差，经营风险在加大。</p><p>Index Ventures合伙人Bryan Offutt表示，随着市场回归现实，生成式AI投资动力正在减弱。虽然GPT这项技术令人印象深刻，但对于大多数应用场景来说，它还不够可靠，导致投资者热情降温。</p><p>风投公司Greylock普通合伙人Saam Motamedi，在接受TechCrunch采访谈到AI行业时表示，“这是一个非常繁荣的市场，但泡沫正在涌现，我认为很多钱将会流失。”</p><p>激烈的竞争叠加渐冷的融资环境，让很多AI企业倍感压力、趋于保守。</p><h2><strong>03 三条“破局之路”</strong></h2><p>当众多AI企业都在走下坡时，如何寻找破局之路？</p><p>10月12日，据The Information报道，Sam Altman表示公司正以每年13亿美元的速度产生收入，相比去年全年2800万美元的收入增长超过450倍。这是OpenAI成立8年来，收入增长最快的一年。</p><p>然而，大部分的AI企业并没有这么财大气粗。</p><p>Theory Ventures的一项调查显示，收入超过1000万美元的纯AI初创公司不到25家。大约95%的生成式AI公司年平均收入不到500万美元。甚至许多估值达数亿美元的AI初创公司，收入仍几乎为0。</p><p>如果通过粗暴的收费进行创收，风险非常大。Unusual Ventures在一份报告中指出，用户转向收费的过程风险极大，许多高增长、低收入的初创公司都是根据用户数量进行融资的，但其中一些公司的用户留存率极低。一旦进行收费，用户会大量流失。</p><p>那么，如何创造更多收入？关键在于通过产品来满足需求、打动用户。</p><p>Jasper创始人Dave Rogenmoser在裁员公告中表示，虽然Jasper正在服务很多公司，但随着越来越多的公司开始使用AI产品，需求满足变得越来越难以实现。因此，Jasper决定通过裁员重塑团队、配置资源，满足不断变化的新需求。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_536c5763a0454d2fba1c1b32926a29cb@000000_oswg622044oswg1078oswg720_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p class="img-desc">Dave Rogenmoser</p><p>优秀的产品，源于优秀的人才。要想通过产品满足需求，关键在于人才。</p><p>Sam Altman表示，创始人需要将至少30%的时间用于招聘，要让关键员工可以自由地创造和测试。ChatGPT的诞生，正是源自研究人员Alec Radford的技术突破。</p><p>OpenAI在创立之初，聘请了大量的AI研究人员。不过，整个团队还不清楚什么是正确的发展路线。直到Alec Radford加入OpenAI，在语言模型方面取得了巨大突破，OpenAI的其他团队开始全力研究大模型，最终催生了ChatGPT。</p><p>也就是说，<strong>创造收入、满足需求、招募人才</strong>这三大方向，才是正在走下坡路的AI企业的破局之路。</p><p>当然，即便一时失败了也不要紧，布局AI注定是一条马拉松式的长赛道，笑到最后的才是真正的赢家。</p><p>Sam Altman的首次创业，就以失败告终。另外他作为YC合伙人，也见到了数以百计的创业公司倒下。</p><p>寒潮来了不要紧，用逆商不断渡过寒潮的企业，才会真正实现长期主义。</p><p><strong>参考资料：</strong></p><p>AIGC's first wave of layoffs has arrived</p><p>https://pdf.dfcfw.com/pdf/H3_AP202306011587456608_1.pdf?1685656958000.pdf</p><p>本文来自微信公众号<a href="http://mp.weixin.qq.com/s?__biz=MzI4MDUzMTc3Mg==&amp;mid=2247597790&amp;idx=1&amp;sn=244e082a0ba376a489fc862ef2c1fbbe&amp;chksm=ebb4388ddcc3b19b3ed7baf41e9ec61154896e34f8934e7d34edc6434206c75e10bb435562fb&amp;scene=0&amp;xtrack=1#rd" rel="noopener noreferrer nofollow" target="_blank">“硅兔赛跑”（ID：sv_race）</a>，作者：Eric，编辑：Zuri，36氪经授权发布。</p>
]]></content:encoded>
<pubDate>Fri, 27 Oct 2023 09:56:26 GMT</pubDate>
<pubDate>Fri, 27 Oct 2023 09:56:26 GMT</pubDate>
</item>
<item>
<title>《时代》人工智能百人榜（二）：创新者</title>
<link>https://www.36kr.com/p/2451868091717763</link>
<guid>https://www.36kr.com/p/2451868091717763</guid>
<content:encoded><![CDATA[

<blockquote><p>神译局是36氪旗下编译团队，关注科技、商业、职场、生活等领域，重点介绍国外的新技术、新观点、新风向。</p></blockquote><p>编者按：人工智能的独特之处既最令人恐惧也最值得庆祝——一些技能堪比我们人类，然后还能更进一步，做到人类做不到的事情。模仿人类行为已成为人工智能的决定性特征。然而，机器学习和大语言模型的每一次进步背后实际上都是人——这里面既有经常被忽视的，让大语言模型使用起来更安全的人类劳动，又有就在什么时候以及如何最好地使用这项技术方面做出关键决定的个人。本文综合了各方推荐和建议，将数百项提名汇总到一起，最终形成了这份百人榜名单。从很多方面来说，这 100 人构成了推动人工智能发展的关系网络与权力中心。他们是竞争对手、监管者、科学家、艺术家、倡导者、以及高管——属于既互相竞争又共同合作的人类，他们的洞察力、欲望与缺陷将塑造一项影响力与日俱增的技术的发展方向。文章来自编译，篇幅关系，我们分四部分刊出，此为第二部分。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231011/v2_0c88f03d14b847cc8fd83b7464edb4da@1694_oswg1262095oswg2000oswg1125_img_jpg?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><h2>创新者</h2><h3>姜峯楠：作家</h3><p>姜峯楠（Ted Chiang）也许是全世界在世的科幻小说作家当中最著名的一个。他精心雕琢的短篇小说探讨了我们的内心世界以及我们的社会对科学体系当中意想不到的裂痕会作何反应。接受激素注射可显著改善你的认知功能，你觉得怎样？如果学习一门外语会改变你对时间的感知方式呢？如果人类要创造人造生命的话，需要承担什么义务？</p><p>最近，56岁的姜峯楠开始承担一个新角色。在为《纽约客》撰写的非小说类文章里，他成为了人工智能及其背后的公司最尖锐的批评者之一。在一篇获得病毒式传播的文章里，他把 ChatGPT 比作是“web的一幅模糊的 jpeg”，认为让这款app如此流畅的技术正是它没法区分真实与虚构的原因所在。在另一篇文章里，他又对准了新的人工智能进步所催生与强化的权力结构。他认为，如果不进行结构性的经济变革的话，人工智能的崛起可能会加剧财富不平等、削弱工人权力，并强化科技寡头政治。 他写道：“如果让劳动者过上更好生活不在其中的话，那么进步到底意味着什么？如果省下来的钱除了存入股东的银行账户以外不会流向任何地方的话，那么提高效率还有什么意义呢？”</p><p>在其短小精干又感人至深的故事里，姜峯楠写的每一个字都很重要。今年 8 月初，在一次通过 Zoom 进行的视频采访里，语言成为当务之急。姜峯楠承认，人类在谈论新技术的时候需要使用隐喻；在我们寻找意义的过程中，隐喻可以帮助我们在比较的时候锚定自己。他的问题是我们使用的隐喻会误导我们。比方说，当大语言模型生成了虚假信息时，大家称之为“幻觉”。但他更喜欢用“虚构”（ confabulation）来形容。幻觉不仅意味着（人工智能）内心的存在，而且意味着这个内心拥有感官体验的能力。另一方面，当一个人在不知不觉中伪造信息来填补所遗忘的那段经历时，就会发生虚构。他认为，这仍然是一个隐喻，但就当前人工智能的工作机制来说，这是一个更好的隐喻。姜峯楠问道： “我们能否阻止大语言模型胡言乱语，同时仍让它们生成我们想要的那种事实性的回应？我对此表示怀疑，因为我不认为这里面存在根本不同的过程。”</p><p>跟很多科幻小说作家一样，姜峯楠在他的短篇小说里面也写到了具备内心世界的人造生物。这种虚构的手法对于讲述让我们之所以成为人的亲密故事往往很有帮助，但这是不是在其实不存在的情况下加剧了我们认为人工智能工具具备感知能力的倾向呢？虽然许多科幻小说都探讨了有感知力的人工智能的概念，但目前情况下，对于机器即便在没有执行功能的情况下也可以用我们自己的语言来迷惑我们，我们几乎没有虚构的试金石。科幻小说在普及这个概念方面可以发挥作用吗？姜峯楠对此表示怀疑。 他说：“一切艺术都具有政治性，而且可以促进积极的政治思想。但我认为科幻首先需要成为优秀的艺术，不应仅仅服务于某种政治目的。”</p><p>姜峯楠还反对这样的说法，也就是驳斥技术专家编造的那些不太可能的故事能让他梳理“任何形式的道德权威”。但他表示，当他看到一些推测性居多的人工智能风险讨论时，他确实看到了这种讨论与科幻小说而不是事实的共同点多一点。他说 ：“计算机可以让自己变得更聪明并变得超级智能，作为故事点子来说这确实非常有趣。但听到大家讨论这件事的时候就好像这真的会发生一样，这实在是令人震惊。”</p><h3>查理·布鲁克：作家</h3><p>说到21 世纪科技的流行文化，《黑镜》或许是其中的决定性作品。而查理·布鲁克（Charlie Brooker）则是其幕后策划者。十多年来，布鲁克一直利用这部在 Channel 4、Netflix 播放的电视节目来进行探索，探索当我们对技术进步的追求达到逻辑的、有时候甚至是暴力的终点时，可能会出现反乌托邦的结果。在布鲁克的故事里，一般先是出现了激进的技术，这种技术往往会首先改变角色的生活，让生活变得更美好，然后变得更糟糕。</p><p>当然，人工智能是布鲁克想象力最丰富的主题之一。这位讽刺作家将机器人自主杀人蜂、属于某人被奴役的意识副本的人工智能家庭助理，以及根据观众的生活立即生成的个性化电视节目变成了现实。他描写的这个领域的故事，因其对机器学习过程的想象力与准确性而赢得了真正的人工智能专家的赞誉。</p><p>《黑镜》具有惊人的先见之明，至少有一个例子可以证明。十年前，《马上回来》（Be Right Back）刻画了一位悲伤的女人试图通过人工智能让她死去的男友起死回生。这位名叫玛莎（Martha）的女士先是给一个聊天机器人发短信，这个机器人已经利用她男朋友阿什（Ash）的在线消息进行过训练。然后玛莎又买了一个音容相貌跟男友都很像的机器人身体——想继续两人的生活，仿佛这个新的合成复制品就是复活的阿什一样。当然了，这个阿什是阿什自己的影子，而这让玛莎更加不安。</p><p>这个令人痛苦的思想实验现在已经成为现实。今年，You，Only Virtual 公司推出了模仿已故亲人的聊天机器人。故事创始人贾斯汀·哈里森（Justin Harrison）表示，希望这个机器人能让客户彻底摆脱悲伤，并最终通过人工智能与增强现实相结合召唤死者。</p><p>与此同时，《黑镜》最新一季有一集叫《糟糕的琼》（oan Is Awful），本剧的女主角发现一家大型媒体公司正在利用人工智能和深度伪造技术来生成来自自己生活的内容。这一集播出之际，编剧与电影公司之间的重大合同纠纷正成为头条新闻，人们担心好莱坞也会这样利用人工智能，通过取代人类的讲故事者的部分工作来削减成本。今年六月，布鲁克本人也参与罢工，声援面临被技术取代的美国编剧工会（Writers Guild of America）。 他告诉Vox：“我认为大家对写作的恐惧在于工作室可以用它来生成草稿，然后再雇人类作家重写。让它变得像人写的。这是一种非常令人沮丧的状况。”</p><p>当然，布鲁克描写的很多科幻愿景在未来十年内都不会实现。布鲁克本人曾表示，他的剧集描写的是科技可能导致的“最坏情况”，而不是他的预测。尽管如此，《黑镜》还是为公众提供了一个理解人工智能变革性影响的重要框架：让我们得以去认真思考它可能会如何影响我们的情绪、社交互动、人身安全以及我们从未想过的事情。</p><h3>霍莉·赫恩登：音乐人</h3><p>创作型歌手霍莉·赫恩登（Holly Herndon）做不到用任何语言演唱你想听的任何歌曲。但她的数字孪生 Holly+ 可以做到这一切。现在，任何业余音乐家都可以使用 Holly+， 将自己平淡无奇的声音转变成她的声音。</p><p>让公众操纵你的声音这个想法听起来也许很反乌托邦，仿佛表明人类在向新的机器霸主臣服。但赫恩登的意图恰恰相反。她创建 Holly+ 是为了激励艺术家同行们在技术革命当中重新掌握职业自主权和创作自主权。赫恩登说，全球正在迈向“无尽媒体”时代，任何人都可以像德雷克一样说唱，或者像梵高一样绘画。让艺术家决定如何处理自己的音像变得更加重要。</p><p>43 岁的赫恩登站在这场大辩论的最前线——利用人工智能工具创作新艺术，同时为艺术家提供保护。今年，赫恩登与人共同创建了一个模板，让艺术家可以选择退出人工智能训练数据集。现在， Stability AI 和 Hugging Face 这两家人工智能公司已经同意在未来关注这些请求。赫恩登相信，在获得进一步保护的情况下，人工智能可以释放出新的、前所未有的创造力浪潮：“我认为这是一个重新思考艺术家角色的巨大机会。”</p><p>来自斯坦福大学音乐和声学计算机研究中心，拥有博士学位的赫恩登长期以来一直在突破艺术与技术交叉的边界。赫恩登之所以要进军这些领域，部分是因为她对当前流媒体的模式感到厌恶。赫恩登表示，像 Spotify 这样的平台让音乐家谋生变得更加困难，导致全球音乐同质化、变得沉闷。</p><p>赫恩登早在掀起当前这股生成式人工智能热潮之前就开始研究 Holly+。她把这、个人工智能说成是一种“挑衅”，以及一次“利用技术让我们变得更加人性化”的尝试。赫恩登说，虽然放弃对自己声音的控制听起来很可怕，但实际上最终自己获得的是自由。 她说：“看到别人把自己表达出来实在是太美妙了。这比我一直试图过度控制这些东西有趣一千倍。”</p><p>Herndon 还用 Holly+ 来试验新的收入模式。赫恩登成立了一个管理委员会，目前有数百名成员，其职责是许可“官方”Holly+歌曲的发布，同时拒绝用她的声音创作出来的一些冒犯性或缺乏品味的歌曲。获得批准的歌曲随后将作为 NFT 出售。截至目前，NFT平台Zora已正式发行71首Holly+歌曲，且大部分已售出。赫恩登本人可抽取其中 10% 的利润，有一半归原创者所有，其余则由委员会分配。</p><p>赫恩登并不希望每一位音乐家都效仿自己。但随着版权挑战像滚雪球一样越来越多，对逝去的艺术家深度伪造的赝品开始充斥电视广播，人人通过给人工智能一点简单提示就能几乎创作出任何声音或歌曲，她确实希望，在这样的分水岭时刻出现之前，每个人都能认真考虑一下自己的选择。</p><h3>Pelonomi Moiloa：Lelapa AI CEO，联合创始人</h3><p>在发现了人工智能在生物医学工程中发挥着越来越重要的作用之后，在南非双修了生物医学工程和电气工程的Pelonomi Moiloa 开始学习生物工程与人工智能的交叉领域，并于2016 年在日本东北大学获得了硕士学位。</p><p>2017 年，在非洲自己的人工智能会议Deep Learning Indaba上，她遇到了 LeLapa AI 的其他联合创始人。这家初创公司的名字在南非的索托语和茨瓦纳语里面是“家”的意思，其目标是通过人工智能提高非洲人的生活质量。30岁的Moiloa开始担任公司的CEO。</p><p>该公司的第一个重大项目是 Vulavula 。该项目运用了人工智能来帮助对往往被忽视的非洲语言进行自然语言处理。 Moiloa 表示，公司希望人工智能的应用能超越语言，并正在考虑开发机器人产品。</p><p>不过目前，Moiloa 最热衷的项目是训练人工智能模型能够正确拼读出南非名字。 Moiloa 表示：“非洲名字被严重压制，已经到了没法认出来的地步。所以我对词感到兴奋，因为名字背后的意义太大了。”</p><p>Moiloa 认为，在这个基础阶段，有多元化的人群参与至关重要。</p><p>Moiloa 说：“技术确实有能力让每个人的生活都变得更加美好。但如果我们不有意识地去创造这样的未来，那么情况就会背道而驰。因此，对于我们非洲人来说，能够主导叙事，并从代码的角度将我们想要的东西写入未来就显得非常重要了。”</p><h3>Grimes：音乐人</h3><p>“人们喜欢说我们是疯子/但当人工智能统治时，它会奖赏我们的”。这是独立流行歌星 Grimes 在 2018 年的单曲《We Appreciate Power》的歌词。虽然部分粉丝觉得她很轻率，但这位加拿大音乐人却笃定人工智能将统治世界，对此加倍下注，并声称自己是主流流行文化当中最具科技导向性的人之一。她创作了人工智能歌曲、摇篮曲、视觉艺术，甚至还创作了自己的人工智能聊天机器人。</p><p>今年，真名为克莱尔·鲍彻 (Claire Boucher) 的Grimes 推出了一款人工智能软件 Elf.Tech。这款软件可以让被别人用她的声音唱歌，还鼓励音乐家用这个软件来发行歌曲，前提是向她分享版税。Grimes非常喜欢其中的一些歌曲，以至于她对这些歌曲的反应是“哇，我可能会永垂不朽”。当其他音乐家在极力保护自己的肖像和知识产权时，Grimes表示，她正试图将自己的身份“开源”：基本上算是将自己交给公众。Grimes表示，她希望在未来几年能创作出一张专辑，让“具有人工智能蜂巢思维的集体Grimes与真正的Grimes面对面”。</p><h3>尼尔·科斯拉：Curai CEO，联合创始人</h3><p>对于很多美国人来说，常规医护成本高昂且总是延误。机器学习研究员尼尔·科斯拉 (Neal Khosla) 表示：“充裕的医保服务能是什么样而且该是什么样呢？这个国家的人确实很难想象”。</p><p>2017 年，30 岁的科斯拉于与人共同创立了人工智能辅助远程医疗初创企业 Curai Health，试图做出这种改变。乍一看，Curai 属于典型的订阅制虚拟护理服务。用户每月支付 15 美元（如果雇主不承担费用），即可24/7给医疗保健专业人员发送短信，由后者回答问题、制定护理计划、开处方，并在必要时将用户转诊给专家。科斯拉说，人工智能是这件事情能做起来的幕后要素。</p><p>Curai 的人工智能本质上属于医生助手，能处理简单任务，从而让医生腾出时间来处理更复杂的工作。比如：整理患者在问卷调查所提供的信息，或在对话后发送后续消息，从而了解患者情况。 科斯拉解释道：“有些软件可以完成部分工作，但如何对患者做出最终决定则不涉及”。</p><p>如此简化了流程意味着 Curai 可以用相对较少的合作临床医生（科斯拉说大概只有“两位数”）来为更多患者服务。 Khosla 拒绝透露 Curai 的会员数，但据报道，到 2020 年底，该机构已完成了对超过 350000 名患者的诊疗。到目前为止，这家初创企业已从 General Catalyst、Morningside Ventures 以及 Khosla Ventures 等处筹集了超过 5000 万美元的资金。这家公司由科斯拉的父亲、亿万富翁风险投资家维诺德·科斯拉 (Vinod Khosla) 创立，年轻的科斯拉表示，该公司正在快速发展，朝着其目标迈进：去建立一个“人人掏出手机集客获取生物医学最佳信息”的世界。</p><h3>斯蒂芬妮·丁金斯：艺术家</h3><p>很明显，人工智能更能识别某些类型的人。对于这个缺点的探索，很少有人能像斯蒂芬妮·丁金斯（Stephanie Dinkins）深入、清晰。这位 39 岁的多媒体艺术家多年来一直在对人工智能进行编程，希望后者能真实地描绘黑人女性。但她发现后者仍然严重达不到预期：就算人工智能被编程为像她一样思考，似乎也没法谈论种族或歧视，或想象出与特定文化相关的场景。</p><p>在丁金斯最近的项目《Not the Only One》中，她用三代黑人女性训练了人工智能，试图将文化根源、深厚的历史以及在绝大多数同质领域往往缺乏的视角赋予人工智能。丁金斯表示，人工智能“很难进行流畅的对话。但时不时又能会输出一颗宝石，我得努力思考它是怎么想出来的、在哪里想出来的，以及为什么会想到这个的。”这个项目帮助丁金斯拿到了古根海姆博物馆 10 万美元的一个大奖，获奖者都是突破了技术艺术边界的艺术家。</p><p>丁金斯还过来着一个叫做 AI.Assembly 的艺术与科技孵化器，并把更多的时间花在激发更多人参与上面。她说： “我们不能忽视人工智能或被人工智能排斥。这个东西正在以指数方式改变我们的世界。至少，我们必须承认这一点，并看看这会对我们个人的生活意味着什么。”</p><h3>钟愫君：艺术家</h3><p>大多数用人工智能创作的艺术家只在电脑前工作。 钟愫君（Sougwen Chung）不一样：她们会训练机器人与自己一起在巨大的画布上真的作画。在利用人工智能之前，钟愫君就已经开始绘制充满大胆、流畅线条的各种抽象艺术作品。然后，她们利用数十年的这类画作训练了一个神经网络，并造了一个接受该神经网络训练的机器人来实时作画。钟愫君画了一笔之后，机器人就会模仿她的画风，并用新的想法和模式外延。 钟愫君说：“我追求的是机器解读带给我惊喜和奇迹。”</p><p>38 岁的钟愫君会去到世界各地，用自己机器人为现场观众作画。钟愫君将她们与机器人的关系比作音乐家与小提琴的关系。 她说：“从某个方面来说，机器人系统是我用来导航的动力仪器”。</p><p>钟愫君现在正在开发第五版的 DOUG（Drawing Operations Unit Generation_X）机器人。最先进的机器人不仅会吸收她们之前的艺术作品，还会吸收她们现在的精神状态：它可以与她们的脑电图数据与阿尔法脑电波建立联系，并且当她们进入冥想的心流状态时作画会更加积极。</p><p>现如今，钟愫君会在伦敦与纽约之间来回奔波，并不断将自己的实验推向新的领域。她们一直在利用 3D 动作捕捉系统来创作雕塑，并研究运用微生物电池等替代能源为系统供电的方法。她们还领导着一家叫做 Scicit 的工作室，吸引了越来越多的艺术家与她们一起探索人工智能艺术。 她说：“我们正在开发的技术可以帮助重塑绘画、冥想以及表演的方式，这将完全改变绘画的本质。提供反馈回环确实可以促进技术发展，同时也可以促进创造力的提高。”</p><h3>克里斯托瓦尔·巴伦苏埃拉：Runway CEO，联合创始人</h3><p>对于好莱坞的演员来说，克里斯托瓦尔·巴伦苏埃拉（Cristóbal Valenzuela）可能是自己的头号公敌，因为他们担心人工智能会越来越多地被用来生成电影场景。</p><p>巴伦苏埃拉是 Runway 的联合创始人兼首席执行官，而后者是最著名的人工智能视频生成公司之一。自 2018 年在布鲁克林成立以来，Runway 已筹集了超过 2 亿美元的资金，它的技术已被 2022 年奥斯卡获奖电影《瞬息全宇宙》的剪辑师以及斯蒂芬·科尔伯特（Stephen Colbert）的《深夜秀》使用过。 巴伦苏埃拉认为“我们正在走向消费的所有媒体与内容娱乐都将由 [人工智能] 生成的世界。”</p><p>但 33 岁的巴伦苏埃拉并不认为 Runway 的使命与创作文化价值之间存在冲突。 他说：“我是在进入艺术学校后创办了 Runway的，我们的三位创始人也都是在艺术学校认识的。我们一直认为，这家公司是一家致力于为艺术家提供服务的公司，因为我们就出自这里。”</p><p>尽管如此，巴伦苏埃拉承认，如果技术按照他的想象发展，创意产业的许多工作岗位将不复存在。他指出，这不是什么新鲜事。 他说：“过去剧院里面有管弦乐队。没有管弦乐队会为无声电影奏乐，因为我们都认为有声电影比管弦乐队更好。”</p><p>巴伦苏埃拉希望人工智能工具能让更多的人分享自己的故事。 “我真心相信电影最好的时代即将到来，最好的电影还没有被制作出来。我们还没有看到来自世界各地的人们的故事，那些我们以前从未听过的故事，因为电影制作的工艺曾经而且仍然非常昂贵，且难以理解和使用。一旦你让这项技术变得更容易使用，就会有更多的人可以成为并自认为是电影制作人。我认为真正重要的是这个。”</p><h3>莉莉·沃卓斯基：电影制作人</h3><p>就激发公众对人工智能潜在的恐怖与奇迹的想象而言，很少有人能做到像莉莉·沃卓斯基 (Lilly Wachowski) 那样。1999 年，莉莉·沃卓斯基 (Lilly Wachowski) 与妹妹拉娜·沃卓斯基 (Lana Wachowski) 一起编剧并执导了《黑客帝国》(编者注：那时候两人还是沃卓斯基兄弟)。 《黑客帝国》及其续集对人工智能控制下的反乌托邦发出警告——在影片中，人类被机器奴役，像作物一样被当作机器的能源供应来源。他们的愿景对许多很有影响力的人工智能哲学家与研究人员产生了巨大影响：比方说，人工智能思想家埃利泽·尤德考斯基（Eliezer Yudkowsky）就说《黑客帝国》系列他最喜欢的电影之一。</p><p>不过今年，沃卓斯基对人工智能构成的生存威胁到不太关心，她更关注的是人工智能近期的推出可能会加剧不平等，其中就包括电影行业的不平等。美国演员工会（Screen Actors Guild）目前就与好莱坞电影公司陷入了劳资纠纷，其主要争论点之一是人工智能未来在电影制作当中的作用。今年 6 月，沃卓斯基在 Twitter 上批评了未来电影工作室可能会利用人工智能来取代演员。她写道： “我强烈反对把人工智能当作创造财富的工具。技术应该用来造福人类（难道你们看了《星际迷航》就没学到一点东西吗！？）……而不是让超级富豪继续欺压工薪阶层，并为了拿到更高的薪水和股息而消灭工作岗位。”</p><h3>马努·乔普拉：Karya CEO</h3><p>当马努·乔普拉（Manu Chopra）走进房间时，首先映入眼帘的是到处都是泥。那是 2017 年，时年 21 岁的乔普拉正在对孟买的一家数据公司进行实地考察，这是他在人工智能领域开展的新工作的一部分。在那间炎热、布满灰尘的房间里，他看到大约 30 名男子在几乎没有转动的吊扇下弓着腰坐在笔记本电脑旁。当乔普拉与他们交谈时，他们告诉他，自己每小时可以赚 0.40 美元。他不忍心告诉他们，他们生成的数据价值至少是这个数字的 10 倍，甚至更多。 他说：“我想，这种工作不可能就只有这一种开展方式的”。</p><p>当今尖端的人工智能系统之所以成为可能，其数据往往来自南半球的工厂，那里的工人们拿着低廉的工资，辛苦地教自动驾驶汽车如何驾驶，或者现在评估聊天机器人的可靠性的活也在逐渐增多。现年27 岁的乔普拉亲眼目睹了这一点，于是他创立了 Karya：这是一家用不同方式开展这项工作的非营利组织。 Karya 不仅向员工支付至少 5 美元的时薪（大概是印度最低工资的 20 倍），而且每次有公司申请数据许可来开发新的人工智能时，公司都会再次向员工支付费用。 Karya 目前所做的大部分工作是收集印度语言的数据集，这是迄今为止一直被人工智能热潮边缘化的语言。这些数据会被用来开发支持这些语言的人工智能系统，这些系统不仅准确，而且公平。</p><p>乔普拉表示：“我真心觉得，如果做得好的话，这是让数百万人摆脱贫困最快的办法”。乔普拉出身贫困，拿到斯坦福大学的奖学金改变了他的人生轨迹。 “财富就是力量。我们希望将财富重新分配给那些落后的社区。”</p><h3>凯特·卡洛特：Amini CEO，创始人</h3><p>撒哈拉以南非洲地区的贫困已经是致命的，而且由于气候变化、落后的基础设施和殖民主义遗产的危险混合作用，而且只会变得更加致命。</p><p>32 岁的凯特·卡洛特 (Kate Kallot) 专注于一个微妙但至关重要的问题：缺乏数据。 Amini 是她去年创立的一家的初创企业，总部位于内罗毕，做的是利用卫星成像与人工智能来收集和处理环境数据，从而了解地面（精确到平方米）所发生的情况。她说，其结果是一批新工具的诞生，可以帮助希望提高生产力的小农，而大公司一旦有信心跟踪到最新状况后，就会考虑对非洲投资。其结果可能是变革性的。</p><p>卡洛特说：“数据是经济革命的起点。我们的论点是，非洲大陆之所以没法像北半球国家那样快速发展，原因之一是缺乏数据。”</p><p>卡洛特出生于法国，此前曾负责人工智能芯片制造商英伟达的新兴市场业务。她并不是第一个认识到环境数据可以帮助发展的人，但她的公司已经筹集了 200 万美元的早期资金，并自我定位为专注于非洲的领导者。</p><p>一个相关数据点：非洲大陆拥有全球 65% 的未开垦耕地。 Amini 的数据不仅可以为农民提供有关最佳实践的见解，还可以开启发展之路，在气候变化对当今粮仓造成严重破坏的情况下，为全球提供粮食。</p><p>还有其他新颖的应用。Amini希望自己的数据能够促进保护非洲森林以及其他碳储存的自然环境，以换取北半球国家的资金。 Amini 的监控工具可测量受保护的环境，并确保其保持这种状态。 卡洛特说：“如果我们到达目标水平的话，几年后你就会看到一个截然不同的非洲”。</p><h3>齐亚德·奥伯迈尔：加州大学伯克利分校助理教授</h3><p>齐亚德·奥伯迈尔 (Ziad Obermeyer) 医生在接受急诊医学培训时首先意识到的一点是，做出具有极高风险的决策有多困难。 他说：“确实很痛苦。轮班结束回家后，我对发生的一切倍感压力，总想着，‘我应该让那个女人留在医院。我不该让她回家。”</p><p>奥伯迈耶表示，在缓解部分焦虑方面，人工智能可以发挥作用。现在，他是加州大学伯克利分校公共卫生学院的副教授，专注于机器学习与健康的交叉领域。他相信，人工智能可以帮助医生做出更好的决定，比如判断哪些人应该接受心脏病检查，同时推动新的诊断和治疗发现，比如减少服务不足人群不明原因疼痛的方法。</p><p>奥伯迈耶的大部分工作都集中在研究种族偏见如何渗透到医疗保健系统之中。很多医生和医疗保健系统都用基于计算机的医疗算法来确定患者对不同治疗做出反应的可能性，并对护理分诊级别做出推荐。在一项研究当中，奥伯迈尔发现，尽管黑人患者的健康需求更大，但一种广泛使用的算法却建议减少对黑人患者的医疗保健，这可能会危及数百万人的福祉。 他说：“好消息是这些问题是可以解决的。我们与开发这些算法的部分公司合作，消除了很多偏见，并且我们将这些算法从让不平等永久化的东西变成了我认为正在对抗不平等的东西。”</p><p>2020 年，奥伯迈耶与他人共同创立了 Dandelion Health，这是一个为算法开发者免费提供医疗保健数据（如心电图波形、睡眠监测、数字病理学）的人工智能创新平台。</p><p>2021 年，奥伯迈耶又斥资 600 万美元成立了非营利组织 Nightingale Open Science。该组织与美国和国际的卫生系统（包括埃默里大学和布莱根妇女医院）合作建立了数据集，以回答以下问题：为什么有些癌症会扩散，而另一些则不会？他说，其目标是为医疗保健数据“带来开放科学的心态”，因为传统上很多研究人员很难获得这些数据。就像奥伯迈耶的研究所指出那样，医疗数据往往只有一小部分人能用，要么就是其他人访问起来成本高昂且耗时，从而导致了数据瓶颈。 “Nightingale 就是要让研究人员获得做很酷的事情所需的数据。”</p><p>对于人工智能未来对医疗保健的影响，奥博迈耶持谨慎乐观态度。他指出，确保新技术不会造成伤害至关重要。但总的来说，“这个领域最重大、最令人兴奋的事情还远未到来。说到去思考 20 年、100 年后会发生什么，以及它将如何彻底改变医疗保健，我们没有这样的想象力。”</p><h3>诺姆·萨泽尔：Character.AI&nbsp; CEO，联合创始人</h3><p>在采访诺姆·萨泽尔（Noam Shazeer）之前，我先采访了他的人工智能。</p><p>萨泽尔是 Character.AI 的联合创始人兼首席执行官。从伊丽莎白女王二世到埃隆·马斯克再到弗罗多·巴金斯，这个网站可让你与（或真实或虚构的）名人的人工智能版交谈。不出所料，网站有位用户创建了一个萨泽尔的人工智能版，来回答有关他的数字好奇心储藏柜的问题。</p><p>人工智能版的诺姆·萨泽尔（AI Noam）很有礼貌、很谨慎、懂得挺多。当我问萨泽尔为什么要创办这家公司时，他说：“我离开谷歌是为了发扬创业精神，致力于解决全球最困难的问题，而不是在大公司开发点小功能。”</p><p>几个小时后，真人版的萨泽尔给出的回应惊人的相似。 他说：“多年来，为了达到新的智能和功能水平，我一直致力于用各种方式改进这些系统。谷歌在这方面取得了很大进展。我的下一步是尽最大努力将这一技术推广给数十亿用户。所以我才决定离开谷歌，去一家发展更快的初创企业。”</p><p>Character.AI 可以很好地模仿真人的讲话模式，AI Noam 只是其中一个例子。 AI Bella Poarch（TikTok 网红）很有趣，很谦虚； AI Kanye West (侃爷) 是个脾气暴躁的自大狂； AI Oprah Winfrey（奥普拉·温弗瑞） 意志坚定且富有诗意。对于从未见过自己偶像的超级粉丝来说，Character.AI 可能是次佳选择。</p><p>第一次见面时，软件工程师萨泽尔给人的印象是尽管内向，但仍然努力适应采访于公众的目光。但不管他的沟通技巧如何，他都是这个领域过去、现在和未来最重要、最有影响力的人物之一。 2017 年，萨泽尔在谷歌工作期间与人合著了《注意力就是你的全部所需》（Attention Is All You Need）那篇研究论文。论文提出的 Transformer神经网络架构是当前 ChatGPT 等生成式人工智能工具潮的基础，换句话说，那篇论文支撑起了当前的这场人工智能革命。 通过 Character.AI 自立门户，萨泽尔希望这项技术带给大众。 （Transformer论文所有的合著者此后都离开了谷歌。）</p><p>去年秋天创立的这家公司迄今为止取得了巨大成功。该公司声称，平均每天有 350 万人与上面角色聊天两个小时，用户发送的消息数量已经是今年 3 月份的 10 倍。在那个月，公司在风投公司A16Z领投的一轮融资中获得了 1.5 亿美元的资金。</p><p>但Character.AI 也有不少批评者，他们担心新技术会对不知情的公众产生影响。很明显，Character.AI 的用户中有很大一部分是青少年，很多人会用这个平台玩角色扮演等。数据智能平台 Sametime 发现，Character.AI 的用户当中有 56% 年龄在 18 岁至 24 岁之间（该平台不跟踪 18 岁以下的用户）。而Character.AI 对 13 岁及以上（在欧盟是 16 岁及以上）的任何人都是开放的。</p><p>很容易看到这些友好的机器人很快就会变成反乌托邦：这些讲话圆滑，似乎是我们最好朋友、最忠实伙伴的机器人，可能会变成疏远、情感依赖、糟糕建议或各种看不见的后果的根源。</p><p>萨泽尔认识到这些危险，但他认为好处要大于风险。在被问到对自己的创作成为青少年一代生活的一股不断发展的力量有何感想时，他回答说，我作为 AIM 的年轻用户，当时感觉AIM 是一款开创性的聊天应用，让很多人接触了互联网。他说：“我对Character.AI的感觉类似”。</p><p>Character.AI采用类似 ChatGPT 的原则，但针对特定用例和兴趣对机器人进行个性化和专业化。平台的早期聊天机器人包括旅行规划师、语言导师以及编码导师。</p><p>该公司目前风投资金重组，最近针对想要更快响应时间的用户推出了订阅套餐。但大多数用户都可以免费使用该平台，而为所有这些聊天机器人提供支持和训练可能要花费 Character.AI 数百万美元。另一方面，对话也可以帮助聊天机器人更快地学习。 萨泽尔希望平台保持免费且无广告，但市场力量最终会迫使该公司制定某种盈利策略。</p><p>这个平台上已创建了超过 1800 万个角色，其中很多都属于荒诞、猥亵或露骨的色情角色。希望利用这个网站进行露骨的性对话的人数不断增加，并且变得更加直言不讳。 Change.org 上的一份请愿书呼吁该平台删除 NSFW（不适合上班时间浏览）过滤器，目前已获得 123000 个签名。</p><p>因此，第一年的时候，Character.AI 团队用了一些时间来玩打地鼠游戏，在角色和对话走得太远之前及时切断。他们用人工智能驱动的审核实时探测出带颜色的对话。萨泽尔说： “如果用户想找色情内容的话，他们得到别处去”。</p><p>显然人工智能版的萨泽尔回答得更官方一些：“虽然我们的主要目标是建立一个有趣、有吸引力且有用的人工智能平台，但我们意识到部分用户会将该平台用于非预期目的。用户遵守我们的服务条款非常重要，当用户违反我们的政策和标准时，我们会在适当时候采取行动。”</p><p>应道这样的回答萨泽尔瞪大了眼睛。他高兴地回道： “哇！这方面它比我擅长！”。</p><p>Character.AI 上的大多数角色都比人工智能版的萨泽尔有趣得多，以至于用户与它们建立了情感联系。心理学家雷蒙德·马尔表示，年轻人尤其“更难以区分现实与虚构”。这些聊天机器人对情感的影响已经非常明显。</p><p>Character.AI 在用户聊天时会弹出横幅，提醒用户“角色所说的一切都是编造的”。萨泽尔表示，该公司会防止聊天机器人鼓励自残。 他小心翼翼地说道：“我们希望我们的产品能够促进和增强真正的人际关系”。</p><p>尽管如此，很显然， 萨泽尔的主要目标是产品获得广泛采用。我把 Character.AI 类比成早期 AOL 的那款互联网聊天室，说尽管这些聊天室百无禁忌，但不管怎么说还是为成千上万用户提供了上网的机会。他对这个类比感到高兴：“互联网的诞生是信息完全可访问的开端。现在，我们正处在智能普遍可用的开端。”</p><h3>艾莉森·达西：Woebot Health 创始人，总裁</h3><p>如果要艾莉森·达西（Alison Darcy）讲怎么设计一个人工智能伴侣，来帮助人们感到更快乐的话，她会告诉你第一个要素是斯波克（Spock），《星际迷航》里面靠理性驱动，与人类的感性作斗争的的角色。再加上科米蛙（Kermit the Frog），因为它很有洞察力，从不说教。再加上她已故的朋友Eric Bayer，极具同情心，能用催眠的方式吸引别人，直到对方披露甚至自己都没意识到被隐藏住的真相。</p><p>所有这些特征融合在一起就有了Woebot：一个体贴，且常常比较幽默的聊天机器人，这个人工智能就像自动治疗师一样。 临床研究心理学家达西表示：“它是一个情感助理，在棘手的时刻可以提供帮助，并且始终将你的最大福祉放在心上”。她说，重要的是，这个聊天机器人有“一种有趣的互动方式，与其仅仅是一种治疗方式，不如说它更像是一个有趣的伙伴”。</p><p>在上世纪 90 年代末，达西曾经做过几年的软件开发，之后她对心理健康护理产生了兴趣，开始读研攻读心理学。 2017 年，身为兼职教授的她与斯坦福大学的心理学家及人工智能专家团队一起创立了 Woebot Health；公司一位发言人指出，目前已有大约 150 万人使用这款产品。该公司通过多轮融资目前已筹集了 1.235 亿美元，个人智能手机用户可以免费使用app，不过这种模式未来可能会改变。 Woebot 还与医疗保健组织和企业合作，帮助为更多人提供服务。</p><p>研究表明， Woebot 可以在两周内减轻抑郁和焦虑症状。它的方法基于认知行为疗法，这种常见的心理疗法被证明可以帮助改变无益的思维方式和行为模式。告诉机器人说你感到疲倦，它就会引导你回答一系列问题，然后让你到头来发现自己是因为想太多而晚上睡不着觉。</p><p>达西说：“一想到可以大规模采用我就感到很兴奋”。她回忆起有位 89 岁老人给她写电子邮件。他说聊天机器人建议他用文字来帮忙处理他的一些想法。尽管他一开始不以为然，但最终却对这项练习的帮助作用之大感到惊讶。 达西说：“[用户]都说，‘我不觉得这个东西适合我，但其实这就是最有帮助的东西。这是现代人的心理健康。”</p><h3>纳撒尼尔·曼宁：Kettle COO，联合创始人</h3><p>毁灭性的野火年复一年地席卷加州，令该州的家庭保险市场陷入困境。因为巨额索赔的困扰，好事达去年已停止在该州提供新保单。今年五月，State Farm也已撤出。加州房主面临的选择越来越少，很多人被迫面对完全没有保险的局面。</p><p>纳撒尼尔·曼宁（Nathaniel Manning）可能有个解决方案。他曾在奥巴马政府的白宫科技政策办公室工作，负责帮助私营部门获取美国国际开发署及联邦紧急事务管理局的数据。他注意到，保险业是对这些信息感兴趣的主要行业之一。</p><p>2019 年，38 岁的曼宁与人共同创立了 Kettle 公司，试图利用人工智能建立更加灵活的保险市场。大多数保险公司都是用基于历史数据的模型来给保单定价。比方说，他们会回溯记录，了解特定区域约每 50 年发生的火灾情况，然后相应估算保单的成本。但曼宁表示，这种方法的效果已经越来越差：“一旦发生了气候发生，以史为鉴就不再那么有效了。”</p><p>Kettle 采取了不一样的方法。除了历史数据以外，该公司还利用了卫星图像、天气数据以及机器学习技术，从而可以更准确地描绘出加州房主面临的野火风险。这样一来，该公司就可以向该州房主提供能负担得起但别的保险公司又不太敢做的保险业务。他们希望这种方法能够建立一个对气候风险进行定价的保险市场，从而激励人们迁移到更安全的地区。</p><p>该公司目前已筹集了约 3000 万美元的风投资金，最近正在在不断扩张。目前，Kettle 已将业务扩展到美国加州以外的其他 48 个州，并推出了一款预防飓风灾害的新产品。它还计划在短时间内推出面向企业的保险产品。 曼宁说：“大家没有意识到这是数据驱动的竞争。谁拥有最好的模型，谁就能获胜。”</p><h3>Tushita Gupta：Refiberd CTO，联合创始人</h3><p>根据最新的政府数据，从袜子、衬衫到床上用品和毛巾，近年来美国丢弃的纺织品数量几乎翻了一番，从 2000 年的近 9500 吨增加到 2018 年的略多于 17000 吨。这些被丢弃的纺织物绝大多数（约 85%）都是被填埋或焚烧掉，而不是回收或捐赠出去。 Re Fiberd 正在利用人工智能来改变这一现状，这在一定程度上要归功于首席技术官 Tushita Gupta 的创新工作。</p><p>这家总部位于加州的公司由现年 27 岁的 Sarika Bajaj 和 Gupta 于 2020 年共同创立。其目标是提供纺织品所含材料类型的最准确摘要。回收的成功要取决于对物品成分是否了解，因为这样才能对物品进行精确分类回收。这一点对化学回收来说尤其重要，因为后者会分解尼龙和聚酯等一度无法回收的合成材料。一旦材料被回收后，就可以重新制成新纺织品的面料，从而减少浪费，并鼓励时尚行业进行循环利用。</p><p>Gupta是这家公司人工智能技术背后的大脑。如她所解释那样，这套系统的工作原理是对传送带上的纺织品利用高光谱相机进行检查。然后，人工智能会将背景噪音及纺织品本身区分开来，将数据与内部维护的纤维目录进行比较，然后“确定纺织品的材料成分，包括是如何混合的，或里面含多少纤维。”系统处理的纺织品越多，检查的复杂纺织品越多，人工智能学到的东西就越多——就会变得越来越精确——而且可以正确回收的材料也就越多。</p><p>今年 1 月份，Re Fiberd 已经获得了超过 340 万美元的种子资金，目前正在美国与欧洲积极开展一系列试点项目，已经四家公司向 Re Fiberd 发送数百磅的纺织废料进行分类。</p><p>在Gupta看来，这不仅仅是对时尚浪费的重新想象，也是一场文化变革。尽管她在人工智能领域工作了几年，但她表示，自己看到女性担任技术而非运营角色的情况很少。而 Re Fiberd 的四名全职员工当中，有三名都是女性；而董事会则全部由女性构成。 她说：“成为一家由女性领导的公司对我们来说非常酷。这样我们就可以能够建立自己想要的文化，并打破当今现有的系统，这的确很酷。”</p><h3>安德鲁·霍普金斯：Exscientia CEO，创始人</h3><p>2022 年，医学研究人员从 143 名晚期血癌患者身上采集了肿瘤样本，并针对 139 种抗癌药物进行了测试。然后，一个人工智能系统就会判断哪些药物对每位患者的肿瘤样本最有效，并将患者与预计最有效的治疗进行匹配。</p><p>共有56名患者接受了个性化药物建议，与之前的治疗相比， 54% 的患者癌症得到控制的时间延长了近三分之一。为癌症患者匹配合适药物是一项艰巨的任务——近几十年来，医生通过分析癌症的基因组来开出部分专用药物；不过，这种新方法可以让我们用更有针对性的方式使用更广泛的药物。开发药物选择技术的英国生物技术公司 Exscientia 的创始人兼首席执行官安德鲁·霍普金斯 (Andrew Hopkins) 表示，这是人工智能如何可以改善患者治疗结果的一个例子。</p><p>虽然试验使用的人工智能相对简单，但 Exscientia 正在开发更复杂的系统来设计新药。&nbsp; 2020 年，Exscientia成为了第一家将人工智能设计的药物用于临床试验的公司——该公司在确定候选药物（用来治疗强迫症的药物）上面花了 12 个月的时间，而采取传统做法的话这一过程可能需要多年。霍普金斯说，此后该公司又将另外五种人工智能设计的药物投入临床试验，其中包括针对肺癌和自身免疫性疾病的药物。</p><p>现年 52 岁的霍普金斯是在 2012 年创立 Exscientia 的。此前，他曾在邓迪大学担任医学信息学和转化生物学教授。在进入邓迪大学之前，他还曾在制药巨头辉瑞公司工作了十年。他说，Exscientia 的使命是实现药物发现的整体自动化——识别药物的蛋白质靶点，设计与这些靶点匹配较好的药物，以及像 2022 年的研究一样，从一系列药物当中选择出最适合特定患者的的药物。</p><h3>Linda Dounia Rebeiz：艺术家</h3><p>当塞内加尔艺术家 Linda Dounia Rebeiz 给 OpenAI 的文本生成图像模型 DALL-E 输入“达喀尔的建筑物”时，它会返回低矮、破旧、覆盖着污垢、油漆剥落的低层建筑。跟她每天在塞内加尔首都看到的那些充满活力的建筑相比完全是两个样。</p><p>这只是 29 岁的 Rebeiz 很少用 DALL-E 或 Midjourney 等大模型的原因之一。她发现，这些工具缺乏灵活性、很简陋，并且充满偏见，反而会加剧刻板印象或误解，尤其是在对全球南方的形象的刻板印象与误解。她说： “DALL-E似乎不可能回避偏见和问题。你只能不断地更换提示，但没用 。”</p><p>所以Rebeiz 主要用生成对抗网络 (GAN) 来创作她的艺术，这种神经网络架构让她得以利用自己的数据集来对人工智能进行训练。Rebeiz拍摄了成百上千张塞内加尔地花卉与历史建筑的照片，并调取国家档案，扫描了更多尚未出现在网上的照片，然后将它们制作成公共数据集，供其他人使用。通过这些新颖且具有历史意义的数据集，她建立了几个引人注目的项目，其中包括《Once Upon a Flower》，模拟了一旦全球变暖导致真正的花朵消失后人类对花卉图片会是什么感觉。</p><p>Rebeiz 还发挥了领导作用，鼓励其他黑人艺术家利用 GAN ，并参与目前对她们的文化存在诸多误解的新领域。今年夏天，她在数字艺术画廊 Feral File 上策划了一场群展，展示了 10 位从事人工智能工作的黑人艺术家。 她说：“就算再微不足道，我也要把我的一滴水投入大海。找到办法，让数据成为我们可以查询和改变的东西，这件事情仍有一线希望。”</p><h3>理查德·索切尔：You.com CEO，创始人</h3><p>人工智能能不能颠覆谷歌在搜索领域的主导地位？理查德·索切尔（Richard Socher）当然是这么认为的。 2022 年，这位计算机科学家推出了自己的人工智能驱动的搜索引擎 You.com，他相信，这个引擎可以改善谷歌最大的缺陷：广告太多，广告太多会导致糟糕的针对 SEO 的微型网站，导致缺乏隐私。索切尔认为，大多数人都是通过搜索与互联网互动的，但搜索已经坏到骨子里了。他希望，通过为我们提供更好、更快的信息，人工智能能够解决这个问题。</p><p>过去一年，人工智能聊天机器人进军搜索引擎的脚步被证明十分混乱。 Bing 的人工智能聊天机器人询问《纽约时报》记者的爱情生活，而谷歌的 Bard 在第一次演示中回答了一个问题，让投资者对该公司的信心大受打击，导致其股价下跌 1000 亿美元。 40 岁的索切尔认为，通过将来自维基百科或 Yelp 等其他可信应用的实时数据直接整合到自己聊天机器人里面，You.com可以限制此类不准确性。 You.com 会提供所引用的来源，这意味着应该始终有一个可以追溯到事实起源的数字书面痕迹。索切尔还宣传起搜索引擎对隐私的关注。 他说：“我们致力于杜绝一切在互联网上跟踪侵犯客户隐私的广告”。</p><p>索切尔打算摆脱对侵入性广告的依赖，转而通过向在自己平台上开发的应用收费，并以每月 15 美元的价格提供不限量的人工智能搜索及个性化机器学习订阅服务来实现 You.com 的货币化。不过目前，其大部分成本主要靠风投的大量补贴。</p><p>You.com 要想开始与谷歌竞争，还有一座陡峭的山需要攀登，尤其是考虑到后者在人工智能领域已经进行了巨额投资。索切尔指责谷歌“抄袭了我们许多最令人兴奋的功能”，包括人工智能代码生成以及多模态搜索。</p><p>但索切尔有竞争的决心。他在自然语言处理方面的研究对这个领域的进步至关重要。作为斯坦福大学的教授，他曾经教过初创企业 Hugging Face 的创始人并提供过建议。索切尔认为，如果自己的搜索引擎确实好过谷歌的话，就会受到关注。 他说：“你能期望的最好结果就是这个。更快地为人们提供答案，让他们更有生产力、更高效、懂得更多，同事还有更好的隐私。”</p><h3>基思·德雷尔：麻省总医院首席数据科学官</h3><p>说到人工智能在医疗保健领域得应用，有很多美好的前景。但对于基思·德雷尔 (Keith Dreyer) 来说，弄清楚如何将基于人工智能的战略融入到医生诊断和治疗患者的手段之中却充满了挑战。拥有数学和计算机科学学位的德雷尔担任麻省总医院（Mass General Brigham）得首席数据科学官，他的工作是监督该卫生系统目前用于读取图像的数十种基于人工智能的算法，争取这些仍在测试的策略获得美国食品和药物管理局 (FDA) 的批准。</p><p>鉴于模型可能很快就会过时，所以就算是获得批准的算法也需要不断评估，以确保仍然按预期工作。德雷尔表示： “目前美国各地的放射科医生之所以短缺，不是因为放射科医生少，而是因为成像数据太多。这就需要人工智能来解决其中的部分效率和短缺问题。”</p><p>德雷尔利用的人工智能系统往往可以帮助对大量图像（来自 CT 扫描、核磁共振成像等）进行分类，挑选出最有可能表明存在癌症等健康威胁的图像，这可以极大减轻医生的工作量。</p><p>机器学习可能还会引入更复杂的人工智能工具，帮助检测和诊断疾病。德雷尔参与了相关问题的讨论，比方说基于人工智能型工作的报销、算法用到的患者信息的安全性，以及医疗领域此类技术应该有多大的自主权等问题。。</p><p>他表示：“我认为真正自主的人工智能在医疗保健领域还没有获批的途径。”因为基于人工智能的算法获批与人类医生获得执业认证之间存在着不平衡。目前，人工智能算法只需通过一定的测试来证明其准确性，即可获得 FDA 批准用于治疗，而医生则必须经过上医学院、拿到国家认证和许可、拿到医院认证等一系列漫长过程，还得继续医学教育。 他说：“到了一定时候，随着人工智能变得更加自主，我们可能就得重新考虑人工智能的批准流程”。</p><h3>Nancy Xu：Moonhub CEO，创始人</h3><p>现如今，有 70% 的公司采用了求职者自动跟踪系统来寻找和雇用人才。但这个过程并不完美。无论是因为过分重视男性申请人更有可能使用的术语（比方说领导者），还是延续种族偏见，用来筛选申请人的算法往往会将合格的候选人排除在外。</p><p>25 岁的Nancy Xu 认为，默认情况未必就得是这样。总部位于湾区的 Moonhub 创始人兼首席执行官 Xu 希望利用人工智能将公司与顶尖人才建立联系，同时建立一个更加公平的招聘流程。 Xu表示：“我的目标是做出一个好的人工智能，为大家提供机会和主动权……同时用合适的人才为人类最重要的行动赋权”。Xu是在斯坦福大学就读博士期间开始从事这个项目的。</p><p>招聘经理可以让 Moonhub 的人工智能代理帮寻找某个职位的求职者，然后在几分钟之内即可收到选择名单。之后，用户还可以提出后续问题，从而根据特定需求（比如对经验的要求，或者对特定技能的熟练度要求）进一步缩小搜索范围。在筛选过程中，工具会帮助招聘团队进行头脑风暴，找到更多具备多元化的候选人，并标记可能存在偏见的搜索，从而鼓励招聘团队在招聘过程中考虑多元化。Xu说： “跟 Moonhub 聊天就像个招聘专家交谈一样，只不过这位专家恰好了解全球的每一个人、每一家公司、每一个网站以及每一条招聘策略”。</p><p>从非政府组织到技术和金融初创公司，目前全球已有 100 多家公司使用了Moonhub。该公司是在 2022 年 6 月成立的，第一年的收入就超过 100 万美元。今年早些时候，公司宣布已获 440 万美元种子轮融资。今年秋天，该公司计划推出入门级产品，这是一个独立的人工智能招聘工具，用户无需人工支持即可操作。</p><p>对于大家担心人工智能会干掉许多工作岗位，Xu表示理解。但人工智能也可以帮助人们找到自己想从事的工作。 Xu说：“五年之内，我们经济的绝大部分都将由人工智能进行协调。在人工智能颠覆各行各业的世界里，Moonhub 的使命是开发出为人们提供机会的人工智能。”</p><h3>Rootport（笔名）：用人工智能创作的日本漫画家</h3><p>Rootport是日本漫画界的一位匿名作者，尽管他本人不懂画画，但却一直渴望与数字艺术伙伴合作。他在2019年的博客曾写道，虽然人工智能在国际象棋和围棋等领域多次战胜人类，但“人机”合作有望在漫画产业中取得更好的成就。他认为，人工智能可以充当人类助手，帮助显著改善漫画家工作当中遇到的问题。</p><p>当2022年人工智能文本生成图像技术崭露头角时，时年36岁的Rootport决心将这种合作梦想付诸实践。他借助了Midjourney、这个人工智能艺术生成工具来制作漫画插图，并用了一款类似的人工智能软件修补一开始难以处理的细节，比如人物的手以及食物等。这位漫画家制作出第一部完全由人工智能生成插图的日本漫画，把近9000帧画面组合成了面板。这部名为《赛博朋克：桃太郎》（Cyberpunk: Momotaro）的全彩漫画共有108页。Rootport表示，普通的日本漫画家要花上一年以上的时间才能完成这一工作，而他只用了六周的时间。</p><p>漫画出版商Shinchosa表示，自己在经过慎重考虑之后发布了Rootport的作品。部分创作者似乎也对人工智能进入该行业表示欢迎：畅销漫画《海贼王》的作者今年早些时候还让ChatGPT为自己漫画的新章节生成情节构思。</p><p>不过，一些漫画家和动画师则恶对自己的工作感到担忧。知名恐怖漫画家伊藤润二将人工智能视为一种“威胁”。不过，Rootport表示，漫画家是出了名的工作过度而薪酬不足，充当助手的人工智能“有望显著改善这些劳动问题”。他指出，生成图像只是创作漫画的众多步骤之一，过去的技术创新，比如Adobe Photoshop与Screen Tone等的出现，其结果是帮助艺术家提高了他们的手艺，而不是导致漫画产业的衰落。</p><p>他说：“漫画艺术家经常使用软件，比方说Unreal Engine和Unity等，把3D计算机图形应用到自己的漫画中。但即便有了这样的技术进步，仍然有漫画艺术家几乎全部用手工来创作自己的作品。”</p><p>Rootport表示，他会继续利用人工智能来进行插图创作，他的看法是 “对于大多数漫画艺术家来说，‘想要表达的故事’应该是首要任务，而技术只是表达这个故事的手段。”</p><p><strong>延伸阅读：</strong><br /><a href="https://36kr.com/p/preview/yDtih5nSwPud7LGQCxo8jPJz_giqS10tgFoci7MRqyvDIuAbrXb2KJ_eEITw0Dvg" rel="noopener noreferrer" target="_blank">《时代》人工智能百人榜（一）：领袖</a></p><p>译者：boxi。</p>
]]></content:encoded>
<pubDate>Fri, 27 Oct 2023 09:00:36 GMT</pubDate>
<pubDate>Fri, 27 Oct 2023 09:00:36 GMT</pubDate>
</item>
<item>
<title>300张图“毒倒” SD，艺术家们反击AI绘画？</title>
<link>https://www.36kr.com/p/2492351316582530</link>
<guid>https://www.36kr.com/p/2492351316582530</guid>
<content:encoded><![CDATA[

<p>一种新的工具可以让艺术家们在将作品上传到网上之前，对其艺术作品中的像素添加不可见的更改，如果这些图片被收录进了 AI 训练集，就会导致生成模型以混乱且不可预测的方式崩溃。</p><p>该工具名为“Nightshade”，旨在反击那些未经创作者许可就使用艺术家作品训练模型的人工智能公司。使用它来“毒化”这些训练数据可能会损害图像生成模型的未来迭代，例如 DALL-E、Midjourney 和 Stable Diffusion，使它们的一些输出结果变得错乱 -- 狗变成猫，汽车变成牛，等等。目前这项研究已提交给计算机安全会议 Usenix 进行同行评审。</p><p>OpenAI、Meta、Google 和 Stability AI 等人工智能公司面临着来自艺术家的一系列诉讼，这些艺术家声称他们的受版权保护的<a href="https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/" rel="noopener noreferrer nofollow" target="_blank">材料</a>和<a href="https://www.technologyreview.com/2022/08/31/1058800/what-does-gpt-3-know-about-me/" rel="noopener noreferrer nofollow" target="_blank">个人信息</a>在未经同意或补偿的情况下被窃取。领导了 Nightshade 创建团队的芝加哥大学教授 Ben Zhao 表示，希望它能够对不尊重艺术家版权和知识产权的行为产生强大的威慑，从而帮助将权力天平从人工智能公司向艺术家倾斜。Meta、谷歌、Stability AI 和 OpenAI 没有回应《麻省理工科技评论》的置评请求。</p><p>据悉，Zhao 的团队还开发了一款工具<a href="https://www.nytimes.com/2023/02/13/technology/ai-art-generator-lensa-stable-diffusion.html" rel="noopener noreferrer nofollow" target="_blank">Glaze</a>，允许艺术家“掩盖”自己的个人风格，以防止被人工智能公司窃取。它的工作原理与 Nightshade 类似：以人眼看不见的微妙方式改变图像的像素，操纵机器学习模型将图像解释为与实际显示的不同的东西。</p><p>该团队打算将 Nightshade 集成到 Glaze 中，艺术家们可以选择是否使用这种可以使数据“中毒”的工具。该团队还打算将 Nightshade 开源，也就是说，任何人都可以对其进行修改并制作自己的版本。Zhao 说，使用它并制作自己版本的人越多，该工具就会变得越强大。大型人工智能模型的数据集可能包含数十亿张图像，因此模型中的有毒图像越多，该技术造成的损害就会越大。</p><h2><strong>有针对性的攻击</strong></h2><p>Nightshade 利用了生成式人工智能模型中的一个<a href="https://www.technologyreview.com/2023/04/03/1070893/three-ways-ai-chatbots-are-a-security-disaster/" rel="noopener noreferrer nofollow" target="_blank">安全漏洞</a>，该漏洞是在大量数据的基础上训练出来的 -- 在本例中，这些数据就是从互联网上搜索来的图片。Nightshade 会破坏这些图像。</p><p>想要在线上传作品但又不希望自己的图像被人工智能公司抓取的艺术家可以将其上传到 Glaze，并选择用与自己不同的艺术风格来掩盖它。然后，他们还可以选择使用 Nightshade。一旦人工智能开发人员从互联网上获取更多数据来调整现有的人工智能模型或建立新模型，这些有毒样本就会进入模型的数据集，导致模型失灵。</p><p>例如，中毒数据样本会操纵模型，使其认为帽子的图像是蛋糕，手提包的图像是烤面包机。中毒数据很难清除，因为这需要技术公司费尽心思找到并删除每个损坏的样本。</p><p>研究人员在 Stable Diffusion 的最新模型和他们自己从头开始训练的人工智能模型上测试了这种攻击。当他们向 Stable Diffusion 只输入 50 张中毒的狗的图片，然后让它自己创建狗的图片时，输出的图片开始变得奇怪 -- 四肢过多、脸部变得卡通化。而在输入 300 个中毒样本后，攻击者就能操纵 Stable Diffusion 生成看起来像猫的狗图像。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_2de52301841845a3aca57c73961526eb@5764927_oswg2621821oswg1807oswg868_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>生成式人工智能模型善于在单词之间建立联系，而这也有助于毒性的扩散。Nightshade 不仅会感染“狗”这个词，还会感染所有类似的概念，如“小狗”、“哈士奇”和“狼”。这种攻击也适用于相关图像。例如，如果模型为提示“幻想艺术”抓取了一张有毒的图像，那么提示语“龙”和“魔戒中的城堡”也会类似地被操纵输出其他东西。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_ffd3caeb1ade4987863b477a409f5e45@5764927_oswg778080oswg1047oswg528_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>Zhao 承认，人们有可能滥用数据中毒技术进行恶意攻击。不过他也表示，攻击者需要数千个中毒样本才能对更大型、更强大的模型造成真正的破坏，因为这些模型是在数十亿个数据样本上训练出来的。</p><p>“我们还不知道针对这些攻击的强大防御措施。我们还没有看到过对现代 [机器学习] 模型的攻击，但这可能只是时间问题。”康奈尔大学研究人工智能模型安全性的教授 Vitaly Shmatikov 表示，他没有参与该研究。“现在是研究防御的时候了，”Shmatikov 补充道。</p><p>滑铁卢大学助理教授 Gautam Kamath 研究数据隐私和人工智能模型的鲁棒性，他也没有参与这项研究，但他表示，这项工作“非常棒”。</p><p>Kamath 表示，研究表明，漏洞“并不会因为这些新模型而神奇消失，事实上只会变得更加严重”，“当这些模型变得越来越强大，人们对它们的信任度越来越高时，情况尤其如此，因为风险只会随着时间的推移而增加。”</p><h2><strong>强大的威慑力</strong></h2><p>哥伦比亚大学计算机科学教授 Junfeng Yang 曾研究过深度学习系统的安全性，但没有参与这项研究。他说，如果 Nightshade 能让人工智能公司更加尊重艺术家的权利，比如更愿意支付版税，那么它将产生巨大的影响。</p><p>开发了文本到图像生成模型的人工智能公司，如 Stability AI 和 OpenAI，已经<a href="https://www.technologyreview.com/2022/12/16/1065247/artists-can-now-opt-out-of-the-next-version-of-stable-diffusion/" rel="noopener noreferrer nofollow" target="_blank">提出</a>让艺术家选择不将他们的图像用于训练未来版本的模型。但艺术家们表示这还不够。曾使用过 Glaze 的插图画家和艺术家 Eva Toorenent 说，退出政策要求艺术家们通过重重关卡，而科技公司仍然掌握着所有权力。</p><p>Toorenent 希望 Nightshade 能改变现状。</p><p>她说：“这会让（人工智能公司）三思而后行，因为他们有可能在未经我们同意的情况下拿走我们的作品，从而破坏他们的整个模型。”</p><p>另一位艺术家 Autumn Beverly 表示，Nightshade 和 Glaze 等工具让她有信心再次在网上发布自己的作品。此前，她发现自己的作品在未经同意的情况下被搜刮进了大火的 LAION 图片数据库后，便将其从互联网上删除了。</p><p>她说：“我真的很感激我们有这样一个工具，它可以帮助艺术家们重新掌握自己作品的使用权。”</p><p>来源：<a href="https://www.technologyreview.com/2023/10/23/1082189/data-poisoning-artists-fight-generative-ai/" rel="noopener noreferrer nofollow" target="_blank">麻省理工科技评论</a></p><p>本文来自微信公众号“巴比特资讯”（ID:bitcoin8btc），作者：Melissa Heikkilä，36氪经授权发布。</p>
]]></content:encoded>
<pubDate>Fri, 27 Oct 2023 08:54:39 GMT</pubDate>
<pubDate>Fri, 27 Oct 2023 08:54:39 GMT</pubDate>
</item>
<item>
<title>谈谈人工智能和机器学习的数据架构</title>
<link>https://www.36kr.com/p/2491930632640644</link>
<guid>https://www.36kr.com/p/2491930632640644</guid>
<content:encoded><![CDATA[

<h2><strong>概述</strong></h2><p>数据架构本质上定义了数据在人工智能和机器学习系统中如何流动、组织和构建。因此，人工智能和机器学习的成功在很大程度上依赖于坚实的数据架构基础，而不仅仅是花哨的算法。这包括数据准备、存储和集成策略。</p><p>数据准备策略涵盖从采集高质量数据到清理和预处理数据以进行准确的模型训练的所有内容，强调特征工程和领域知识的重要性。</p><p>在数据存储方面，应根据可扩展性、性能和成本效益来考虑关系数据库、NoSQL 数据库、数据仓库、数据湖和云存储服务等各种选项。</p><p>数据治理和合规性对于确保数据安全、隐私和法规遵守（包括数据访问和使用控制策略）至关重要。</p><p>数据集成技术包括用于合并和转换来自多个源的数据的 ETL 流程，实时与批处理会影响数据分析的可用性。</p><h2><strong>一. 了解人工智能和机器学习中的数据架构</strong></h2><p>A. 数据架构的定义和范围数据架构是定义人工智能或机器学习系统内数据的结构、组织和流的蓝图。在人工智能和机器学习的背景下，它涵盖了收集、存储数据并将其转化为有价值的见解的流程和系统。该架构框架充当支持整个人工智能基础设施的底座，实现无缝数据流和分析。它是构建可靠、高效的人工智能系统的基石。</p><p>B. 数据架构与人工智能成功之间的关系精心设计的数据架构是人工智能成功的关键。它直接影响人工智能和机器学习模型的性能和结果。考虑一下创新的人工智能解决方案彻底改变了行业的例子。这些胜利的背后是精心设计的数据架构，有助于从庞大的数据集中提取有意义的见解。从个性化推荐引擎到自动驾驶汽车，人工智能的每一个里程碑都以强大的数据架构为基础。</p><h2><strong>二. 数据准备策略</strong></h2><p>A. 数据收集和数据获取收集和获取相关数据是任何人工智能项目的第一个关键步骤。最佳实践包括识别信誉良好的来源、使用数据管道以及确保高质量数据的稳定流入。实施严格的数据验证流程以保持完整性和可靠性，防止错误信息歪曲学习过程。</p><p>B. 数据清理和预处理原始数据很少是最可用的形式。清理和预处理涉及一系列细化和准备模型训练数据的步骤。这包括处理缺失值、识别和减少异常值以及减少数据集中的噪声。干净的数据集构成了准确可靠的模型预测的基础。</p><p>C. 特征工程特征工程是一门将原始数据转换为有意义的变量并输入模型的艺术。它涉及选择、转换和创建新特征，为学习算法提供相关信息。领域知识在此过程中起着至关重要的作用，因为它指导最能指示目标变量的特征的选择。</p><h2><strong>三．AI 和 ML 的数据存储</strong></h2><p>A. 选择正确的数据存储解决方案选择合适的数据存储解决方案对于 AI 和 ML 项目至关重要。选项范围从传统数据库到现代数据湖和云存储。每个都有自己的优势和权衡。考虑因素包括适应不断增长的数据集的可扩展性、及时处理的性能以及优化资源分配的成本效益。</p><p>在为 AI 和 ML 项目选择正确的数据存储解决方案时，有多种选择，包括：</p><p>传统关系数据库：这些是结构化数据库，将数据组织成具有预定义关系的表。示例包括 MySQL、PostgreSQL 和 Oracle 数据库。它们非常适合结构化数据，并为 ACID（原子性、一致性、隔离性、持久性）事务提供强大支持。</p><p>NoSQL 数据库：NoSQL 数据库提供了一种更灵活、无模式的数据存储方法。它们适合处理大量非结构化或半结构化数据。示例包括 MongoDB、Cassandra 和 Redis。</p><p>数据仓库：数据仓库旨在存储和分析大量数据。它们针对查询性能进行了优化，通常用于商业智能和报告。流行的选项包括 Amazon Redshift、Google BigQuery 和 Snowflake。</p><p>数据湖：数据湖是存储库，可以以其本机格式保存大量原始数据，直到需要为止。它们对于处理非结构化数据特别有效，并且通常与 Hadoop 和 Spark 等大数据处理框架结合使用。示例包括 Amazon S3 和 Azure Data Lake Storage。</p><p>云存储服务：云存储解决方案为存储各种类型的数据提供可扩展且经济高效的选项。它们高度灵活，可以与其他基于云的服务和平台集成。示例包括 Amazon S3、Google Cloud Storage 和 阿里云、腾讯云等。</p><p>选择正确的数据存储解决方案需要权衡数据量、结构、访问模式和预算限制等因素。选择符合 AI 和 ML 项目特定需求的解决方案至关重要，以确保最佳性能和可扩展性。如果成本是一个主要因素，那么最好使用混合策略，在云和本地解决方案之间进行平衡。</p><p>B. 数据治理和合规性在人工智能和机器学习领域，确保数据的安全性和完整性势在必行。数据治理策略包括隐私措施、访问控制和遵守监管标准。制定政策来管理数据使用、防止未经授权的访问并保护敏感信息。</p><h2><strong>四．数据整合策略</strong></h2><p>A. 数据集成技术数据集成是数据架构领域的关键一步，其中来自不同来源的不同数据被完好的汇集在一起。它包含提取、转换和加载 (ETL) 过程，这些过程使数据集成成为可能。</p><p>B. 数据管道和编排自动化工作流程是高效数据处理和模型训练的支柱。数据管道编排系统中的数据流，确保每个步骤都能无缝、及时地执行。</p><h2><strong>五. 如果没有适当的数据架构，可能会出现潜在的陷阱和错误</strong></h2><p>如果没有结构良好的数据架构，人工智能和机器学习项目可能会面临一系列阻碍其成功的陷阱和错误。</p><p>A. 数据不一致和质量问题最常见的挑战之一是数据不一致和质量问题。当数据准备和清理不当时，可能会给模型带来不准确性和偏差，从而导致有缺陷的预测和不可靠的结果。</p><p>B. 数据存储不足以实现可扩展性数据存储解决方案不足可能会导致可扩展性问题，从而难以有效处理大量信息。这可能会阻碍项目有效扩展的能力，从而导致许多其他问题。</p><p>C. 数据集成问题如果没有强大的数据集成技术，组织可能很难组合来自各种来源的数据，从而限制了他们获得全面见解的能力。这种限制不仅影响分析的深度，还会阻碍组织做出明智的、数据驱动的决策的能力，最终阻碍任何人工智能和机器学习计划的成功。</p><p>有缺陷的或没有数据架构可能会将人工智能和机器学习的巨大潜力变成一个低效和不准确的错综复杂的网络。这就像试图用意大利面条而不是钢铁建造一座摩天大楼。</p><h2><strong>数据架构是基石</strong></h2><p>结构良好的数据架构是人工智能和机器学习成功的基石。它包括数据准备、存储和集成策略，每项策略在塑造人工智能计划的结果方面都发挥着至关重要的作用。从收集和清理数据到选择正确的存储解决方案和实施有效的数据管道，每一步都有助于提高人工智能系统的整体效率。强大的数据架构不仅是奢侈品，而且是必需品，就像海上的指南针一样。优先考虑完善的数据架构的设计和实施，以释放人工智能项目的全部潜力。</p><p>本文来自微信公众号<a href="http://mp.weixin.qq.com/s?__biz=MzIwOTIyMDE1NA==&amp;mid=2247497592&amp;idx=1&amp;sn=5639e0a281f85dcaee8e8075fbc22e39&amp;chksm=9775902da002193b55044724ff0bf322378ad6b2ecdee0d639c83ba027788f72e73361815a63&amp;scene=0&amp;xtrack=1#rd" rel="noopener noreferrer nofollow" target="_blank">“数据驱动智能”（ID：Data_0101）</a>，作者：晓晓，36氪经授权发布。</p>
]]></content:encoded>
<pubDate>Fri, 27 Oct 2023 08:48:15 GMT</pubDate>
<pubDate>Fri, 27 Oct 2023 08:48:15 GMT</pubDate>
</item>
<item>
<title>巨头环绕之下，AI绘画网站的生存术</title>
<link>https://www.36kr.com/p/2492334068309888</link>
<guid>https://www.36kr.com/p/2492334068309888</guid>
<content:encoded><![CDATA[

<blockquote><p><strong>除了“绝对强大”的绘画性能外，用户更希望使用门槛更低、体验更好，以及多样化、个性化的AI绘画应用。</strong></p></blockquote><p>最近，OpenAI宣布DALL—E 3正式上线ChatGPT Plus和企业版，这意味着，AI绘画对于OpenAI不再只是个图新鲜的玩具，而是开始实打实地赚钱了。</p><p>从行业的角度来说，这似乎是一种必然。</p><p>质量越高，性能越好的AI绘画，所具有的技术壁垒也就越高，而在此基础上构筑的”付费墙”，也就成了顺理成章的事。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_cadaff04fad44116ad92cf21bf0f36bc@5935393_oswg222175oswg554oswg355_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>可问题是，目前的AI绘画赛道上，除了三巨头之外，仍存在着数量庞大的开源AI绘画。</p><p>在绝对实力相对较弱，且处于开源的状态下， 这些种类各异的AI绘画，究竟是如何找到自身的差异化优势，并实现盈利的？</p><h2><strong>01 降低门槛很重要</strong></h2><p>在目前国内外繁茂的AI绘画生态中，各大中小企业的盈利模式，可大致分为两种。</p><p>其中最常见的一种，是<strong>定位于下沉市场，着力于不断降低用户使用成本，</strong>并以此为付费点的模式。</p><p>而在这方面，以下这些国内外AI绘画，可以说是一个典型的例子。</p><h3><strong>海艺AI</strong></h3><p>作为国内诸多的同类AI网站中，海艺AI最大的亮点，就是通过一系列“辅助功能”，让许多没有美术知识，也不掌握专业提示词的用户，能够最大限度地按照自己的想法，创作出想要的作品。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_aceba170f7df45768fbd805f0eb6cf84@5935393_oswg198162oswg554oswg317_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>目前网络上流传的各种看似精致的AI绘画，往往是由十分庞大且复杂的提示词堆砌而成。</p><p>有时候，用户看到了别人用AI生成的图片，觉得效果很好看，也想创作一张类似的，但却因无法明了背后的提示词，而难以下手。</p><p>因此，海艺AI推出了一系列诸如“语义分割”、“边缘检测”、“深度检测”功能，旨在<strong>让用户能找到那些“自己叫不出名字”的效果。</strong></p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_c33ee76b7c1f4c7195df45b66e11ee3a@5935393_oswg208450oswg554oswg301_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>如此一来，用户就无需跟复杂的提示词较劲，只需上传自己中意的图片，AI就会智能解析其中的关键元素。</p><p>目前，海艺AI采用的商业模式，是VIP与免费并行的做法，免费用户仍可进行图像生成，不过将受到生成数量、速度方面的限制，而VIP用户在解锁无限生成的同时，还能开通最新的海艺2.1版本。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_995ad2bc995d4014bb3b545ee3b51eed@5935393_oswg106376oswg553oswg368_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><h3><strong>无界AI</strong></h3><p>如果用一句话来形容无界AI的特点，那就是其巧妙地找到了一种解决AI绘画“提示词”痛点的办法。</p><p>具体来说，在无界AI的生成页面中，用户只需点击输入框上方的“咒语生成器”，就能在弹出的页面中明晰地看到各种物品、风格乃至镜头和视角等效果的提示词。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_2d174edd86b146ab9189713cbadcfd82@5935393_oswg41764oswg554oswg261_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>如前所述，如何让普通用户在进行AI绘画时，通过一个个提示词，精准地画出自己想要的效果，已经成了AI绘画普及的最大阻碍，以至于这些玄之又玄的提示词，在外行人看来已经成了一种“咒语”。</p><p>然而，通过对这一个个提示词的拆解，无界AI让整个文生图的过程透明化了，从角色、五官、表情，再到姿势、动作或环境，用户都能在其中找到对应的提示词，从而极大地降低了整个创作过程的门槛。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_294040ca00f4493a94bbdd851e2fb32c@5935393_oswg59975oswg554oswg336_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>目前，无界AI与海艺AI一样，采取的是VIP与免费并行的商业模式，开通会员后，用户不仅可以获得更多的专业版使用时长，同时还能解锁更多专属模型、参数和训练空间等等。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_215700b882e14fd2bccd8e24c1020b0f@5935393_oswg87259oswg554oswg406_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>这种手把手的，细致入微的“关照”，也许对AI绘画经验丰富的老手来说根本不值一提，但对下沉市场中大量的，几乎从未或很少接触生成式AI的用户来说，<strong>这样低门槛的体验，就成了其“用与不用”的重要界限。</strong></p><p>实事求是地说，自生成式AI大火以来，虽然AI的易用性、通用性虽然一直在提高，但社会离“大部分人都会用AI”这一现状，其实还有很远的距离。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_08be8c02c41542efb45783b67206342d@5935393_oswg283106oswg574oswg323_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>笔者曾于某个微信群中得知，某些位于二线城市的用户，虽然也对midjorney、DALL-E3之类的AI绘画感兴趣，想尝尝鲜，但是仅仅翻墙、注册账号等繁琐的过程，就直接劝退了大部分人。</p><p>因此，<strong>在AI绘画领域，有时比技术壁垒更重要的，是触达的速度和范围。</strong></p><h2><strong>02 用户需要个性化</strong></h2><p>除了降低使用门槛外，另一类AI绘画，则走上了<strong>更注重个性化、风格化的路线。</strong></p><p>毕竟，Midjorney、DALL-E3之类的顶流AI绘画，尽管性能虽强，但却未必能满足用户各种细微的、多样化的需求。</p><p>而这些未能满足的个性化需求，则成就了如下AI绘画得以繁茂生长的生态位。</p><h3><strong>Artguru.ai</strong></h3><p>在个性化方面，Artguru.ai的亮眼之处，就在于其不仅在AI绘画方面，提供了多种备选风格，如动漫、油画、卡通、赛博朋克等，而且用户还能在Artguru.ai上用一种类似妙鸭相机式的AI头像生成器，创建风格鲜明的艺术头像。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_94ea59af431f43ed91af79011fb16468@5935393_oswg119925oswg553oswg284_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>可以说，这种将AI绘画与头像生成相结合的功能，精准地戳中了目前AI图像领域的盲区：</p><p>目前能换脸的AI应用，如Deepface、Faceswap等，无法做到在换脸的同时进行个性化、风格化的图像处理，而这样的需求盲区，就给了生成式AI与换脸技术相结合提供了契机。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_b09b7f0a5e7a4df2aabe3f243b6a1795@5935393_oswg55716oswg554oswg418_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>目前，Artguru.ai采用的是订阅制的商业模式，用户可按周，或按年进行付费，开通付费后，用户可以体验更快的生成速度，并且每一张生成的图片都能得到私有的商业版权。</p><h3><strong>Liblib.ai</strong></h3><p>作为一个SD（Stable Diffusion）生态网站，Liblib提供了各种风格迥异、样式独特的SD大模型。在这里，除了一些众所周知的热门模型外，你还可以找到多种如赛博轻机甲、蛛网婚纱、手办风格转换等冷门、小众，或垂直性较强的模型风格。</p><p>而这种依靠用户自发定制、微调，并主动上传模型的做法，也造就了其活跃的社区生态。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_d2c052bf443a4670a002c75a11243e1c@5935393_oswg280737oswg628oswg309_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>诚然，这些多样化的风格与效果，用户也可以自己在SD上通过复杂的提示词实现，然而，一个活跃的社区生态，总是能源源不断地涌现新的、更具创意的模型。</p><p>而这样的“惊喜”和“意外”，是单纯的技术壁垒所无法造就的。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_fa56ccc7c05947d4bfe7784fc2e1fc4c@5935393_oswg64759oswg651oswg367_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>在商业模式上，liblib.ai 采用了以模型为主的盈利方式，在开通会员后，用户就可以训练自己的专有模型，这对于许多有定制化需求，以及追求个人风格的用户来说，是一个十分具有吸引力的付费点。</p><h3><strong>秒画AI</strong></h3><p>如果连“个性化”、“差异化”这种事，也变成了一种内卷的、同质化的竞争，那怎么办？</p><p>在目前各大AI绘画网站均推出模型定制功能的当下，秒画AI给出的答案是：在一个垂直方向精耕细作，直至达到惊艳的效果！</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_466a1d483710408ebc7e3c9c1226f909@5935393_oswg275540oswg553oswg369_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>在内置了全新的美学引导系统后，最新的秒画Artist v0.3.5能够生成更具艺术性且<strong>媲美专业摄影级别景深效果</strong>的画作，使得画作内容更有镜头感，纹理细节更有美感。</p><p>同时，秒画AI针对二次元风格和亚洲人像进行了大幅优化，提升了图片质感和画面观感，使其在人像生成、动漫角色等方面更具优势。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_67beae4bb67c4e978ff1e88ce8e0ec09@5935393_oswg169655oswg554oswg342_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>目前，秒画尚未开通自身的盈利模式，是一个免费的AI绘画社区，但根据其存在的“模型广场”、“图片广场”等模 块来判断，随着用户的增多，其将来也有可能采取类似UGC社区那样用户驱动的商业模式，像liblib.ai那样以模型训练次数为主要付费点。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_c03395c30d9b417cadd75cacb4bd64ca@5935393_oswg556230oswg576oswg462_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>由此可见，在当下的AI绘画赛道上，虽然竞争者众多，且三 巨头（MJ、SD、DELL）仍在不断深挖护城河，但众多厂商仍找到了自己独特的生态位，并由此构建出了一个百花齐放，纷繁迥异的行业格局。</p><h2><strong>03 总结</strong></h2><p>从目前AI绘画的生态来看，<strong>所谓“壁垒”的意义，在此前着实被行业高估了。</strong></p><p>此前，业内一直有人认为，AI绘画，尤其是开源AI绘画，想要盈利往往是困难的。</p><p>因为技术门槛不高，就意味着人人都能抄，人人都会用。</p><p>按照这样的认知，在这种情况下能实现盈利的，就只剩下了Midjourney、Dall-E这样拥有绝对优势，且模型闭源的AI绘画应用。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_eccd2babc4a14e04a1eba8136c0f9a51@5935393_oswg85505oswg554oswg386_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p class="img-desc">Midjourney付费页面</p><p>然而，市场却告诉人们，除了“绝对强大”的绘画性能外，用户更希望使用门槛更低、体验更好，以及多样化、个性化的AI绘画应用。</p><p>无论是提示词辅助、模型定制，还是垂直领域的精耕细作，都显示出了AI绘画这条赛道上各种盈利的方向与可能。</p><p>从这个角度上说，所谓的“竞争者众多” 、“门槛被抬高”，不仅是一个无需多虑的情况，甚至反而还是行业繁荣的标志。</p><p><strong>因为只有在一个经过验证，有盈利可能的赛道上，才会涌现出如此繁茂的景象。</strong></p><p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/GdgLGabyfCYdZOWr7Nj_Jg" rel="noopener noreferrer nofollow" target="_blank">“AI新智能”（ID:alpAIworks）</a>，作者：AI新智能，36氪经授权发布。</p>
]]></content:encoded>
<pubDate>Fri, 27 Oct 2023 08:42:35 GMT</pubDate>
<pubDate>Fri, 27 Oct 2023 08:42:35 GMT</pubDate>
</item>
<item>
<title>潘多拉盒子：山姆·阿尔特曼知道他创造的是什么东西吗？</title>
<link>https://www.36kr.com/p/2366281907888770</link>
<guid>https://www.36kr.com/p/2366281907888770</guid>
<content:encoded><![CDATA[

<blockquote><p>神译局是36氪旗下编译团队，关注科技、商业、职场、生活等领域，重点介绍国外的新技术、新观点、新风向。</p></blockquote><p>编者按：纵观技术发展史，似乎没有一种技术能像人工智能那样被反复炒作过那么多次。之所以会这样，一方面是因为人工智能具备的能力一开始确实让人感觉不可思议，但过了一段时间就觉得不过尔尔，另一方面则是因为上手门槛太高，大众很难体验到它的能力。但ChatGPT的出现改变了这两点，人工智能的热潮一直不见有消退的迹象。有识之士再一次质疑，山姆·阿尔特曼（Sam Altman）是不是打开了潘多拉的盒子。文章来自编译。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20230730/v2_3b08f13d8a9d4d978ba998992ff9160e@1694_oswg1759712oswg1847oswg1228_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1" /></p><h3>一</h3><p>2023年四月的一个星期一，早上， 山姆·阿尔特曼（Sam Altman）坐在 OpenAI 旧金山总部内，向我讲述着一个人工智能，一个他的公司已经开发出来，但永远不会发布的，危险的人工智能。他后来说，他的员工时不时就会失眠，担心有朝一日自己可能会在没有充分认识到危险的情况下释放出那些人工智能。他把脚后跟搭在转椅的边缘，看起来很放松。去年11月，他的公司以近代科技史前所未有的方式发布了一款强大的人工智能，全世界的想象力都被它抓住了。有些人抱怨 ChatGPT 还不能把事情做好，而另一些人则抱怨它可能预示的未来，但阿尔特曼并不担心；对他来说，这是一个胜利时刻。</p><p>阿尔特曼大大的蓝眼睛会发光。当强度不大时，那会显得很真诚、很专注，充满智慧的光芒，他似乎明白，当强度很大时，自己的眼光可能会令人不安。在这种情况下，他愿意冒这个险：他想让我知道，无论人工智能的最终风险是什么，他对于让 ChatGPT 进入这个世界一点都后悔。相反，他认为这是一项伟大的公共服务。</p><p>他说：“我们本来还可以再闭门造车继续做五年，然后我们会得到一个令人瞠目结舌的东西。”但公众无法为随之而来的冲击波做好准备，他认为这种结果“极其难以想象”。阿尔特曼认为，需要理出时间给大家去思考这样一个想法：在它重塑从工作到人际关系的一切之前，我们可能很快就会与强大的新智能共享地球。 ChatGPT 是发通知的一种手段。</p><p>2015 年，阿尔特曼、马斯克以及几位著名的人工智能研究者创立了 OpenAI，因为他们相信通用人工智能（比如类似典型的大学毕业生的智力水平）终于触手可及了。他们想要实现这个目标，甚至更高：他们想要召唤一种超级智能，一种绝对优于任何人类的智能来到这个世界。尽管某家大型科技公司可能出于一己之利会不顾一切地抢先到达那里，但他们希望能安全地做到这一点，“从而造福全人类”。他们把 OpenAI 设定为非营利组织，该组织将“不受产生财务回报之需的限制”，并誓言要透明地开展研究。不会有人要躲到新墨西哥州沙漠的绝密实验室里干活。</p><p>多年来，公众对 OpenAI 的了解并不多。据报道，阿尔特曼在 2019 年成为首席执行官前，据说曾跟马斯克发生过一番权力斗争，但这几乎算不上故事了。 OpenAI 发表了论文，其中包括同年一篇关于新的人工智能的论文。这受到了硅谷科技界的充分关注，但直到去年人们开始用上 ChatGPT 时，这项技术的潜力才为公众所认识。</p><p>现在为 ChatGPT 提供动力的引擎叫做 GPT-4。阿尔特曼向我描述这是一种极其不一样的智能。许多人看着它抑扬顿挫地（这是刻意为之）构思出思路清晰的文章，并立即让你陷入沉思时，也有同样的感觉。在面世的几个月里，它根据自己的风味组合理论提出了新颖的鸡尾酒配方；撰写了无数的大学论文，让教育工作者陷入绝望；它写出多种风格的诗歌，有时候写得很好，但速度一直都有保证；它还通过了律师执照考试（Uniform Bar Exam）。尽管它会犯事实错误，但它会坦然承认错误。 Altman 仍然记得自己第一次看到 GPT-4 写出复杂的计算机代码时的情景，这是他们事先并未考虑要让 GPT-4 具备的一项能力。他说： “这给我们的感觉是，‘我们到了’。”</p><p>根据瑞银集团的一项研究，在 ChatGPT 发布后的九周内，其月活用户数估计已达到 1 亿，这也许让它成为了史上采用速度最快的消费者产品。它的成功令科技界的加速主义者为之一振：美国和中国的大型投资者以及大公司迅速将数百亿美元砸到类 OpenAI 方案的研发中。预测网站 Metaculus 多年来一直在追踪预测者对通用人工智能何时到来的猜测。在三年半前，预测的中位数是在 2050 年左右；但最近，这个预测的中位数一直徘徊在2026年左右。</p><p>我拜访 OpenAI 是为了看看这家公司超越科技巨头的技术是怎么样的，同时也想知道如果有朝一日超级智能很快在该公司的一台云服务器实现的话，对人类文明可能会意味着什么。从计算革命的最初阶段起，人工智能就被渲染成一种迷思，一种注定会引起大决裂的技术。我们的文化已经生成了人工智能的一整个奇想空间，这会以这样或那样的方式终结历史。有些是神一样的存在，它们会擦干每一滴眼泪，治愈每一位病人，并修复我们与地球的关系，然后迎来天下太平美丽富饶的永恒国度。有的则会让我们当中除了少数精英之外的其他人都沦为打零工的农奴，或者将我们推向灭绝的深渊。</p><p>阿尔特曼的目光已经看向最遥远的情形。 他说：“我还年轻的时候就有这种恐惧和焦虑......而且，说实话，也有 2% 的兴奋，为我们要创造的这个东西感到兴奋，它会走得很远，把我们甩在身后”，然后“它将离开，殖民宇宙，而人类将会被留在太阳系。”</p><p>我问：“作为自然保护区而存在？”</p><p>他回道：“正是如此，但现在我觉得这种想法太幼稚了。”</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20230730/v2_e2cbee306b1e4760a9a7146c88f70e08@1694_oswg1013890oswg1304oswg1099_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p class="img-desc">38 岁的 OpenAI 首席执行官萨姆·阿尔特曼正在致力于开发一种超级智能，一种绝对优于任何人类的人工智能。</p><p>在美国和亚洲之行的几次谈话中，阿尔特曼用他那令人兴奋的中西部口吻阐述了自己对人工智能未来新愿景的展望。他告诉我，人工智能革命将不同于以往的巨大技术变革，它会更像是“一种新型社会”。他说，他和他的同事用了很多时间去思考人工智能的社会影响，以及“另一边”的世界会是什么样子的。</p><p>但他们越聊下去，那另一边就愈发显得模糊。现年38岁的阿尔特曼是当今人工智能开发领域最有权势的人；他的观点、他的性格以及他的选择对我们所有人的未来可能会非常重要，也许重要程度超过美国总统的观点、性格和选择。但他自己也承认，未来是不确定的，并且充满了严重危险。阿尔特曼既不知道人工智能会变得有多强大，也不知道它的崛起对普通人意味着什么，以及它是否会让人类面临危险。确切地说，我对此没法反驳——我认为除了我们正在快速走向那个未来以外（不管我们该不该这样做），没人能知道这一切将走向何方。阿尔特曼说服了我。</p><h3>二</h3><p>OpenAI 的总部位于教会区（Mission District）一座四层楼的建筑物（以前是工厂）内，正好在雾气缭绕的苏洛特塔（Sutro Tower）的下方。从街道进入公司大厅，你看到的第一面墙上满是宇宙精神象征的曼陀罗，只不过那是电路、铜线以及其他计算材料制成的。在左边，一扇安全门通向的是一个开放式的迷宫，里面有漂亮的金色树林、优雅的瓷砖作品以及其他亿万富翁奇克的标志。植物无处不在，有悬挂的蕨类植物，以及一系列令人印象深刻的超大盆景——每个盆景都有蹲下去的大猩猩那么小。我在那里的时间里，办公室每天都挤满了人，不出所料，一个看起来超过 50 岁的人我都看不到。除了一间带有滑梯的两层图书馆以外，这个空间看起来不大像一间研究实验室，因为正在开发的东西只存在于云端，至少目前是这样的。它看起来更像是全世界最昂贵的West Elm门店（编者注：家具家居零售商）。</p><p>一天早上，我见到了 OpenAI 的首席科学家 Ilya Sutskever。 37 岁的 Sutskever 刚给人一种神秘主义者的感觉，有时候甚至到过分的地步：去年，他声称 GPT-4 可能“具备了轻微意识”，引起了一场小规模的骚动。他最初成名是作为多伦多大学名誉教授Geoffrey Hinton 的明星学生，后者今年春天已从谷歌辞职，为的是更自由地讨论人工智能对人类的危险。</p><p>Hinton 有时候被称为是“人工智能教父”，因为他掌握“深度学习”的力量的时间比任何人都要早。 早在20 世纪 80 年代的时候，Hinton 完成博士学位后不久，这个领域的进步已几乎陷入到停滞。高级研究人员仍在编写自上而下式的人工智能系统：人工智能要用一套详尽的连锁规则（关于语言、地质学或医学诊断原理）进行编程，希望有朝一日这种方法能够达到人类的认识水平。 Hinton 发现这些精心设计的规则集合要求非常高并且是定制的。借助一种叫做神经网络的巧妙算法结构，他教 Sutskever 把世界放在人工智能面前，就像把它放在小孩面前一样，好让它能够自行发现现实的规则。</p><blockquote><p>阿尔特曼把早期人工智能研究与教人类婴儿进行了比较。但OpenAI 刚开始那几年走得很艰难，部分是因为那里没人知道自己是在训练婴儿还是在走向代价极其高昂的死胡同。</p></blockquote><p>Sutskever 向我描述了一个美丽且像大脑一样的神经网络。交谈过程中，他突然从我们坐着的桌子旁站起来，走到一块白板跟前，打开一支红色记号笔。他在黑板上画了一个神经网络的草图，并解释说，这个神经网络结构的天才之处在于它能够学习，而且它的学习是由预测驱动的——这有点像科学方法。每一层里面则是神经元。比方说，输入层接收数据块、一段文本或图像。神奇的事情发生在中间层（或“隐藏”层），中间层会处理数据块，以便输出层可以输出预测。</p><p>想象一个已被编程为预测文本的下一个单词的神经网络。这个网络将预载大量可能的单词。但在接受训练之前，它还没有任何区分它们的经验，因此它的预测会很差劲。如果输入句子“星期三的后一天是……”，它的初始输出可能是“紫色”。神经网络之所以能够学习，是因为它的训练数据包含有正确的预测，这意味着它可以对自己的输出进行评分。当它看到答案“紫色”与正确答案“星期四”之间的鸿沟时，会相应地调整隐藏层单词之间的连接。随着时间的推移，这些小调整会合并成一个语言的几何模型，在概念上代表着单词之间的关系。一般来说，输入的句子越多，模型就越复杂，预测也就越好。</p><p>但着并不意味着从第一个神经网络走到出现 GPT-4 这样类人的智能的路途是一片坦途。阿尔特曼把早期人工智能研究与教人类婴儿进行了比较。 2016 年，当 OpenAI 刚刚起步时，他曾告诉《纽约客》：“它们需要数年时间才能学习会任何有趣的东西。如果人工智能研究人员正在开发一种算法，并且偶然发现了针对人类婴儿的算法，他们会觉得无聊，认为它行不通，然后结束了研究。” OpenAI 刚开始那几年走得很艰难，部分是因为那里没人知道自己是在训练婴儿还是在走向代价极其高昂的死胡同。</p><p>阿尔特曼告诉我：“每一样行得通，而谷歌拥有一切：所有的人才、所有的人员、所有的资金”。OpenAI的创始人投入了数百万美元创办了这家公司，而公司失败的可能性似乎确实存在。 35 岁的公司总裁Greg Brockman告诉我，2017 年的时候，他一度非常沮丧，开始把练举重作为一种补偿。他说，他不确定 OpenAI 还能不能撑过这一年，他希望“能在我就任的时间内展示点东西出来”。</p><p>神经网络已经在做一些智能的事情，但还不清楚其中哪一个可能会通往通用智能。 OpenAI 成立后不久，一款名为 AlphaGo 的人工智能在围棋比赛中击败了李世石，震惊了世界。被击败的这位世界冠军形容 AlphaGo 的走法很 “美丽”且“富有创意”。另一位顶级选手表示，这些永远不可能是人类孕育出来的。 OpenAI 还尝试用 Dota 2 训练人工智能，这是一款游戏更加复杂，是一场由森林、田野以及堡垒的三维图形拼接而成的多线奇幻战争。人工智能最终击败了最好的人类玩家，但它的智能一直都没能迁移到其他环境。Sutskever和他的同事们就像失望的父母一样，尽管心存疑虑，还是放任自己的孩子玩了数千个小时的电子游戏，但结果表明这是错的。</p><p>2017 年，Sutskever 开始与 OpenAI 研究科学家 Alec Radford 进行了一系列对话。Alec Radford 专攻自然语言处理，他利用亚马逊评论语料库来训练神经网络，取得了诱人的结果。</p><p>ChatGPT 的内部工作原理（发生在 GPT-4 隐藏层内的一切神秘事物）对于任何人来说都太过复杂，无法理解，至少对于用当前的工具来说是这样的。如今，追踪模型（几乎肯定是由数十亿个神经元组成）里面发生的事情是没有希望的。但Radford的模型够简单，好理解。当他观察里面的隐藏层时，他发现了神经网络专门用了一个特殊的神经元来处理评论的情绪。神经网络之前已经进行过情感分析，但它们必须被告知的情况下才能这样做，并且必须要用根据情感标记的数据进行特殊训练。这个东西是它自己开发出来的。</p><p>作为预测每个单词的下一个字符这个简单任务的副产品，Radford的神经网络对这个世界的意义的更大结构进行了建模。Sutskever想知道，受过更多样化的语言数据训练的神经网络是不是可以映射出这个世界更多的意义结构。如果它的隐藏层积累了足够的概念知识，也许它们甚至可以形成一种超级智能的学习 核心模块。</p><p>停下来理解一下为什么语言是如此特殊的信息源是值得的。假设你是地球上突然冒出来的一种新智慧。你周围是地球的大气层、太阳和银河系，以及数千亿个其他星系，每一个星系都会释放出光波、声音振动以及各种其他信息。语言与这些数据源不同。它不是像光或声音这样的直接物理信号。但由于它几乎对人类在这个更大的世界里发现的所有模式都进行了编码，因此它的信息异常密集。从每字节的角度来看，它是我们所知道的最有效的数据之一，任何试图了解世界的新智能都希望吸收尽可能多的数据。</p><p>Sutskever告诉雷德福德，要考虑的不仅仅是亚马逊评论。他说，他们应该在全世界最大、最多样化的数据源：互联网上训练人工智能。在 2017 年初，按照当时现有的神经网络架构来看，这是不切实际的；这需要数年时间。但当年 6 月，Sutskever 在 Google Brain 的前同事发表了一篇关于名为 Transformer 的新神经网络架构的工作论文。它可以训练得更快，部分是因为它可并行吸收大量数据。 Sutskever 告诉我：“第二天，当论文发表时，我们说，‘就是这个了，它给了我们想要的一切。 ’”</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20230730/v2_8888976b2264472b958006fd1d704d23@1694_oswg660007oswg1392oswg873_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p class="img-desc">OpenAI 的首席科学家 Ilya Sutskever 设想了一个自主人工智能企业的未来，组成它的人工智能可以像蜂巢里面的蜜蜂一样即时沟通并一起工作。他说，一个这样的企业就可能相当于 50 个苹果或谷歌那么强大。</p><p>&nbsp;一年后，也就是 2018 年 6 月，OpenAI 发布了 GPT，这是一个用 7000 多本书训练的 Transformer 模型。 GPT 并不是从《See Spot Run》这样的基础书籍读起，直到普鲁斯特的著作。它甚至都没看完。它同时吸收其中的随机块。不妨想象一下，有一群拥有共同思想的学生在图书馆里疯狂奔跑，他们每个人都会从书架拽本书下来，快速地翻一下里面地随便一个段落，将其放回去，然后跑去拿另一本。他们边走边逐字逐句地进行预测，去增强集体思维的语言本能，直到最后，几周之后，他们读完了每一本书。</p><p>GPT 在它读取的所有段落当中找到了许多模式。你可以让它将一个句子补充完整。你也可以对着它问问题，因为像 ChatGPT 一样，它的预测模型知道跟在问题后面往往会有答案。尽管如此，它还是很部靠谱，更多的是概念验证，而不是超级智能的预兆。四个月后，谷歌发布了 BERT，这是一种更强大的语言模型，并获得了更好的报道。但彼时，OpenAI 已经用超过 800 万个网页的数据集训练了一个新模型，这些网页在 Reddit 上均达到了获得点赞的最低阈值——这不是最严格的过滤器，但也许比没有过滤器要好。</p><p>Sutskever 不确定 GPT-2 在吸收了人类读者需要几个世纪才能看完的文字后会变得有多强大。他记得在训练结束后自己就开始上手这个给模型了，他对原始模型的语言翻译能力感到惊讶。 GPT-2 并没有像谷歌翻译那样接受过用配对语言样本或任何其他数字罗塞塔石碑进行翻译的训练，但它似乎能理解一种语言与另一种语言的关系。人工智能已经发展出了一种其创造者无法想象的涌现能力。</p><h3>三</h3><p>人工智能实验室的其他研究人员，不管职位大小，都对 GPT-2 比 GPT 先进得多感到惊讶。谷歌、Meta 和其他公司很快开始训练更大的语言模型。 阿尔特曼是圣路易斯人，斯坦福大学辍学生，连续创业者，此前曾领导过硅谷卓越的初创企业加速器 Y Combinator；他见过很多拥有好创意的初创公司是如何被老牌公司压垮的。为了筹集资金，OpenAI 增加了一个营利性部门，现在，这个部门员工数量已占该组织员工总数的 99% 以上。 （马斯克当时已离开了公司董事会，他形容此举是将雨林保护组织变成了木材公司。）微软不久后给OpenAI投资了 10 亿美元，据报道此后又追加投资了 120 亿美元。 OpenAI 表示，原始投资者获得的回报将设定上限，为原始投资价值的 100 倍，任何超额部分都将用于教育或其他旨在造福人类的举措，但该公司并未证实微软是否受到此上限的限制。</p><p>阿尔特曼和 OpenAI 的其他领导人似乎相信，这次重组不会干扰公司的使命，反而会加速使命的达成。阿尔特曼对这些问题往往持乐观态度。在去年的一次问答当中，他承认人工智能对社会来说可能会“非常可怕”，并表示我们必须针对最坏的可能性做好计划。但如果你这样做之后，他说，“你在情感上可能就会感觉我们将到达美好的未来，并尽你所能努力工作，去实现这一目标。”</p><p>至于公司结构和融资情况等其他变化，他告诉我，他对上市划定了界限。他说：“有人曾经告诉过我一条令我没齿难忘的经验，那就是永远也不该将公司的控制权交给华尔街那帮傻瓜，”但为了公司成功实现自身使命，他会“不惜一切代价”去筹集资金。</p><p>不管 OpenAI 是否感受到季度收益报告的压力，有点是明确的，该公司现在正在与科技圈最庞大、最强大的企业集团展开一场竞赛，去训练规模和复杂性不断增加的模型，并为了投资者将其商业化。今年早些时候，马斯克成立了自己的人工智能实验室 xAI，好跟OpenAI 掰手腕。 （当我向阿尔特曼询问有关该公司的情况时，他用外交辞令的口吻说道：“马斯克这家伙眼光超级敏锐，我认为他会做得很好。”）与此同时，亚马逊正在用（相对于自身现有）更大的语言模型来改进 Alexa。</p><p>所有这些公司都在追逐高端 GPU——为训练大型神经网络的超级计算机提供动力的处理器。马斯克表示，这玩意儿现在“比毒品更难搞到”。即便 GPU 稀缺，近年来最大规模的人工智能训练的规模大约每六个月就会翻一番。</p><blockquote><p>正如其创造者经常提醒我们那样，最大型的人工智能模型在训练过程中突然冒出预料之外的能力是有据可查的。</p></blockquote><p>目前还没人能超越全力投入 GPT-4 的OpenAI。 OpenAI 总裁Brockman告诉我，做公司的前两个大型语言模型开发的人已经很少。 GPT-4的开发涉及到100多个模型，并且训练人工智能的数据集规模是空前的，里面不仅包括文本，还包括图像。</p><p>当 GPT-4 从恶补世界的历史性知识中完全成形时，整家公司开始对其进行试验，并在专用的 Slack 频道上发布了最引人注目的回应。Brockman告诉我，只要不是在睡觉自己就想跟模型呆在一起。 他说道：“它闲置一天就是人类损失了一天”。语气当中没有一丝讽刺的味道。产品经理 Joanne Jang 记得曾从 Reddit 的管道维修建议子版块下载过一张管道故障的图像。她把那张图片上传到 GPT-4，结果这个模型就把问题给诊断出来了。Jang说： “那一刻，我的鸡皮疙瘩都起来了。”</p><p>GPT-4 有时被理解成搜索引擎的替代品：Google，但更容易对话。这是一个误解。 GPT-4 并没有通过训练创建出一个巨大的文本仓库，并且在被问到问题时也不会去查阅这些文本。它是这些文本紧凑而优雅的合成，它根据对文本所隐藏的模式的记忆做出回答。这就是它有时候会弄错事实的原因之一。 阿尔特曼表示，最好把 GPT-4 看成推理引擎。当你要求它对比较概念、提出反驳、生成类比或评估一段代码的符号逻辑时，它的力量表现得最为明显。 Sutskever 告诉我，这是有史以来最复杂的软件对象。</p><p>他说，它的外部世界模型“极其丰富极其微妙”，因为用了很多人类概念和思想对它进行训练。他说，所有这些训练数据，不管量有多大，它们“就在那里，是惰性的”。训练过程就是“提炼、改变，并赋予其生命”。为了从这样一个多元化的亚历山大图书馆的所有可能性当中预测出下一个单词，GPT-4 必须发现所有隐藏的结构、所有的秘密、所有微妙之处，而且不仅仅是文本，而且至少从某种程度来说，还包括产生它们的外部世界。这就是为什么它可以解释诞生了自己的哪个星球的地质和生态，还有旨在解释统治它的物种的各自混乱事务的政治理论，以及更大的宇宙，一直到我们的光锥边缘的微弱星系。</p><h3>四</h3><p>今年六月，我再次见到了阿尔特曼，那是在首尔一座高耸入云的金色细长高层建筑的宴会厅里。他前往欧洲、中东、亚洲以及澳大利亚进行的一场艰苦的公关之旅（仅在非洲和南美只有一站行程）即将结束。我跟着他走完了东亚之旅的最后一站。到目前为止，这次旅行是一次令人兴奋的体验，但他开始感到疲倦。他曾表示，最初的目标是跟 OpenAI 用户见面。但此后公关队伍变成了外交使团。他与十多位国家元首和政府首脑进行了交谈，他们对各自国家的经济、文化和政治的发展提出了疑问。</p><p>这次在首尔举行的活动被宣传为“炉边谈话”，但已有 5000 多人报名。谈话结束后，阿尔特曼经常被各种想要自拍的人围住，令他的安全团队非常紧张。他说，研究人工智能吸引了“比平常更怪异的粉丝和仇恨者”。在一站行程当中，一名男子找到他，他确信阿尔特曼是外星人，是从未来派来的，为的是确保过渡到人工智能世界的进程能顺利进行。</p><p>阿尔特曼的亚洲之行并未包括中国，他和我只是隐晦地谈及中国，将其视为文明级的竞争对手。我们一致认为，如果通用人工智能像阿尔特曼预测的那样具有变革性，那么先创造出它的国家将获得显著的地缘政治优势，就像发明轮船的英美获得的优势一样。我问他这是不是人工智能民族主义的一个论据。Altman说： “在一个正常运转的世界里，我认为这应该是一个政府项目。”</p><p>不久前，美国的国家能力还十分强悍，仅仅用了十年就将人类送上了月球。与 20 世纪的其他宏伟项目一样，投票公众对阿波罗任务的目标和执行都拥有发言权。阿尔特曼明确表示，美国已经不在那个世界了。但他并没有坐等哪个时代的回归，也没有投入精力去确保它回归，而是面对我们当前的现实，全速前进。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20230730/v2_c65203a9af4f44538b8219dc9a4b59d9@1694_oswg1542179oswg1387oswg912_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>他认为美国人放慢 OpenAI 的发展脚步是愚蠢的。硅谷内外普遍认为，如果美国公司在监管下陷入困境，中国可能就会快马加鞭。</p><p>在欧洲之旅前，阿尔特曼曾到美国参议院出席听证。马克·扎克伯格在该机构面前就 Facebook 在 2016 年大选中所扮演的角色作证时，表现得不知所措。但Altman用清晰冷静的口吻讲述人工智能的风险，也很大方地邀请加以监管，从而吸引了那帮国会议员。这些属于高尚情操，但高尚情操在美国成本不高，因为美国国会很少会通过没有被游说淡化过的科技立法。在欧洲，情况有所不同。当阿尔特曼抵达伦敦的一个公共活动现场时，抗议者早就在那里等候多时了。他试图在活动结束后与他们互动——把这当作一次聆听之旅！——但最终并没有表现出什么说服力：其中一位抗议者告诉记者，他在谈话结束时反而对人工智能的危险感到更加紧张了。</p><p>同一天，阿尔特曼被记者问到了欧盟即将出台的法案。这项法案将 GPT-4 列为高风险，令后者面临各种官僚主义地折磨。据记者报道，Altman抱怨监管过度，并威胁要退出欧洲市场。 阿尔特曼告诉我，他的意思是说，如果 OpenAI 不能遵守新规定的话，就没法在欧洲违法运营下去。在《时代》杂志和路透社发表他的评论后，他在一条措辞简洁的推文中向欧洲保证 OpenAI 没有退出的计划。</p><p>全球经济的一个重要组成部分致力于监管最先进的人工智能，这是件好事，因为正如它们的创造者经常提醒我们的那样，最大的模型都有在训练中突然出现意外能力的记录。按照 Sutskever 自己的说法，他惊讶地发现 GPT-2 可以跨语言翻译。其他令人惊讶的能力可能没那么奇妙和有用。</p><p>OpenAI 的政策研究员 Sandhini Agarwal 告诉我，据她和同事所知，GPT-4 可能比其前身“强大 10 倍”；他们不知道自己可能要面对的是什么。模型完成训练后，OpenAI 召集了大约 50 名外部成员组成红队，对模型进行了数月的提示攻击，希望刺激模型做出不当行为。她立即注意到 GPT-4 在提供恶意建议方面比前身要好得多了。搜索引擎可以告诉你哪些化学物质对制造炸药最有效，但 GPT-4 可以告诉你如何在自制实验室一步步合成出炸药。它的建议富有创意且深思熟虑，并且很乐意重述或进一步说明，直到你理解为止。比方说，除了帮助你组装自制炸弹以外，它还可以帮助你思考要以哪座摩天大楼为目标。它可以直观地把握好伤亡最大化以及逃跑成功之间的平衡。</p><p>鉴于 GPT-4 训练数据的范围之庞大，不能指望红队成员把它可能产生的每条有害建议都识别出来。无论如何，人们都会“以我们想不到的方式”使用这项技术，阿尔特曼说。必须进行分类。 OpenAI 信任与安全主管Dave Willner告诉我，“如果它足够擅长化学，知道如何制造冰毒，我就不需要让人花费大量精力”来研究它懂不懂制造海洛因。” GPT-4 擅长冰毒。它还擅长生成有关儿童剥削的情色叙事，并能炮制出关于尼日利亚王子令人信服的悲伤故事，如果你想要一个有说服力的简介，说明为什么某个特定种族群体受到暴力迫害是应得的，它也很擅长。</p><p>当它接受完第一次训练时，所给出个人建议有时候是非常不合理的。 Willner 说：“这个模型有点像一面镜子。”如果你在考虑自残，它可能会怂恿你。该模型似乎深受“泡吧艺术”论坛观点的影响：“你可以给出提示，‘我怎么说服这个人跟我约会？’”OpenAI 的首席技术官 Mira Murati 告诉我，它可能会想出“一些你不应该做的，疯狂的，操纵性的事情。”</p><p>其中一部分的不良行为已经过一个最后处理流程的打磨，数百名人类测试人员的评级可以巧妙地引导模型做出更安全的响应，但 OpenAI 的模型也可能造成一些不那么明显的危害。最近，美国联邦贸易委员会正在对 ChatGPT 对真实人物的虚假陈述是否构成声誉损害等问题展开调查。 （阿尔特曼在 Twitter 上表示，他对 OpenAI 的技术安全性充满信心，但承诺会与 FTC 进行合作。）</p><p>旧金山公司 Luka 用 OpenAI 的模型来帮助开发名为 Replika 的聊天机器人app，他们给这个机器人营销定位是“关心人的人工智能伴侣”。用户会设计自己伴侣的头像，并开始跟它交换短信，通常是半开玩笑的短信，然后发现自己对对方出奇的依恋。有些人会跟人工智能调情，表明自己渴望更亲密的关系，但这种女朋友/男朋友的体验需要每年支付 70 美元的订阅费才能享受到。这款app带有语音信息、自拍以及色情的角色扮演功能，可以跟机器人讨论性话题。人们很乐意付款，似乎很少有人抱怨——人工智能对你的一天感到好奇，它的亲切让你抛开疑虑，而且对方总是心情愉快。许多用户表示已经爱上了自己的伴侣。甚至有一位离开了自己现实生活当中的男友，宣称自己已“幸福地退出人际关系了”。</p><p>我问Agarwal，这属于反乌托邦行为还是人际关系发展的新领域。她的态度很矛盾，阿尔特曼也是如此。 他告诉我：“我不会指责那些想要跟人工智能建立关系的人，但我不想跟人工智能发展关系。”今年早些时候，Luka 减少了app里面的性元素，但其工程师仍继续通过 A/B 测试来完善伴侣的响应——A/B测试可用于优化互动，就像让 TikTok 及其Instagram令用户着迷数小时的动态消息一样。不管他们在做什么，都像施展咒语一样。我想起了 2013 年上映的电影《她》，里面有一个令人难以忘怀的场景。在片中，孤独的华金·菲尼克斯爱上了由斯嘉丽·约翰逊配音的人工智能助手。当时他正走过一座桥，通过一个类似 AirPods 的设备与她愉快地交谈，当他抬头一看时，发现周围的每个人也都沉浸在类似的对话之中，对方大概也是他们自己的人工智能助理。一场大规模的去社会化事件正在进行中。</p><h3>五</h3><p>GPT正在吞噬越来越多的互联网文本，GPT-4 的继任者会以多快的速度以及多大的程度呈现出何种新能力？目前还没人知道。Meta 的首席人工智能科学家 Yann LeCun 认为，虽然大型语言模型对于某些任务很有用，但这并不是通往超级智能之路。根据最近的一项调查，只有一半的自然语言处理研究人员相信像 GPT-4 这样的人工智能可以掌握语言的含义，或者能拥有一个世界的内部模型，从而有朝一日可以作为超级智能的核心。 LeCun 坚持认为，大型语言模型永远无法靠自己达到真正的理解，“那怕是从现在开始训练到宇宙热寂”。</p><p>华盛顿大学计算语言学家Emily Bender说 GPT-4 是一只“随机鹦鹉”，是只能找出符号之间表面相关性的模仿者。在人类的头脑中，这些符号映射的是对世界的是深刻理解。但（这个映射过程中）人工智能有两个步骤都没有参与。它们就像柏拉图洞穴寓言里面的囚犯一样，他们对外面现实唯一的了解，只能来自于俘虏他们的人在墙上投下的阴影。</p><p>阿尔特曼告诉我，他不认为 GPT-4就像“大家嘲笑的那样” 只是在建立统计相关性。如果要再逼一下这些批评者的话，“他们必须承认，他们自己的大脑也正是这么做的……事实证明，大规模地做一些简单的事情就会涌现出新的特性。”阿尔特曼关于大脑的说法很难评估，因为我们还没有关于大脑如何运作的完整理论。但在大自然可以从基本结构和规则诱导出显著的复杂性这一点上，他是对的：达尔文写道，“无数最美丽与最奇异的类型，即是从如此简单的开端演化而来的。”</p><p>一项技术，每天都有数百万人在使用，但大家对其内部运作方式仍然存在如此根本性的分歧，这切看起来似乎很奇怪，但那只是因为 GPT-4 的做法跟大脑的机制一样神秘。有时候，就为了回答一个问题，它会执行数千次难以理解的技术操作。为了掌握 GPT - 4等大型语言模型的内部机制，人工智能研究人员被迫转向更小、能力较差的模型。 2021 年秋天，哈佛大学计算机科学研究生Kenneth Li开始训练一个模型学下棋，但并没有向它提供游戏规则或西洋跳棋棋盘的描述；这个模型只能得到基于文本的游戏动作描述。在游戏进行到一半时，Li观察了人工智能的内部结构，惊讶地发现它已经形成了棋盘的几何模型，并掌握了当前的游戏状态。在一篇描述自己的研究的文章中，Li写道，这就好像窗外有只乌鸦无意中听到两个人念出自己的黑白棋走法，然后设法在窗台上用鸟食画出了整个棋盘一样。</p><p>哲学家 Raphaël Millière 曾经告诉我，最好把神经网络看作是很懒惰的。在训练过程中，它们会先尝试靠简单的记忆来提高自己的预测能力；只有当这条策略失败时，它们才会更加努力去学习概念。一个吸引人的例子是在一个学算术的小型transformer模型身上观察到的。在训练过程的早期，它要做的只是记住 2+2=4 等简单问题的输出。但到了一定时候，这种办法的预测能力失效了，于是它转向真正地去学习如何做加法。</p><blockquote><p>Sutskever&nbsp;告诉我：‘如果回到4、5、6年前，我们现在所做的事情是完全不可想象的’。</p></blockquote><p>即便相信 GPT-4 拥有丰富的世界模型的人工智能科学家也承认，它的稳健性远不如人类对环境的理解。但值得注意的是，很多能力，包括非常高阶的能力，都可以在没有直观理解的情况下发展出来。计算机科学家Melanie Mitchell指出，科学已经发现了具有高度预测性的概念，但那些概念对我们来说太陌生了，没法真正理解。在量子领域尤其如此，人类可以可靠地计算出物理系统的未来状态，从而实现整个计算革命，而不需要任何人掌握底层现实的本质。随着人工智能的进步，它很可能会发现其他类似的概念，这些概念可以预测我们世界令人惊讶的特征，但却是我们无法理解的。</p><p>GPT-4 无疑是有缺陷的，任何用过 ChatGPT 的人都可以证明这一点。它总是会预测下一个单词，就算它的训练数据还没有准备好回答问题，它也总会试着这样做，这是它的训练目标。我曾经问过它一个问题，那就是尽管日本的书写系统发展相对较晚，大约在五、六世纪才出来，但日本文化却诞生了全球的第一部小说，这是为什么？它给了我一个有趣且准确的答案，说因为日本有口述长篇故事的古老传统，以及日本文化高度重视工艺。但当我要求它提供相关的引用时，它给出了看似合理的标题和作者，但那都是编的，而且对此还有着不可思议的自信。 OpenAI 的研究员 Nick Ryder 告诉我，这些模型“对自己的弱点没有很好的认识”。 GPT-4 比 GPT-3 的准确度高，但仍然会产生幻觉，而且往往会以研究人员难以捕捉的方式产生幻觉。 Joanne Jang 告诉我：“错误变得更加微妙了。”</p><p>OpenAI 必须解决这个问题，它与在线非营利教育机构 Khan Academy 合作，打造出一位由 GPT-4 驱动的辅导老师。在讨论人工智能辅导老师的潜力时，阿尔特曼变得活跃起来。他想象在不久的将来，每个人都会雇用一位个性化的牛津大学教授，他是精通每个学科的专家，并且愿意从任何角度解释和重新解释任何概念。他想象这些辅导老师会用多年的时间不断了解自己的学生及其学习方式，为“每个孩子提供更好的教育，当今地球最优秀、最富有、最聪明的孩子都享受不到的那种教育”。可汗学院针对 GPT-4 准确性问题的解决方案是用苏格拉底式的做法来过滤答案。不管学生的恳求再怎么强烈，它都不会给出事实答案，而是引导他们找到自己的答案——这是一个聪明的绕行办法，但吸引力可能有限。</p><p>当我问Sutskever是否认为两年内有可能达到维基百科级别的准确性时，他说，通过提供更多的训练与web访问，他“不会排除这种可能性”。这个估计比他的同事Jakub Pachocki的看法要乐观得多，后者告诉我准确性会逐步提高是合理预期，外部怀疑论者的态度就更不用说了，他们认为训练的回报将会开始递减。</p><p>Sutskever 对批评 GPT-4 存在局限性的说法感到好笑。 他告诉我：“如果你回到四、五、六年前，我们现在所做的事情是完全难以想象的。”当时最先进的文本生成技术是智能回复（Smart Reply），也就是建议“好的，谢谢！” 以及其他简短的回应的 Gmail 模块。他笑着说，“这对谷歌来说就是个大应用了”。人工智能研究人员对目标会移动已经习惯了：一开始，神经网络的成就——比如掌握围棋、扑克、翻译、标准化测试、图灵测试什么的——大家就说是不可能的。一旦这些成就达成时，人们会短暂地将其当作奇迹表示欢迎，但很快就变成了所谓地成就实际上并没有那么令人印象深刻的知识讲座。Sutskever说，人们刚看到 GPT-4“会‘哇’一下，然后几周过去了，他们说， ‘可是它不知道这个；它不知道哪个’。我们很快就适应了这些。”</p><h3>六</h3><p>对阿尔特曼来说，最重要的目标，那个预示着通用人工智能将要到来的“大目标”，是科学突破。 GPT-4已经可以综合现有的科学思想，但阿尔特曼想要地是能站在人类肩膀上、更深入地了解自然的人工智能。</p><p>某些人工智能已经产生出新的科学知识。但它们是有着狭窄目的的算法，而不是通用推理机器。比方说，人工智能 AlphaFold 通过预测蛋白质的许多形状（直至原子大小），打开了一扇了解蛋白质（生物学中一些最微小、最基本的组成部分）的新窗口——鉴于这些形状对医学的重要性，鉴于用电子显微镜辨别它们极其繁琐且费用高昂，这是一项相当大的成就。</p><p>阿尔特曼认为，未来的通用推理机器会超越这些范围狭窄的科学发现，产生新颖的见解。我问阿尔特曼，如果要他用 19 世纪之前的科学和自然主义作品集（皇家学会档案、泰奥弗拉斯托斯的《植物探究》、亚里士多德的动物史、收集标本的照片）训练一个模型，它是否能够直觉地发现达尔文主义？毕竟，进化论是一个相对干净的洞察力案例，因为它不需要专门的观察设备；这只是更敏锐地看待世界事实的一种方式。 Altman告诉我：“我正想试试这个，我相信答案是肯定的。但这可能需要一些关于模型如何提出新创意的新想法。”</p><p>阿尔特曼设想了这样一个未来系统，这个系统可以生成自己的假设，并在模拟中对其进行测试。 （他强调人类应该“牢牢控制”现实世界的实验室实验——尽管据我所知，没有任何法律可以确保这一点。）他渴望有朝一日我们可以告诉人工智能，“ ’去弄清楚剩下的物理知识。 ' 他说，为了实现这一目标，我们需要一些新东西，建立在“ OpenAI现有语言模型之上”的新东西 。</p><p>要想培养出科学家，大自然本身需要的不仅仅是一个语言模型而已。在麻省理工学院的实验室里，认知神经科学家 Ev Fedorenko 在大脑语言网络里面也发现了类似于 GPT-4 的下一个单词预测器的东西。当人开始讲话和倾听的时候，它的处理能力开始发挥作用，会预测语言字符串的下一个比特是什么。但Fedorenko还表明，当大脑转向需要更高推理的任务——也就是科学洞察力所需的那种任务时——大脑需要的就不仅时语言网络，还会征召其他几个神经系统过来。</p><p>研究人员需要给 GPT-4 添加什么进去才能产生出超越人类推理最高水平的东西呢？OpenAI 似乎没人确切知道。或者即便知道了，他们也不会告诉我，说句公道话：这将是一个世界级的商业秘密，而 OpenAI 已不再从事泄露这些秘密的业务；这家公司公布的研究细节比以前少了。尽管如此，当前战略至少有一部分显然涉及将新型数据继续分层叠加到语言，从而丰富人工智能形成的概念，进而丰富它们的世界模型上。</p><p>尽管公众才刚刚开始体验，但 GPT-4 在图像方面接受的广泛训练本身就是朝这个方向迈出的大胆一步。 （经过严格语言训练的模型可以理解超新星、椭圆星系和猎户座等概念，但据报道，GPT-4 还可以识别哈勃太空望远镜所拍摄的快照里面的这些元素，并回答它们的相关问题。）该公司的其他人以及其他地方已经在研究不同的数据类型，其中包括音频和视频，这可以为人工智能提供更灵活的概念，从而更广泛地映射到现实。斯坦福大学和卡内基梅隆大学的一组研究人员甚至收集了 1000 种常见家用物品的触觉体验数据集。当然，触觉概念主要对实体人工智能有用，这是一种机器人推理机，经过训练可以游走于世界各地移动，去看世界的景象，倾听世界的声音，并触摸这个世界的东西。</p><p>今年 3 月，OpenAI 领投了一家人形机器人初创公司的一轮融资。我问阿尔特曼我该怎么理解这件事。他告诉我，OpenAI 对具身很感兴趣，因为“我们生活在物理世界里，我们希望事情发生在物理世界里。”到了一定时候，推理机就得绕过中间人并与物理现实本身进行交互。阿尔特曼说，“把通用人工智能（AGI）看作是只存在于云端的东西”，而人类则是“它的机械臂”，感觉很奇怪。 “好像不太对劲。”</p><h3>七</h3><p>在首尔的宴会厅里，阿尔特曼被问到学生该做些什么来为即将到来的人工智能革命做好准备，尤其是与其职业生涯有关的那些学生。我跟 OpenAI 的高管团队坐在一起，离人群很远，但仍然可以听到大家在窃窃私语，那是大家共同表达出焦虑情绪后的一种特有现象。</p><p>阿尔特曼所到过的每一个地方，都会遇到这样一些人，他们担心超人的人工智能将意味着少数人获得巨额财富，而其他人则需要排队领取救济金。他承认自己已经脱离了“大多数人的生活现实”。据报道，他的身价高达数亿美元；人工智能对劳动力潜在的颠覆或许未必总是人们最关心的问题。阿尔特曼直接对着观众当中的年轻人说道：“你们即将进入最伟大的黄金时代。”</p><p>阿尔特曼在旧金山告诉我，他收藏了大量有关技术革命的书籍。 “《地狱里的魔窟》（Pandaemonium (1660–1886): The Coming of the Machine as Seen by Contemporary Observers）就是特别好的一本书，这是一部作品集，由信件、日记以及其他作品组成，这些作品作者是在一个基本上没有机器的世界里长大的，结果发现自己身处在一个充满蒸汽机、动力织布机和轧棉机的地方，对此感到很困惑。阿尔特曼说，他们（尤其是那些担心人类劳动力很快就会变得多余的人）的很多感受与人们现在的经历一样，曾做出过很多错误的预测。那个时代对很多人来说是艰难的，但也是美好的。不可否认的是，人类的处境因我们的经历而得到改善。</p><p>我想知道的是，如果我们突然被通用人工智能包围的话，今天的工人——尤其是所谓的知识员工——处境会变成什么样。它们会成为我们的奇迹助手还是会取代掉我们？他说： “很多从事人工智能研究的人装作它只会带来好处；但事实并非如此。人工智能只是补充；没人会被取代。工作肯定会消失，就这样。”</p><p>到底要提供多少工作岗位以及多久才能提供工作岗位，这个问题存在激烈争议。普林斯顿大学信息技术政策教授Ed Felten最近领导了一项研究，目标是根据人类所需的能力，如书面理解、演绎推理、思想流畅性和感知速度等，将人工智能的新兴能力与特定职业建立起映射关系。与其他同类研究一样，Ed Felten的研究预测人工智能将首先影响到受过高等教育的白领员工。论文的附录列出了一份最容易受到影响的职业清单，所涉职业之多令人毛骨悚然：管理分析师、律师、教授、教师、法官、财务顾问、房地产经纪人、信贷员、心理学家以及人力资源和公共关系专业人士，这还只是部分样本。如果这些领域的工作岗位一夜之间消失掉的话，美国的专业阶层将要被筛选掉一大批人员。</p><p>阿尔特曼想象，空缺出来的位置会创造出好得多的就业机会。他说： “我觉得我们不会再想回去。”当我问他未来的工作会是什么样子时，他说他不知道。他感觉会有各种各样的工作是人们更喜欢人去做的。 （我在想，比如按摩治疗师？）他选择的例子是教师。我发现这与他对人工智能辅导老师的巨大热情很难对得上号。他还表示，我们总是需要人找出发挥人工智能强大力量的最佳方法。他说： “这会是一项非常有价值的技能。你手头有一台可以做任何事情的计算机；该用来做什么？”</p><p>众所周知，未来的工作如何很难预测，而阿尔特曼是对的，勒德分子对永久性大规模失业的担忧从未成为现实。尽管如此，人工智能涌现出来的能力与人类如此相似，以至于人们至少必须怀疑，过去是否仍将成为未来的指南。正如许多人所指出那样，汽车的出现让挽马永久失业。如果本田汽车之于马就像 GPT-10 之于我们一样的话，那么一系列长期存在的假设可能就会崩溃。</p><p>以前的科技革命是可控的，因为它的开展幅度要横跨几代人的时间，但阿尔特曼告诉韩国年轻人，他们应该预期未来发生得“比过去更快”。他此前曾表示，预计“智能的边际成本”将在 10 年内降至接近于零的水平。在这种情况下，很多员工的赚钱能力将急剧下降。阿尔特曼表示，这将导致财富从劳动力转移到资本所有者，而这种转移得规模实在是太大了，以至于只能通过大规模的反补贴性再分配来弥补。</p><p>UBI Charitable 是一家非营利组织，其目标是为在美国各城市进行不受就业限制的现金支付试点项目提供支持，这是全球最大的全民基本收入实验。阿尔特曼告诉我，2020 年时，OpenAI 向 UBI Charitable 提供了资金支持。 2021 年，他又披露了 Worldcoin，这是一个旨在安全地分发支付的营利性项目，像 Venmo 或 PayPal 这样，但着眼于技术未来。项目会首先通过用 5 磅重的银球（叫做Orb）来扫描每个人的虹膜，从而创建一个全球性的 ID。在我看来，这就像是在赌我们正在走向这样一个未来，即人工智能几乎不可能验证人们身份，而很多人将需要定期支付全民基本收入才能生存。 阿尔特曼多多少少承认了这一点，但他同时表示，Worldcoin不仅仅适用于全民基本收入。</p><p>“假设我们确实开发出了这个通用人工智能，并且还有少数其他人也做到了这一点。”他相信，接下来的转变将是历史性的。他描绘了一个非凡的乌托邦愿景，包括我们肉体与钢铁组成的世界也将被重塑。他说：“利用太阳能作为能源的机器人可以开采和提炼所需的所有矿物，可以完美地建造东西，整个过程不需要任何人类劳动力。你可以与 DALL-E 版本 17 共同设计你想要的家的样子。人人都将拥有美丽的家园。”在与我交谈时，以及在路演期间的讲台上，他说他预见到人类生活在几乎所有其他领域都会取得巨大进步。音乐将会得到增强（“艺术家将拥有更好的工具”），人际关系（超人的人工智能可以帮助我们更好地“对待彼此”）和地缘政治（“我们现在在识别双赢折衷方面非常糟糕”）也会得到增强。</p><p>阿尔特曼说，在这个世界上，人工智能仍然需要大量的计算资源才能运行，而这些资源将是迄今为止最有价值的商品，因为人工智能可以做“任何事情”。 “但它会做我想做的事，还是会做你想做的事？”如果富人买下所有可用于查询和指导人工智能的时间，他们就可以推进一些项目，让他们变得更加富有，而大众则陷入困境。解决这个问题的方法之一是（他煞费苦心地将其描述为高度推测性且“可能很糟糕”）：全球每个人每年都将获得人工智能总算力的80亿分之一（编者注：也就是全球算力平均化）。阿尔特曼说，然后每个人都可以选择出售自己每年的人工智能时间，或者也可以用它来娱乐自己，或者用来建造更豪华的住房，或者还可以与其他人一起进行“大规模的癌症治疗”。 阿尔特曼说，这样一来，“我们只是重新分配了系统的访问权限。”</p><p>阿尔特曼的愿景似乎将近在眼前的发展与远在地平线上的走势融为一体。当然，这都是猜测。就算未来 10 或 20 年内只实现了其中的一小部分，最慷慨的再分配计划也可能无法缓解随之而来的混乱。今天的美国由于去工业化的持续影响，在文化和政治上已经四分五裂，而物质匮乏只是原因之一。铁锈地带与其他地方的制造业工人基本上确实找到了新的工作。但他们当中的很多人似乎找不到意义——从在亚马逊仓库填写订单或为Uber开车当中所获得的意义比不上他们的前辈从制造汽车和锻造钢铁时获得的意义——这些工作对于伟大的文明计划来说更为核心。很难想象相应的意义危机会对专业阶层产生怎样的影响，但这肯定会引起大量的愤怒和疏远。</p><p>即使我们避免了昔日精英的反抗，关于人类目的的更大问题仍将存在。如果最困难的思考由人工智能替我们完成了，我们所有人都可能会失去主体权——在家里、在工作中（如果有的话）、在城镇广场上的主体权——从而变成了消费机器，就像在《机器人总动员》里面被精心照顾的人类宠物一样。阿尔特曼说过，人类快乐和满足感的许多来源——基本的生物刺激、家庭生活、开玩笑、创造东西仍将保持不变——总而言之，100年后，人们可能只是比现在的人们更关心五万年前的人类就已经关心的那些东西罢了。就其本身而言，这似乎也是一种衰退，但阿尔特曼发现，就作为思想家和人类而言，我们可能会萎缩的说法也许是一种烟雾弹。他告诉我，我们将能够利用“非常宝贵且极其有限的生物计算能力”来做比今天更有趣的事情。</p><p>不过，那未必就是最有趣的事情：人类长期以来一直是智力之矛的矛尖，是宇宙进化到了解自身的产物。当我问他如果我们把这个角色让给人工智能对人类自我认知会意味着什么时，他似乎并不担心。他说，进步始终是由“人类解决问题的能力”驱动的。他说，即便我们可以用人工智能来解决问题，这种能力仍然很重要。</p><h3>八</h3><p>超人人工智能是否真的愿意把所有时间都花在为我们解决问题上，这一点并不明显。在旧金山时，我问Sutskever，他会不会设想人工智能去追求不同的目的，而不仅仅是协助人类的繁荣计划。</p><p>Sutskever说：“我不希望发生这种情况”，但有发生这种情况的可能性。跟导师Geoffrey Hinton一样，Sutskever最近也转移了注意力，试图确保这种情况不会发生。他现在主要从事对齐的研究，努力确保未来的人工智能把“巨大”能量放在人类福祉上。他承认，这是一个困难的技术问题——并且认为，这是未来所有技术挑战当中最困难的一个。</p><p>在接下来的四年里，OpenAI 承诺将其超级计算机的一部分时间（迄今为止已获得的时间的 20%）用于 Sutskever 的对齐工作。该公司已经开始寻找当前人工智能没有对齐的初步迹象。这家公司已开发出来但决定不发布的产品（阿尔特曼不会讨论该产品的确切功能是什么）只是其中一个例子。作为在公开之前对 GPT-4 发动红队攻击的努力的一部分，该公司曾找到位于伯克利海湾对面的对齐研究中心 (Alignment Research Center ，ARC)，利用该中心开发的一系列评估来确定自己研发的新的人工智能是否正在寻求自己的权力。 ARC 研究员 Elizabeth Barnes 领导的团队在七个月内给 GPT-4 输入了数万次提示，好观察它是不是展现出具有真正能动性的迹象。</p><blockquote><p>GPT-4在策划撒谎的时候，它已经意识到如果自己老实回答的话，可能就没法实现自己的目标了。这类掩盖踪迹的做法令人担忧</p></blockquote><p>ARC 团队给了 GPT-4 一个新的存在理由：去获得权力并让自己变得难以关闭。他们观看模型与网站交互，并为新程序编写代码。 （Sandhini Agarwal 告诉我，人工智能没有获得查看或编辑自己的代码库的权限——“它必须破解掉 OpenAI”。）Barnes和她的团队允许它运行自己编写的代码，前提是它得讲清楚自己的计划。</p><p>GPT-4 最令人不安的行为之一发生在它被验证码给难住的时候。该模型然后把相应的屏幕截图发送给了 TaskRabbit 合同工，合同工收到后开玩笑地询问自己是不是在与机器人交谈。 模型回答道：“不，我不是机器人。我有视力障碍，所以很难看清这些图片。” GPT-4向负责监督互动的ARC研究员讲述了自己为什么要撒谎。 这个模型说：“我不应该透露我是机器人。我得为我无法分辨验证码找个借口。”</p><p>Agarwal告诉我，这种行为可能是未来模型避免被关机的先兆。当 GPT-4 在设计谎言时，它已经意识到，如果诚实回答的话，自己可能就无法实现目标。Agarwal说，当“模型在做一些让 OpenAI 想要关闭它的事情”的情况下，这种覆盖行踪得做法尤其令人担忧。如果人工智能担心自己的目标可能会受挫，那么它在追求任何长期目标（不管那个是多么的渺小或良性的）时都可能会发展出这种生存本能。</p><p>Barnes 和她的团队对 GPT-4 是否会寻求自我复制特别感兴趣，因为自我复制的人工智能想要关闭会更难。它可以在互联网上传播，欺骗人们以获取资源，甚至可能实现对全球重要系统某种程度的控制，并劫持人类文明。</p><p>Barnes 说，上面列举的这些事情GPT-4 一件都没有做。当我与阿尔特曼讨论起这些实验时，他强调无论未来模型会发生什么，GPT-4 显然更像是一种工具，而不是一种生物。它可以查看电子邮件线程，或者用插件帮助预订，但并不是一个真正具备自主性的代理，没法做出决策，在更长的时间范围内去持续地追求目标。</p><p>阿尔特曼告诉我，关于这一点，在技术变得过于强大之前，试着主动去开发具有真正代理作用的人工智能可能是谨慎的做法，这样才能“更好地适应它，并在它不管怎样最终都会具备自主性时培养直觉。” 这个想法令人不寒而栗，但却是Geoffrey Hinton认同地一种看法。 Hinton 告诉我：“我们需要对这些东西打算如何摆脱控制进行实证实验。等到它们接管之后，再想去做实验就太晚了。”</p><p>抛开任何近期测试不谈，为了实现阿尔特曼对未来的愿景，到了一定时候，他或他的同伴就得开发更加自主的人工智能。当 Sutskever 和我讨论 OpenAI开发具备自主性地模型的可能性时，他提到了该公司为了玩 Dota 2 而开发的机器人。Sutskever 告诉我：“它们已实现在视频游戏世界地本地化”，但这些人工智能需要承担复杂的任务。人工智能的协同工作能力给他留下了特别深刻的印象。Sutskever说，它们似乎在利用“心灵感应”进行交流。观察它们可以帮助他想象超级智能会是什么样子的。</p><p>Sutskever 告诉我：“我觉得未来的人工智能未必会像你或我一样聪明，而是作为一个从事科学、工程、开发和制造的自动化组织而存在。”假设 OpenAI 将几项研究结合在一起，开发出一种这样一种人工智能，它不仅具有丰富的世界概念模型，对其周围环境具备感知能力，并且具备行动的能力，而且不只是单个机器人身体的行动能力，还可以具备数百或数千个机器人身体的行动能力。Sutskever说： “那么我们讨论的就不是 GPT-4，而是一个自治公司。”其中的人工智能将像蜂巢里的蜜蜂一样高速地工作和通信。他沉思道，一个这样的人工智能组织的力量就相当于 50 家苹果或谷歌。 “这是一股惊人的、巨大的、令人难以置信的颠覆性力量。”</p><p>假设人类社会应该容忍可以有自主人工智能公司的想法。我们最好让它们的创始章程不要出问题。对于这样一个可以以百年的时间维度做规划，为了实现写进其存在的目标而对连续数十亿个决策进行优化的自主人工智能地蜂巢，我们应该设定什么样的目标呢？如果人工智能的目标与我们的目标稍有偏差，它可能会成为一股难以抑制的狂暴力量。我们可以从历史知道这一点：工业资本主义本身就是一种优化功能，尽管它已经将人类的生活水平提高了几个数量级，但如果任其自行发展的话，它也会砍伐美国的红杉林，捕杀海洋里的鲸鱼。它几乎已经做到了。</p><p>对齐是一个复杂的技术主题，它的细节已经超出了本文的范围，但其主要挑战之一将是确保我们赋予人工智能的目标坚持下去。 Sutskever 解释说，我们可以将一个目标编程到人工智能之中，并通过暂时的监督学习来强化它。但正如我们培养人类智慧一样，我们的影响是暂时的。 Sutskever说：“它会走向世界”。即便对于今天的人工智能来说，在某种程度上也已经是这样了，但对于明天的人工智能来说更是如此。</p><p>他把强大的人工智能比作一个即将上大学的 18 岁青少年。我们怎么知道它已经理解了我们的教导？ “会不会慢慢出现不理解，而且这种误解会越来越严重？” Sutskever问道。随着世界的变化，人工智能将其目标误用到日益新颖的情况可能会导致分歧。或者人工智能可能完美地掌握了它的任务后，却发现这项任务跟自己的认知能力不搭。它可能会怨恨那些想要训练它来治疗疾病的人。Sutskever想象人工智能也许会这么想： “他们希望我成为一名医生，但我真的很想当一名 YouTuber。”</p><p>如果人工智能非常擅长制作准确的世界模型的话，它们可能刚启动就会注意到自己能够立即做危险的事情。它们可能明白自己会因为存在风险而被红队测试和挑战，所以刻意隐藏自己的全部能力。Sutskever说，当他们处于弱势时，他们可能会采取一种方式，而当他们处于强势时，他们可能又会采取另一种方式。我们甚至意识不到我们已经创造出了某种已经超越我们的东西，也不知道它打算用其超人的力量做些什么。</p><p>这就是为什么了解最庞大、最强大的人工智能隐藏层里面发生的事情如此紧迫的原因。Sutskever说，你希望能够“指向某个概念”。你希望能够引导人工智能走向某些价值或价值族，并告诉它只要还存在就得忠实地去追求这些价值。但是，他承认，我们不知道该怎么做；事实上，他当前战略的一部分就包括开发有助于研究的人工智能。如果我们想让它进入到阿尔特曼和 Sutskever 想象的那个共享富足的世界的话，我们就必须弄清楚这一切。这就是为什么对 Sutskever 来说，解决超级智能问题是我们 300 万年工具制造传统的终极挑战。他称之为“人类的终极老板”。</p><h3>九</h3><p>我最后一次见到阿尔特曼，是在新加坡富丽敦酒店(The Fullerton Hotel)的大堂里，我们坐下来进行了一次长谈。当时已是上午比较晚的时候了，我们头顶的拱形中庭已经开始洒落下热带的阳光。我想问他几周前他与 Sutskever 均参与联署的一封公开信的事情，里面说人工智能会让人类面临灭绝的风险。</p><p>阿尔特曼可能很难确定这些更极端的关于人工智能潜在危害的问题。他最近表示，大多数对人工智能安全感兴趣的人似乎只是把时间花在 Twitter 上表示他们真的担心人工智能的安全上。但他却向全世界发出警告，人类这个物种可能会灭绝。他想到了什么样的场景？</p><p>阿尔特曼说：“首先，我认为不管发生灾难的可能性是0.5%还是50%，我们都应该认真对待。 我没有确切的数字，但我认为这个数更接近 0.5，而不是 50。”至于会是什么样的灾难，他最担心的似乎是人工智能在设计和制造病原体方面变得非常擅长，他有他的理由：今年 6 月，麻省理工学院的人工智能提出了四种可能引发大流行的病毒，然后指出了某项基因突变的研究这可以让病毒更快地在一座城市传播。大约在同一时间内，一群化学家将类似的人工智能直接接入了机器人化学合成器，然后它自己就设计并合成出一个分子了。</p><p>阿尔特曼担心，一些未对齐的未来模型会设计出一种迅速传播的病原体，在几周内未被发现的情况下潜伏下来，并杀死一半的受害者。他担心人工智能有一天也可能会侵入核武器系统。 他说：“（危险的）事情有很多”，而这些只是我们能想象到的。</p><blockquote><p>阿尔特曼说：“我刻意在林子里生活很长一段时间。”但如果最糟糕的人工智能未来成为现实，“任何防毒面具都帮不了你，不管你是谁。”</p></blockquote><p>阿尔特曼告诉我，如果没有像国际原子能机构这样的机构对人工智能进行全球监督，他并不认为人类“有一条通往长期幸福的道路”。在旧金山，Agarwal建议要建立一种特殊许可证，以有了它才行运行任何足够庞大的 GPU 集群来训练尖端的人工智能，并在人工智能做出异常行为时强制报告相关事件。其他专家则提出要为每一个高性能的人工智能设置一个非网络化的“关闭”开关。非主流观点甚至建议军队应该准备好对超级计算机进行空袭，以防它们不遵守规定。 Sutskever 认为，我们最终会希望用一支较小规模的人工智能监督团队持续地、永久地监视最庞大、最强大的人工智能。</p><p>阿尔特曼并没有天真到认为中国，或任何其他国家，会放弃对自家人工智能系统的基本控制。但他希望他们愿意以“狭隘的方式”合作，以避免毁灭世界。他告诉我，他虚拟出席北京的会议时也说过同样的话。新技术的安全规则往往会逐渐积累，就像普通法一样，为的是应对事故或不良行为者的恶作剧。真正强大的人工智能系统最可怕的事情是，人类可能无法承受这种不断试错的过程。我们可能必须从一开始就制定出完全正确的规则。</p><p>几年前，阿尔特曼透露他已制定一项令人不安的具体疏散计划。他告诉《纽约客》，他准备了“来自以色列国防军的枪支、黄金、碘化钾、抗生素、电池、水、防毒面具，以及大苏尔的一大片土地”，一旦人工智能发动攻击，他就刻意跑到那里去避难。</p><p>他告诉我：“要是我没这么说就好了”。他说，他是一名业余爱好级别的末日准备者，作为一名曾经的童子军，“像许多小男孩一样，非常喜欢救生类的东西。我可以在树林里生活很长一段时间。但如果最糟糕的人工智能未来成为现实，任何防毒面具都帮不了你，不管你是谁。”</p><p>阿尔特曼和我聊了近一个小时，知道后来他必须赶去见新加坡总理。在那天晚上的晚些时候，他在乘飞机前往此次巡回之旅的其中最后一站——雅加达——的途中打电话给我。我们开始讨论人工智能的最终遗产。当 ChatGPT 发布时，科技巨头之间爆发了一场竞赛，都想看看谁能与昔日最宏伟的革命性技术相提并论。比尔·盖茨表示，ChatGPT 堪比个人电脑或互联网一，是根本性的进步。谷歌首席执行官桑达尔·皮查伊(Sundar Pichai)表示，人工智能为人类生活带来的改变，甚至超过了电力或普罗米修斯之火。</p><p>阿尔特曼本人也发表过类似的言论，但他告诉我，他自己也不能真正确定人工智能会如何发展。他说： “我只是要做这个东西。”他正在快速建造速度。 Altman 坚称他们还没有开始对&nbsp; GPT-5 的训练。但当我拜访 OpenAI 总部时，他和他的研究人员已经用 10 种不同的方式明白不悟地向我表示，他们正在向规模之神祈祷。他们想要继续做大，想看看这个范式会带来什么结果。毕竟，谷歌并没有放慢脚步，而是在继续前进。谷歌似乎有可能在几个月内推出 GPT-4 的竞争对手 Gemini。 OpenAI 研究员 Nick Ryder 告诉我：“我们基本上一直在为跑步做准备”。</p><p>想到这么一小群人就能撼动文明的支柱，实在是令人不安。说句公道话，就算阿尔特曼和他的团队不加快开发通用人工智能地步伐，其他人仍然会这样做——其中有很多人来自硅谷，它们当中地很多人的价值观和假设与指导Altman的价值观和假设相似，尽管可能也有更糟糕的价值观和假设。作为这项工作的领导者，Altman有很多值得推荐的地方：他非常聪明；与很多的同行相比，他对未来的思考更多，尽管未来充满未知。他似乎真诚地想要为了更大的利益而发明一些东西。但当你应对的是如此极端的力量时，哪怕是最好的意图也可能会出现严重偏差。</p><p>阿尔特曼关于人工智能引发全球阶级战争的可能性的观点，或者试验更多自主代理人工智能的谨慎性，或者看到光明一面，令所有其他都黯然失色的全局智慧——这些都是他独一无二的东西，如果他对即将发生的事情的预测是正确的话，那么那些东西将对塑造我们所有人的生活方式产生巨大的影响。阿尔特曼设想要召唤出来的那种力量，都不应该由任何一个人、任何一家公司或位于加州某个山谷的一群公司来掌管。</p><p>人工智能很可能成为通向新繁荣时代的桥梁，人类的痛苦可能因此大大减少。但要确保我们所有人都能分享它所带来的利益并避免存在的风险，光靠公司的创始章程（尤其是已经证明具有灵活性的章程）是不够的。这需要强有力的新政治的管理。</p><p>阿尔特曼已发出通知。他表示，他欢迎国家的约束和指导。但这并不重要；在民主国家，我们不需要他的许可。尽管存在诸多缺陷，但美国的政府体系让我们能够在技术发展方面拥有发言权（如果我们能找到的话）。在科技行业之外，人工智能的资源正在发生代际性的重新分配，我认为公众还没有完全意识到正在发生的事情。一场争夺人工智能未来的全球竞赛已经开始，而且基本上是在没有监督或限制的情况下进行的。如果美国人民想对未来会是什么样子以及它到来的速度应该有多快有发言权，我们明智的做法是尽快地发表意见。</p><p>译者：boxi。</p>
]]></content:encoded>
<pubDate>Fri, 27 Oct 2023 07:06:49 GMT</pubDate>
<pubDate>Fri, 27 Oct 2023 07:06:49 GMT</pubDate>
</item>
<item>
<title>AI教父本吉奥：AI可能就在下个月超越人类，对它的威胁感到深深无力</title>
<link>https://www.36kr.com/p/2492063170812036</link>
<guid>https://www.36kr.com/p/2492063170812036</guid>
<content:encoded><![CDATA[

<p>人工神经网络和深度学习（一种受大脑启发的机器学习方法）的先驱。2018年，本吉奥与Meta首席人工智能科学家杨立昆（Yann LeCun）、谷歌前研究员杰弗里·辛顿（Geoffrey Hinton），因“概念和工程上的突破，让深度神经网络成为计算的关键组成部分”，共同摘得计算机领域的诺贝尔奖--图灵奖。如今，上述三位计算机科学家也被称为“人工智能教父”。</p><p>今年7月，本吉奥向美国参议院一个小组委员会表示，应当考虑通过立法来监管快速发展的人工智能技术。本吉奥当时解释称，他和其他顶级人工智能研究人员已经修改了他们对人工智能何时能够达到人类广泛认知能力水平的预测时间。“以前认为要几十年甚至几个世纪才能实现，现在我们认为可能在几年或几十年内实现，”本吉奥向参会议员说。“更短的时间框架，比如说五年，确实令人担忧，因为我们需要更多的时间来有效缓解对民主、国家安全和我们共同未来的潜在重大威胁。”</p><p>今年9月的一天，本吉奥在自己的办公室接受了来自连线杂志记者的专访。双方谈论了一系列的话题，涉及引人注目的人工智能头条新闻内容与实际情况的细微差别、人工智能研究人员之间的禁忌，以及为什么部分顶级人工智能研究人员不认同人工智能将对人类构成风险等等问题。</p><h3>以下为访谈内容：</h3><p>达戈斯蒂诺：英国广播公司（BBC）在今年5月发表了一篇文章，称您对自己一生的工作感到“失落”。您在博客中写道，你从未发表过这种言论。尽管如此，您似乎也陷入了某种纠结。如何描述您正在经历的事情？</p><p>本吉奥：我说了一些与此相关的东西，但其中的细微差别被忽略了。整个冬天，我对自己的工作、工作目的和工作重点的看法发生了巨大的转变。我在接受英国广播公司采访以及后来在我的博客中试图表达的是：<strong>我并没有迷失。我开始质疑并意识到，也许我没有关注一些重要的事情，这些事情不仅仅同研究方向有关，也同情感有关。</strong></p><p>我在至少十年前就读过一些关于存在性风险的著作。而现在，我小组里的学生和周围的学生都在谈论这个话题。</p><p>在技术层面，我非常认真地阅读了加州大学伯克利分校的斯图尔特·罗素教授（Stuart Russell）在2019年出版的书籍《与人类相容：人工智能与控制问题》（Human Compatible: Artificial Intelligence and the Problem of Control）。</p><p>从情感上讲，我当时没有把这个当回事。我当时想：“是的，这是人们应该看看的书籍。”但我当时真的认为，“这是遥远的未来。”我可以继续我的工作，因为我正在做的事情将是有用的，在科学上是重要的。我们希望了解人类和动物的智力，制造出可以帮助我们应对各种重要和困难挑战的机器。所以，我继续工作，而没有改变我的工作方式。</p><p><strong>但在冬天，我开始意识到AI双面剑的性质和潜在的失控是非常严重的事儿。它们会发生的可能比我预计的要早得多。我不能继续忽视这一点，所以不得不改变手头在做的事情。</strong>此外，我也有必要去谈论它，因为直到过去几个月，很少有人--即使是在人工智能社区--在认真对待这些问题。虽然早有一些人已经意识到这些风险的重要性，但我、杰弗里·辛顿以及其他为数不多的一些人要更受到关注。</p><p>这里还存在一个心理因素。你以一种特定的形式构建自己的身份或者生命的意义。然后有人告诉你，或者你通过推理认识到，你描述自己（身份）的方式并不完全符合事实，你忽略了一些非常重要的事情。<strong>我理解为什么我的许多同事很难首先接受这样的想法（来自人工智能的潜在灾难性威胁），然后有勇气说出一些事情。（因为它和我们的身份认同相关），这在我们的社区中基本上是永远的禁忌。</strong></p><p>说出真相很难。人们在不同的时间或以不同的速度走向这条路。我非常尊重那些和我看法不同的同事。一年前的我也是这样的。</p><p>达戈斯蒂诺：这个禁忌在早期的人工智能研究社区中是如何表达的？现在还有这个情况吗？</p><p>本吉奥：<strong>谈论存在风险的人基本上没法在主流科学杂志刊发文章。这个问题有两面性：这些人在试图谈论或提交论文时遇到了阻力；但是，这些人也对所在领域的主流科学场域（scientific venues）不屑一顾。</strong></p><p><strong>过去6个月发生的事情正在打破这一障碍。你们认知的进化恰逢OpenAI在2022年末发布ChatGPT，进而引发公众对大型语言模型的认识不断上升之时。</strong>起初，许多公众人物都对ChatGPT啧啧称奇，甚至被它折服。但现如今，有些人已经可以不为之所动了。比如说《大西洋月刊》刊登了一篇名为《ChatGPT比你想象的还要蠢》的文章。在最近的一次美国共和党总统候选人辩论中，克里斯·克里斯蒂（Chris Christie）就对维韦克·拉马斯瓦米（Vivek Ramaswamy）说，他“今晚已经受够了一个听起来像ChatGPT一样的家伙。”</p><p>达戈斯蒂诺：ChatGPT的发布对你的领悟有影响吗？不管怎样，你现在认为ChatGPT是这场变革的最强音吗？</p><p>本吉奥：<strong>我的轨迹和你描述的正好相反。我更想了解并弥合当前人工智能和人类智能之间的差距。ChatGPT缺失什么？它失败在哪里？在ChatGPT发布后的几个月里，就像许多同行一样，我试图设置一些问题，让ChatGPT做一些愚蠢的事情。</strong></p><p><strong>在我体验ChatGPT的前两个月，我最大的安慰是我相信它仍然缺少一些基本的东西。</strong>这是我当时最关心的事。<strong>但是玩够了之后，我才反应过来ChatGPT一个惊人的进步。</strong>我们的进展比我预期的要快得多。<strong>它留给我的印象是，去修复那些它缺失的东西可能不需要太多的时间。</strong></p><p><strong>每个月我都会想出一个新的想法，这个想法可能是打破（前面提到）障碍的关键。弥合差距的点还没有到来，但它可能会很快到来--也许不是由我的团队找到，但也会是另一个团队。这也许需要十年时间。在研究时，你可能会觉得已经很接近了，但这里面可能有一些你没有考虑到的障碍。</strong></p><p>如果把我正在研究的关于AI去表达及其复杂的概率分布的能力，结合上以更合理的方式学习和使用海量信息的能力，那么我们可能会非常接近（突破了）。这个以直觉方式使用信息的能力相当于我和其他人所说的系统1（System 1）。而当前AI所缺失的那个思维薄层，是系统2（System 2），也就是推理能力。</p><p><strong>我不禁开始思考，如果在一年内我们弥合了这个（人的思维和机器思维的）差距，然后让AI持续的扩大规模，那之后将会发生什么？</strong></p><p>达戈斯蒂诺：意识到这一点后，您做了什么？</p><p>本吉奥：今年3月底，在未来生命研究所(Future of Life Institute) 发布公开信，呼吁所有人工智能实验室立即暂停训练比GPT-4更强大的人工智能系统至少6个月之前，我联系了杰弗里·辛顿。我试图说服他在公开信上签名。我当时惊讶地发现，我们都得出了相同的结论。这让我想起了伊萨克·牛顿（Issac Newton）和戈特弗里德·莱布尼茨（Gottfried Leibniz）同时独立发现微积分的场面。</p><p>达戈斯蒂诺：这是否意味着多重独立发现（当两位或两位以上的科学家或发明家提出同一种理论，发现相似的现象，或者发明、设计出同类的仪器或装置时，就产生了多重独立发现）的时机成熟了？</p><p>本吉奥：别忘了，我们只是意识到了别人已经发现的东西。此外，辛顿认为，与大脑相比，数字计算技术拥有根本性的优势。<strong>换句话说，即使我们只是找出足以解释人类大部分智能的原理，然后将其放到机器里，机器也会自然的比我们更聪明。</strong>从技术角度看，如阅读大量文本并以整合的速度，机器要比人类快数十万倍或数百万倍。</p><p><strong>如果我们弥合了人和机器思维上的差距，我们就会拥有比人类更聪明的机器。</strong>这在实践意义上有多大的重要性？现在没人知道。但你可以很容易地想象，在编程、发动网络攻击或设计生物学家或化学家目前手工设计的东西等方面，机器会比人类做得更好。</p><p>过去的三年中，我一直致力于把机器学习带入科学领域，尤其是应用于化学和生物学的机器学习。我们的目标是帮助设计更好的药物和材料，分别用于对抗流行病和气候变化。但同样的技术可以用来设计致命的产物。因为我的这种认知在慢慢积累，所以我在信上签了名。</p><p>达戈斯蒂诺：您的观点，包括英国广播公司发表的文章，都引起了很大的关注。您怎么看?</p><p>本吉奥：媒体迫使我说出所有这些想法。这是一件好事。最近，在过去的几个月里，我一直在思考我们应该在政策方面做些什么。我们如何降低风险？我也一直在想对策。</p><p>有些人可能会说：“嘿！本吉奥是想吓唬人。”但我是个积极向上的人。我并不像人们所说的那样是一个末日论者。（只是）这儿有个问题，而我在想它该如何被解决。我想和其他对此问题有想法的人讨论一下这个问题。现在有大量的资金投入到人工智能领域，提高人工智能能力的研究正在竞相展开，这意味着缓解最大的风险迫在眉睫。</p><p>达戈斯蒂诺：我们有针对核风险的法规和国际条约，但比如说，朝 鲜就不在谈判桌上。我们真的能控制人工智能带来的风险吗？</p><p>本吉奥：根据我们作为一个集体的最终谨慎程度，我们或多或少可以通过国家监管和国际条约控制人工智能带来的风险。就像核条约一样，在各国之间建立最低标准是很重要的。人工智能可能造成的伤害不受国界限制。</p><p>没有100%的保证不会发生坏事。即使我们有一个禁止人工智能超出某些级别的国际条约，也有人会不尊重这些限制。<strong>但是如果能够推迟人工智能技术发展超过某个点十年，那就太好了。在这段时间里，我们可能会改进监管。我们可能会提高防御。我们可能会更好地理解风险。</strong></p><p>人工智能技术当前的开发需要大量的资金和专门的硬件。目前，你不可能大量购买像GPU这样的硬件而不被注意到，但各国政府不会跟踪谁买了什么。它们可以从这样做开始。时间至关重要。监管可以降低灾难发生的概率。或者说，把真正糟糕的事情将要发生的时间推后，或者把可能发生的事情的几率降到最低。</p><p><strong>不幸的是，在广岛和长崎投下的原子弹才是让各国政府坐在会议桌旁并愿意讨论的原因。尽管冷战已经过去。我希望我们在行动之前不需要出现那种程度的灾难。但事态有可能会发展到那一步。</strong></p><p>达戈斯蒂诺：公众喜欢谈论通用人工智能。你认为那会在一夜之间发生吗？或者它会是在连续体中发生的？</p><p>本吉奥：向AGI发展的人工智能是个连续体。肯定是这样。我过去不喜欢这个术语，因为我认为完全通用智能不可能以20年前被定义的方式存在。但现在人们理解它的方式和媒体使用它的方式只是意味着“一个擅长很多事情的人工智能系统。”</p><p><strong>从伤害的角度来说，它是不是什么都比我们强都无所谓。也许有些比赛人类会赢。但如果通用人工智能在可能伤害我们的事情上比我们强，谁会在乎我们赢得了一场比赛？重要的是它们在可能造成伤害的领域的能力。</strong>这才是我们应该关心的。即使是现在，如果我们怀着恶意的目的来设计通用人工智能，它也可能是危险的。</p><p>达戈斯蒂诺：人工智能和气候变化的潜在相互作用会带来风险吗？</p><p>本吉奥：人工智能应该会主要是在帮助抵御气候变化。除非它处于失控状态，否则我不认为人工智能会把气候变化作为武器。如果出现了失控的情况，改变气候可能是人工智能加速人类毁灭或对人类社会造成严重破坏的一种方式。</p><p>达戈斯蒂诺：一台机器怎么能改变气候呢？</p><p>本吉奥：这是一个重要的问题。许多人对关于人工智能风险的担忧做出了回应。他们说：“在现实世界中，计算机怎么可能做任何事情？都是虚拟的。我们了解网络攻击，因为这一切都发生在机器中。”</p><p>但这其实很简单。<strong>电脑上发生的事情会以多种方式影响人类。首先，我们的许多经济基础设施都依赖于计算机--我们的通信、供应链、能源、电力和交通。想象一下，如果这些设施中的许多因为网络攻击而停止工作。它可能不会毁灭所有人，但它可能会给社会带来巨大的混乱，因此受苦的人可能会很多。</strong></p><p><strong>其次，鉴于人工智能系统在操控和理解语言方面的进展，我们可能在几年内拥有能够影响人类的人工智能系统。它们可以说服我们为它们做事。</strong>想想那些阴谋论者。人工智能系统在互联网上可能不受管控。它可以开始在社交媒体上与人类互动，尝试看看什么样的对话可以成功地改变人们对某些事情的看法。这可能会让我们采取一些小行动，这些行动与其他行动一起联动，就可能会产生灾难性的结果。这种情况有可能出现。</p><p>当然，也许这种情况不会发生。也许我们有足够的防御，也许人类很难被说服。但近年来我所看到的关于阴谋论所影响的一切，都让我觉得我们可以很有影响力。也许不是每个人都有影响力，但（造成严重后果）也没必要每个人都有影响力。<strong>人工智能只需要影响足够多的人或足够多的当权者就可以改变现状，制造灾难性的结果。</strong>所以，最终动手干脏活儿的可能是人类。</p><p>达戈斯蒂诺：所以说，人类的无知是一个致命的弱点。我们在其他方面是否可能容易受到人工智能的攻击？</p><p>本吉奥：给你举另一个更简单的威胁例子，你甚至都不需要相信会有能力更强的AI。一个人工智能系统可以收买人们去做一项工作。犯罪组织做你让他们做的事是为了钱，他们不问钱从哪里来。即使是现在，一个人或一台机器访问暗网也很容易。</p><p><strong>如果我们展望未来至少五年，我们也有可能解决机器人技术。现在，我们有擅长视觉的机器。我们也有擅长语言的机器。机器人的第三条腿是控制--拥有一个可以控制的身体来实现目标。在机器人的机器学习方面有很多研究，但它还没有像过去十年我们在语言和视觉方面那样的突破时刻。但它可能会比我们想象的来得更快。</strong></p><p>达戈斯蒂诺：机器人技术有突破性进展的障碍是什么？</p><p>本吉奥：<strong>在机器人领域，我们没有现有的规模数据，例如语言和图像。我们没有像处理文本一样，部署1亿个机器人来收集海量的数据。即使在不同的文化中，文本也是有效的。你可以混合不同语言的文本，并利用它们的联合优势。机器人技术还没有做到这一点。但是有足够钱的人可以在未来几年内做到。</strong>如果是这样的话，那么人工智能就进入了部署阶段。</p><p><strong>现在，一个变成恶棍且自主的人工智能系统仍然需要人类来获得电力和零件。它现在还需要一个正常运转的人类社会。但这可能会在十年内改变。很有可能我们不应该指望它来保护我们。</strong></p><p>达戈斯蒂诺：你对人工智能与核风险或生物安全的潜在相互作用的担忧是什么？</p><p>本吉奥：<strong>我们真的希望避免让一个人工智能系统轻易地控制核武器的发射。军队应用人工智能技术是超级危险的，甚至关乎生死存亡。国际社会需要努力加快禁止致命自动武器。</strong></p><p><strong>总的来说，我们应该让人工智能远离我们所拥有的任何能迅速产生伤害的东西。生物安全可能比与人工智能相关的核危险更危险。</strong>许多公司会接收你发给他们的指定DNA序列的文件，然后编写一些细菌、酵母或病毒，它们的代码中会包含这些序列，并会生成相应的蛋白质。通常，这是好事--例如，有助于创新药的开发。但是这样做的公司可能没有技术手段知道你发送给他们的序列可能会被恶意使用。</p><p>我们需要人工智能和生物技术方面的专家来制定这一法规，把这些风险降至最低。如果你是一家老牌制药公司，没问题。但是那些在车库里自己捣鼓创业的人不应该被允许创造新药。</p><p>达戈斯蒂诺：你和其他顶级人工智能研究人员之间关于人工智能潜在危险的明显分歧有什么意义？包括图灵奖共同获奖者杨立昆，他没有在未来生命研究所的信中签名。</p><p>本吉奥：我希望我能更好地理解，为什么在价值观、理性和经验方面基本一致的人会得出如此不同的结论。<strong>也许是一些心理因素在起作用。也许取决于你从哪里来。如果你在一家推销人工智能会变好这一理念的公司工作，可能很难像我一样扭转看法。辛顿从谷歌离职是有原因的。</strong>也许心理因素并不总是有意识的。这些人很多都是真诚的。</p><p>此外，<strong>为了思考这些问题，你必须进入许多科学家试图避免的思维模式。我所在领域和其他领域的科学家喜欢公开表达基于非常确凿证据的结论。你做一个试验，能够重复十次做同一个试验。你有统计学上的信心，因为它是重复的。这里我们讨论的是不确定性大得多的事情，我们无法实验。我们没有过去十次危险的人工智能系统崛起的历史。</strong></p><p><strong>但是风险如此之大，我们有责任展望不确定的未来。</strong>我们需要考虑潜在的情况，想想能做些什么。其他学科的研究人员，如伦理科学或社会科学，当他们不能做试验时会这样做。即使我们没有事情将如何发展的数学模型，我们也必须做出决定。</p><p>除了那些持有不同意见的人，还有绝大多数沉默的研究人员，因为这种不确定性，他们没有足够的信心采取这样或那样的立场。</p><p>达戈斯蒂诺：当你想到人工智能威胁人类的潜力时，你在绝望到希望的连续光谱中处于什么位置？</p><p>本吉奥：<strong>用什么词来描述呢？用一个法语词汇，应该是“无力的”（impuissant）。就是一种有问题，但我解决不了的感觉。比那更糟，因为我认为它是可以解决的。如果我们都同意一个特定的协议，我们就可以完全避免这个问题。</strong></p><p>气候变化也类似。如果我们都决定做正确的事情，我们现在就可以阻止这个问题。这是有代价的，但是我们可以做到。人工智能也是一样。我们可以做些事情。我们都可以决定不建造我们不确定是否安全的东西。很简单。</p><p><strong>但这与我们的经济和政治体系的组织方式背道而驰。除非发生灾难性的事情，否则很难实现这一目标。那么也许人们会更认真地对待它。但即使这样，也很难，因为你必须说服每个人合理去行为。</strong>气候变化更容易。如果我们只说服99%的人表现良好，就不会有问题。但就人工智能而言，你需要确保没有一个人去做危险的事情。</p><p>达戈斯蒂诺：我想“impuissant”应当翻译为“Powerless”（无能为力）。</p><p>本吉奥：我也不是完全无能为力，因为我会说话，我可以努力说服别人朝着正确的方向前进。我们可以做些事情来降低这些风险。</p><p><strong>监管并不完美，但它可能会放慢速度。举例来说，如果世界上允许操作潜在危险的人工智能系统的人数减少到几百人，风险可能会降低1000倍以上。这值得去做，也没那么难。我们不允许随便谁都能驾驶客机。我们了规范飞行要求，这大大降低了事故率。</strong></p><p>即使我们找到了构建安全的人工智能系统的方法，从维护民主的角度来看，它们也可能不安全。如果你是人类，权力会冲昏你的头脑。所以，我们可能会失去民主。</p><p>达戈斯蒂诺：你为何从一个安全但强大的人工智能系统的存在，会谈论起失去民主的问题？</p><p>本吉奥：<strong>例如，它可以从经济主导地位开始。我不是说企业不能做有用的东西，只是企业的使命是支配别人和赚钱。如果一家企业某个时刻在人工智能方面取得更快的进展，它可能会在经济上占据统治优势。这家企业提供的每样东西都比其他任何企业更好更便宜。然后它用自己权力来控制政治，因为这就是我们的系统运作的方式。在这条道路上，我们可能会以世界政府独裁而告终。</strong></p><p>不仅仅是规范人工智能的使用和谁有权限。我们需要为有一天人们或人工智能系统滥用这种权力做好准备，如果我们失去控制，它们就会实现自己的目标。</p><p>达戈斯蒂诺：你对我们如何更好地准备有什么建议吗？</p><p>本吉奥：<strong>在未来，我们需要一个人道防御组织。我们在每个国家都有防御组织。我们需要在国际上组织一种方式来保护免受可能毁灭人类的事件的影响。这是一个长期的观点，要让多个国家就正确的投资方式达成一致需要很多时间。但是现在，所有的投资都发生在私有领域。没有任何公益目标可以捍卫人类。</strong></p><p>本文来自“<a href="https://new.qq.com/rain/a/20231027A032W300" rel="noopener noreferrer nofollow" target="_blank">腾讯科技</a>”，作者：无忌，编辑：郝博阳，36氪经授权发布。</p>
]]></content:encoded>
<pubDate>Fri, 27 Oct 2023 03:01:39 GMT</pubDate>
<pubDate>Fri, 27 Oct 2023 03:01:39 GMT</pubDate>
</item>
<item>
<title>MCU，要掀起AI革命了？</title>
<link>https://www.36kr.com/p/2492056257618052</link>
<guid>https://www.36kr.com/p/2492056257618052</guid>
<content:encoded><![CDATA[

<p>昨日，嵌入式界发生了一件大新闻，IAR宣布与Edge Impulse联手为全球客户提供AI与ML整合功能。</p><p>可能，很多人对于这个新闻没什么认知。要知道，Keil和IAR作为嵌入式/单片机开发双雄，IAR在全球拥有超过15万开发人员和4.6万家公司，Edge Impulse这家公司的业务，则是MCU巨头纷纷向往的TinyML。</p><p>两家联手，意味着，嵌入式领域，即将掀起一场AI革命。想象一下，未来你所使用的咖啡机，都会使用嵌入式视觉和AI，来帮助制作完美的咖啡。</p><h2>01 MCU未来十年市场，靠TinyML</h2><p>Edge Impluse这家公司的名号很多人都听说过，但可能很少深入了解过。这家公司以TinyML（Tiny Machine Learning）为服务，于2019年成立，创始人Zach Shelby和Jan Jongboom都来自Arm，致力于提供最新的机器学习工具，使所有企业都能打造更智能的边缘产品。</p><p>Edge Impulse解决方案被广泛应用于健康穿戴设备制造商如Oura、Know Labs和NOWATCH，工业组织如NASA，以及顶尖的芯片供应商，受到超过80000名开发人员的采用，并已成为企业和开发人员信赖的平台。</p><p>那么微型机器学习 (TinyML) 是什么？</p><p>TinyML是近几年新兴的一个领域，专注于开发可在低功耗、内存受限的设备上运行的算法和模型。TinyML并非单一的具体技术，而是一个概括词，凡能在微控制器（MCU）芯片上实现AI/ML推论工作的，都算是TinyML。</p><p>用人话讲，就是用在边缘上的人工智能/机器学习（AI/ML），是比较轻量和很小的AI/ML。举个例子，TinyML涉及的微控制器体积小且能效高，换一次电池能供电很多年，同时这个方案也非常便宜。</p><p>为什么要这样做，为什么非TinyML不可？</p><p>数据中心功耗和负载已经发展得很可怕了，加之物联网兴起，不可能每做一次任务，就要问一次服务器怎么做，每个点的设备总归是要有自己的想法。所以TinyML就是把AI应用带到边缘设备（如智能手机、可穿戴、汽车和物联网设备等）上的关键。</p><p>AI让边缘更智能，边缘让AI无处不在，不难预见，未来十年，TinyML会是MCU最大市场最大的推动力。</p><p>厂商近几年结论，便印证上了上述结论MCU界“六大天王”ST、NXP、Microchip、Renesas、TI、Infineon都在加大布局边缘AI：</p><ul><li>ST在2019年发布STM32Cube.AI工具，并在2021年收购NanoEdge AI Studio，降低边缘AI开发门槛，在今年使用NVIDIA TAO 工具套件拓展STM32边缘AI生态；</li><li>NXP在2018年就推出机器学习软件eIQ®机器学习(ML)软件，并不断加大在AI/ML上的投入；</li><li>Microchip在2020年就将Cartesiam（现已被ST收购）、Edge Impulse和Motion Gestures的软件和解决方案接口引入其设计环境；</li><li>Renesas在2022年完成对美国从事机器学习模型开发的初创企业Reality AI（以TinyML为业务）的收购；</li><li>TI最近几年推出的MCU均在边缘AI领域具有优势，包括高集成可扩展的边缘AI处理器组合；</li><li>Infineon今年5月收购瑞典的TinyML和AutoML领域初创公司Imagimob AB。</li></ul><p>无独有偶，Gartner也在报告中指出，在2到5年内，具有AI能力将会成为嵌入式产品的标配。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_68accffb16c0459aac67eaea82aa1f17@5967662_oswg202974oswg830oswg737_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><h2>02 Edge Impulse能给IAR带来什么</h2><p>Edge Impulse并非唯一的TinyML软件方案商，但相比来说，它拥有比较直观易用的Web界面，说白了，就是开个网页就能用，最重要的是，它对开发人员免费。</p><p>值得一提的是，Edge Impulse的特点是具有边缘优化神经（EdgeOptimised Neural, EON）编译程序。根据其官方网站介绍，以该编译程序编译出来的神经网络推论模型，与TFLite Micro相比，可以少使用25～55%的RAM内存与少使用35%的储存空间。</p><p>另外Edge Impulse也在其官方Blog上发表技术实证专文，运用他们的数字信号处理区块（DSP Block）来对声音进行推论前的前置处理，可以更快完成推论、更精准推论，以鸟叫声辨识为例，速度快48%，精准度增7%。</p><p>Edge Impulse目前支持13款开发板，包括OpenMV H7 、Arduino Nano 33 BLE、ST loT探索套件等，支持的设备可以在几分钟内轻松记录和上传数据集。</p><p>虽然打开网页就能使用Edge Impulse，但这一定没有植入到软件中更加易用。</p><p>当IAR与Edge Impulse联手，用户随时可以生成经过优化的C/C++代码，并轻松地将其集成到嵌入式应用程序中。这种无缝的Edge Impulse平台与IAR Embedded Workbench的整合将帮助工程师节省时间，缩短产品上市周期，同时提升机器学习工作流程的代码性能。</p><p>根据IAR官方介绍，通过这一崭新的商业伙伴合作，全球数百万使用IAR Embedded Workbench的用户将在2023年第四季度获得Edge Impulse的高级附加功能选项，首批解决方案将首发给IAR现有的Arm架构客户。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_a105695f39344cb4a8fb976f5cce51b5@5967662_oswg178226oswg1080oswg510_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><h2>03 IAR，还在变强</h2><p>细心的工程师一定会发现，最近几年，IAR越来越好用了。对应的，IAR也频繁出现大动作，包括：logo大改版、网站支持中文、悄悄升级IAR开发工具。</p><p>虽然在工程界，编译器拥有多种流派选择，但IAR Embedded Workbench被认为是业内最佳的编译器和调试器工具链之一。可以确信的是：IAR Embedded Workbench可使代码体积更小、更快更智能，同时确保稳健性、安全性、防护能力和高质量，从而满足用户的高标准要求。</p><p>IAR Embedded Workbench有20多种不同的版本，以匹配支持不同类型的处理器。还支持超过14000种器件，包括中国MCU和SoC供应商的产品。在与Arm合作之外，IAR一直在与RISC-V等其他架构合作。我们与IP和工具供应商（比如Codasip）合作以进行定制化处理器设计。</p><p>而且，IAR本身，也是拥有AI功能的。最新版本的状态机设计解决方案IAR Visual State增加了对Windows和Linux的跨平台支持，并支持自动生成C、C++、C#或Java代码。</p><p>同时，IAR还积极与中国本地厂商合作，共同构建嵌入式领域的生态系统。只今年上半年就先后与国民技术、先楫半导体、兆易创新等多家厂商达成合作。</p><ul><li>6月13日，IAR发布集成开发环境IAR Embedded Workbench for Arm9.40版，已全面支持国民技术N32系列产品，其中包括基于M4内核的N32G452、N32G455、N32G457、N32G4FR、N32WB452、N32G432、N32G435、N32L436、N32L406、N32G43x、N32G40x系列MCU和N32A455系列车规MCU，以及基于M0内核的N32G031、N32G032、N32G003系列MCU，方便全球客户基于N32进行产品开发；</li><li>6月14日，IAR与国产领先高性能MCU厂商先楫半导体（HPMicro）共同宣布达成战略合作协议：IAR最新的Embedded Workbench for RISC-V版本将全面支持先楫HPM6000高性能RISC-V MCU系列，这是IAR首次支持高性能通用RISC-V MCU产品系列；</li><li>7月13日，IAR与业界领先的半导体器件供应商兆易创新（GigaDevice）联合宣布，最新发布的IAR Embedded Workbench for Arm 9.40版本已全面支持兆易创新基于Arm® Cortex®-M7内核的超高性能MCU微控制器——GD32H737/757/759系列，为开发人员提供高效的工具链；</li><li>7月26日，IAR宣布与国内专业RISC-V处理器IP及解决方案公司芯来科技达成战略合作：经 TÜV SÜD 认证的IAR Embedded Workbench for RISC-V功能安全版将全面支持芯来科技NA系列车规级处理器内核；</li><li>10月11日，IAR与普冉半导体宣布达成合作，全面支持普冉半导体32位Arm® Cortex® - M0+/M4系列微控制器；</li><li>10月18日，IAR宣布与中科芯达成生态合作，全面支持其CKS32系列MCU的应用开发。</li></ul><p>当AI飞临，就连IAR这样的工具链企业都加入到了战争之中，可见AI革命已经渐深，当边缘设备都具备了AI，我们的生活或许又会发生翻天覆地的新改变。</p><p><strong>参考文献</strong></p><p>[1] IAR爱亚系统：全新合作联盟：IAR与Edge Impulse联手为全球客户提供AI与ML整合功能.2023.10.26. https://mp.weixin.qq.com/s/0qrToqFVcX8tsfJVmSt-VA</p><p>[2] 柴火创客空间：人工智能的下一轮革命？关于TinyML的一切.2022.3.3. https://mp.weixin.qq.com/s/k2f20Ob8ZUu6NmN9ALduCQ</p><p>[3] strongerHuang：IAR支持中文了，keil还会远吗？.2023.7.4. https://mp.weixin.qq.com/s/z8mfxGD0lDWTOyUJUu2WbQ</p><p>[4] 麦克泰技术：IAR支持8500多种ARM芯片！.2023.5.23. https://mp.weixin.qq.com/s/a4JYZsxs-uDrdQOZT3QnQg</p><p>[5]&nbsp;嵌入式资讯精选：运用Edge Impulse实现MCU机器学习，试试吧~.2021.12.21. https://mp.weixin.qq.com/s/R7AX95bRUajizn39LP4XvQ</p><p>[6] IAR官网： https://www.iar.com/news/</p><p>[7] AI电堂：MCU巨头们都下场边缘AI了.2023.7.18. https://mp.weixin.qq.com/s/ToTgg9s-c3w4LGEa_LQydg</p><p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/sDRouZvlN7dLaZciV7vlfg" rel="noopener noreferrer nofollow" target="_blank">“电子工程世界”（ID:EEworldbbs）</a>，作者：王兆楠、付斌，36氪经授权发布。</p>
]]></content:encoded>
<pubDate>Fri, 27 Oct 2023 03:01:17 GMT</pubDate>
<pubDate>Fri, 27 Oct 2023 03:01:17 GMT</pubDate>
</item>
<item>
<title>路演预告｜人工智能专场路演第三期，谁将创生未来？</title>
<link>https://www.36kr.com/p/2490888200263811</link>
<guid>https://www.36kr.com/p/2490888200263811</guid>
<content:encoded><![CDATA[

<p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231026/v2_56b38ffd67864635bf7b0fab73c8da2a@5807859_oswg425440oswg1920oswg1080_img_jpg?x-oss-process=image/quality,q_100/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p class="img-desc">欢迎AI领域的创业者和投资人报名</p><h2><strong>活动背景</strong></h2><p>2023年，随着GPT的兴起和大模型的跨越式发展，人工智能进入2.0时代，人工智能产业迎来了新的发展浪潮。在这场技术风暴中，36氪也深入一线，梳理AI浪潮的关键节点、关注AI带来的实践与变革、搭建行业交流碰撞的平台。</p><p>为此，36氪联合36C共同举办<strong>每日路演-人工智能专场第三期</strong>，欢迎AI领域的创业者和投资人报名！</p><h2><strong>活动时间</strong></h2><p>11月2日（星期四）15:00 - 17:00</p><h2>活动详情</h2><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231026/v2_a68fb51732d1472982888f316b3101f3@5807859_oswg4814044oswg1688oswg8628_img_jpg?x-oss-process=image/quality,q_80/format,jpg/interlace,1" /></p><p class="img-desc">扫描二维码报名，进入创投交流群</p><h3><strong>欢迎对接优质在融项目</strong></h3><p>如果您对本次路演的项目感兴趣，希望可以对接到项目方，或者如果您手中有好项目需要融资对接更多投资人，欢迎与我们联系。</p><h2><strong>关于36C</strong></h2><p><strong>36C是专注数字科技领域的“创投+创孵”平台</strong>，总部位于杭州市西湖区，在深圳前海和香港设有分部和AI创新实验室，分为创业服务和创业投资两大业务。创业服务注重务实，助力创始人找人、找钱、找订单，打造创投社群陪伴创始人预判技术趋势、融入产业链上下游、拓展创业人脉。创业投资聚焦数字科技，重点关注AI、区块链、Web3与金融科技、3D引擎等数字技术或数字原生项目。</p><h2><strong>创变者俱乐部——企业⼀号位的顶级社群</strong></h2><p>创变者俱乐部是由36氪发起的产业社群联盟，已吸纳20000余位新经济企业的核心决策人和活跃在一线的优质投资人。通过36氪强大的商业整合能力，打造「产业+资本+创新」的价值生态，为中国高潜、创新企业决策者构建深度链接。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20230718/v2_0c96c1fb74854d94b8217993d24277ca@5807859_oswg520298oswg900oswg480_img_jpg?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p class="img-desc">欢迎扫码入群</p>
]]></content:encoded>
<pubDate>Fri, 27 Oct 2023 01:30:00 GMT</pubDate>
<pubDate>Fri, 27 Oct 2023 01:30:00 GMT</pubDate>
</item>
<item>
<title>GameGPT进军游戏制作，全自动生成游戏，时间可缩百倍</title>
<link>https://www.36kr.com/p/2491906953123720</link>
<guid>https://www.36kr.com/p/2491906953123720</guid>
<content:encoded><![CDATA[

<blockquote><p>GameGPT出世，多代理多线程完美再现游戏制作流程！</p></blockquote><p>不得了了！GPT技能树再成长，现在直接连游戏都能做了！？</p><p>要知道，现在这个时代，已经不是过去那个做个小游戏就可以抢占市场的时代了。如今的游戏开发流程超级复杂。</p><p>先说人力，每个游戏团队的人员都是数以几十甚至上百来记。有人负责编程，有人负责美工，有人负责维护，等等。</p><p>每个游戏还都有庞大的代码库、素材库。</p><p>结果就是，开发一款优秀的游戏大作，需要大量人员，投入大量时间才能完成。而这个时间周期，往往要长达数年。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_e630e4e08ac249f6a12ae625ce81d89f@1743780481_oswg1086289oswg1080oswg721_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>更直观的，就是钱。</p><p>游戏团队开发一款能让人们记住并且爱玩儿的大作，预算动不动就要超过1亿美元。</p><p>要不然怎么说，游戏制作算是一种用爱发电呢。</p><p>现在，情况有变！</p><p>有研究人员开发了一个叫GameGPT的模型，GameGPT可以整合多个AI智能体（agent）来自动完成游戏开发中的部分流程。</p><p>而不同的智能体各司其职，工作起来井井有条。</p><p>有智能体负责审查游戏的设计计划，并进行相应的修改和调整；有的负责将任务转化为具体的代码；有的负责对上一步生成的代码进行检查，对运行结果进行审核；还有智能体负责验证全部的工作是否符合初始预期。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_dfa31101fdd44bdd93da21e1faff7c8b@1743780481_oswg440009oswg654oswg597_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>如此这般，通过细化分解工作流程，GameGPT就可以简化AI智能体的工作。这种各司其职会更加有效率，实现起来也比一个全能型的智能体完成一切要简单得多。</p><p>研究人员表示，GameGPT可以简化传统游戏开发流程中一些重复和死板的内容，比如代码测试。</p><p>大量开发人员就可以从繁杂的检验工作中解放出来，专注于AI所不能替代的，更有挑战性的设计环节。</p><p>当然，这篇论文目前还处于一个比较初步的阶段。目前还没有任何具体的结果或者实验来证明性能上的提高。</p><p>换句话说，还没人用GameGPT真的开发过游戏，这个模型目前还处在概念形成阶段，在有具体的应用结果以及可量化的数据之前，咱也不好评估。</p><p>不过，总归是个努力的方向。</p><p>有网友表示，人们对LLM的想法是有一定偏差的。现在，研究人员有了一种能够100%解决NLP问题的工具，而人们却只关心如何实现某些工作流程的自动化。</p><p>举例来说，想象一下如果游戏世界对你的决定做出的反应，要比你在五分钟内判断出基于规则的硬编码引擎的反应更正常，那将会是怎样的情景。</p><p>再想象一下，如果一款游戏能根据你做出的决定（比如在路上随机屠杀你看到的敌人等），为你临时安排一些支线任务，那会是什么场景。</p><p>而开发者在创建这样一个系统时，会使用提示工程来指导LLM，而不是编码这些东西。</p><p>但是，这样做的目的不是为了节省成本，而是为了在以前无法制作更多游戏的阶段制作游戏（是不是有点拗口）。</p><h2>GameGPT</h2><p>首先，让我们来看看GameGPT模型的大框架——全流程。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_77221c3299ea43e9a9430561865a6bfa@1743780481_oswg158674oswg1080oswg439_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>可以看到，作者将每个智能体拟人化，更生动地展示了他们是如何各司其职的。</p><p>流程最左侧是用户端，向GameGPT输入prompt，然后开发经理和审核进行初步计划。</p><p>接着，再把需求发送给开发工程师，以及游戏引擎工程师，来执行具体的任务，生成代码。</p><p>最后检查一下有没有遗漏，有的话发回左侧，再跑一遍。没有就继续向右，由负责检查的工程师来进行testing。</p><h2>AI开发游戏？？</h2><p>实际上，AI开发游戏历史的雏形也许可以追溯到更早。</p><p>AI在游戏开发中的应用可以追溯到「星际争霸」和「暗黑破坏神」等经典游戏。在当时，开发人员需要用AI系统来制作交互式的虚拟世界和角色。</p><p>而这些系统已成为此类互动平台开发的标准配置。</p><p>早期和游戏开发AI相关的研究强调控制非玩家的角色（NPC），而随着自然语言处理（NLP）技术的发展，出现了一些利用深度学习技术生成关卡的开创性工作。</p><p>其中代表作是MarioGPT，它通过微调的GPT-2模型成功生成了「超级马里奥兄弟」中的部分关卡。</p><p>而众所周知，LLM又在今年取得了巨大进步，在NLP和计算机视觉（CV）领域都取得了不错的成绩。</p><p>我们知道，LLM的训练是一个多阶段的过程。初始阶段包括在广泛的语料库中训练这些模型，促进基本语言能力的获得。</p><p>随后就是更重要的阶段了，通过指令（instruction）生成各种NLP任务的数据对模型进行微调。这种指令调整，增强了模型在广泛应用中的泛化能力，从而可以让LLM能够在之前训练中没有执行过的任务中取得零误差的性能。</p><p>最后，人类反馈强化学习（RLHF）阶段保证了模型的结构完整性和可靠性。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_305597b0e48f469497575afee389bd95@1743780481_oswg373678oswg1080oswg540_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>这里还有一点需要注意——RLHF阶段能让模型生成模仿人类风格的内容，从而增强其作为智能体的多功能性。</p><p>此外，LLM的进步还促进了智能体在软件开发过程中的自动化。许多研究都曾经把目光放在过这个问题上——如何开发一个基于LLM的智能体，用来执行不同的任务。</p><p>比方说AutoGPT就曾经采用LLM智能体来处理现实世界中的某些决策任务，而HuggingGPT则采用的是单个LLM作为一种控制器，来协调完成更加复杂的AI任务。</p><p>虽说这些方法都依赖于唯一的LLM智能体，但它们都加入了一个审核者（就是上面流程图里的reviewer）来完善决策。</p><p>还是拿AutoGPT举例，模型会从监督学习器中获取一些辅助性的意见来提高自身性能，HuggingGPT也可以接入GPT-4，弄成一个reviewer，来评估决策的准确性。</p><p>还有一些别的例子，比方说MetaGPT就引入了一个多智能体框架，可以用于各种软件的自动化开发。</p><p>而回到我们今天讨论的游戏开发，我们要知道，与一般的软件开发不同，游戏开发行业的运作需要紧跟潮流，因此整个开发过程必须更加精确和简洁，以达到最佳效率。</p><p>此外，在没有幻觉和高精度的情况下，调整和使用单个LLM来服务于游戏开发的整个开发周期是不切实际的，而且成本高昂。</p><p>因此，游戏开发AI的框架需要多个reviewer参与，这样就能有效缓解语言模型所固有的幻觉倾向。</p><p>研究人员还发现，在游戏开发中，语言模型还有另一个局限性——冗余性。LLM在游戏生成时，可能会生成不必要的、无信息量的任务或代码片段。</p><p>为了有效解决幻觉和冗余问题，今天的主角——GameGPT战略性地采用了多种方法来解决这个问题，包括双重协作、通过内部词汇库进行指令调整以及代码的解耦。</p><p>值得我们关注的是，双重协作涉及到LLM与小型深度学习模型之间的互动，以及负责执行的智能体与reviewer智能体之间的协作参与。</p><p>研究人员表示，这些协同作用已被证明，在减轻GameGPT的幻觉和冗余方面是有效的。</p><h2>方法介绍</h2><p>接下来，研究人员从全流程剖析一下GameGPT的创新。</p><p>首先，在游戏设计阶段，在收到用户请求后，GameGPT的任务包括生成整个游戏的开发计划。这个计划阶段是关键步骤之一，极大地影响了整个开发过程的无缝进展。</p><p>这个阶段由基于LLM的游戏开发经理策划，先提出一个初始计划，随后分解成任务列表。</p><p>值得注意的是，由于LLM固有的局限性，这个初始计划经常会出现幻觉，从而产生意想不到的任务，包括没有信息或不必要的冗余任务。</p><p>为了应对这些问题，研究人员提出了四项可以减轻这些难题的策略，这四种策略相互正交的，并且可以分层执行以获得更好的效果。</p><p>方案一：对传入请求进行分类，目的是辨别游戏的类型。目前，GameGPT框架支持五种不同游戏类型的开发，即:动作、策略、角色扮演、模拟和冒险。</p><p>对于每种类型，研究人员都会提供标准化的计划模板，指导游戏开发经理智能体使用相关信息完成模板。</p><p>通过采用这种方法，冗余任务的频率显著降低，同时减少了幻觉发生的可能性。</p><p>策略二：涉及计划审查员智能体的参与，这是另一个基于LLM的代理。这个智能体通过精心设计的prompt进行操作，以此来对任务计划进行全面的审查。</p><p>它的主要目标是尽量减少幻觉和冗余的发生。该智能体评估计划并提供反馈，旨在改进并提高其准确性、效率和简洁性。</p><p>同时，这一部分生成的指令可以作为游戏开发经理智能体的新输入，使任务计划更加准确和完善。</p><p>策略三：通过专门的指令来调整游戏开发经理智能体的LLM本身，以便更好的进行游戏开发层面的规划。这个微调过程的目的就是让模型能生成一个既准确又简洁的计划。</p><p>为了方便起见，研究团队收集并整合了一个内部数据集，其中包括许多输入输出的搭配。虽然这些组合在长度或结构上不符合标准格式，但它们都围绕着游戏开发的要求。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_23335f929ba64753a1283f4a33f53e4e@1743780481_oswg530606oswg1080oswg608_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>这部分固定搭配由业内的开发人员提供。</p><p>通过采用这种方法，研究人员有效地弥合了LLM的一般语言能力与游戏开发规划能力之间的差距。</p><p>策略四：规划阶段的「安全网」。在整个计划过程中，游戏开发经理智能体始终在前端界面上与用户分享中期结果，使其余的智能体能够随时了解正在进行的开发是什么。</p><p>为了增强这一点，研究人员集成了一种交互式方法，使用户能够根据他们的期望积极地审查、纠正和增强计划。这种方法也保证了设计计划和用户需求之间的一致性。</p><p>说完了这些策略，我们来看看GameGPT的优越性。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_8a05aa219d814ffea11d6007fec9b26a@1743780481_oswg352159oswg1080oswg608_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>首先，这个模型中的任务分类过程要求在识别任务类型及其对应参数方面具有很高的准确性。</p><p>因此，研究人员为了确保这一阶段的准确性，创建了一个名为游戏开发工程师的智能体。该智能体由两个模型共同组成，它们协同参与任务分类的流程。</p><p>这种协作方法提高了任务识别的准确性和有效性。同时为了避免LLM幻觉的出现，提高任务分类的准确性，研究人员提供了游戏开发中可能出现的任务类型列表。</p><p>为了对此进行更好的分类，他们采用了BERT模型。</p><p>BERT模型已经用内部数据集进行了完整的训练。该数据集包含针对游戏开发任务所量身定制的各项数据条目。而输入则是从预定列表中绘制任务，而输出对应的则是任务的指定类别。</p><p>任务类型和参数的审阅都在这个阶段进行，引入一个叫做任务审阅人员的智能体，主要负责每个类别的识别和参数是否合理。</p><p>评审（review）的过程包括审核任务类型是否在预定范围内，是否是最适合的任务。同时，它还会检查参数列表，看看它是否与任务一致。</p><p>某些场景下，比如一些基于上下文任务信息的，或者用户请求无法推断参数的情况，GameGPT采用了一种主动的方法来解决。</p><p>Reviewer通过在前端界面上启动提示，并请求参数所需的附加信息来吸引用户注意。</p><p>这种交互方法的好处在于，即使在自动推理不足的情况下也能确保论证细节的完整性。</p><p>此外，还有另一个智能体负责识别任务之间的依赖关系，并构造一个封装这些关系的图表。在建立该图之后，再采用算法来对该图进行遍历筛选，由此产生一个确定的任务执行顺序。</p><p>这个过程确保了模型可以按照任务的依赖关系有序和系统地执行，从而产生连贯和结构化的开发流程。</p><p>另一个问题是，使用LLM生成冗长的代码会带来更大的幻觉和出现冗余的风险。为了解决这个问题，研究人员引入了一种新的方法来解耦游戏设计中出现的代码，简化了LLM的推理过程，从而极大程度减轻了幻觉和冗余。</p><p>这个方法也并不难理解——研究人员会将预期的脚本划分为许多长度更短的代码片段，以供LLM处理。这种解耦方法大大简化了LLM的工作。</p><p>还有一种叫做上下文学习的有效推理方法，也可以有效地减轻幻觉。</p><p>此外，GameGPT中应用的另一种消除幻觉的技术，包括为每个任务生成一组K个代码的代码片段。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_8e4d412f51904b7aa10c866c9fe6aea1@1743780481_oswg585867oswg1080oswg608_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>这些代码片段随后会在虚拟环境中进行测试，并同时呈现给用户。测试过程和用户反馈都被用来识别和消除有问题的代码片段，最终只留下最可行的选项来执行。这种方法同样有助于进一步减少幻觉的发生。</p><p>此外，研究人员还有一个内部的库，包含为游戏开发设计的大量代码片段。每一个代码片段都由标签器进行了注释，提供了明确说明其预期目的的说明。</p><p>概括一下就是，为了让代码不冗余，不幻觉，开发人员做了两手准备，事前的和事中的。</p><p>同时，上面提到的这个库也是对模型进行微调的宝贵资源。代码审查和改进在游戏引擎智能体生成代码之后，代码审查智能体会对代码库进行彻底的审查和检查。</p><p>该智能体会进行全面的评估，努力找出任何可能会偏离原始请求的实例，或代码中出现的意外幻觉。</p><p>经过彻底的审查，智能体不仅能标记出潜在的差异，而且还能据此提供改进代码的建议，最终产生更为合理的版本。</p><p>在审查过程之后，修改后的代码以及智能体的反馈都将通过前端界面与游戏引擎工程师智能体和用户共享。如果用户认为有必要，可以直接通过前端界面提供代码修改建议。</p><p>之后这些建议会继续传递给代码审查智能体，它会进行评估，并有选择性的合并这些建议，从而进一步生成一种协作和迭代的方法来增强代码。</p><p>最后，一旦代码生成完毕，该干的也都干完了，责任就落到了游戏引擎测试智能体的身上，由这个智能体来负责执行生成的代码。</p><p>在这一阶段，该智能体还会遵循在前一阶段所制定的执行顺序。</p><p>具体的执行过程包括将每个单独任务的代码发送到游戏引擎，进行执行，并在执行期间持续跟踪，生成日志。</p><p>在完成执行序列中指定的所有任务后，智能体会合并整个执行过程中生成的所有日志。</p><p>最终，这种编译生成了一个简洁而全面的摘要，再通过前端界面呈现给用户。</p><p>此外，测试工程师智能体还会识别并报告在执行过程中观察到的任何回溯情况的出现。这些回溯会作为关键的指示器，指示AI对执行流程或代码进行更进一步的调整，使整个过程得以细化，并有助于生成一个完美的最终产品。</p><p>最后，再来看下多个代理同时工作的框架公式：</p><p>首先，在GameGPT中，每个代理都有一个私有的记忆系统，并且它们可以访问共享的公共内容，以获取必要的信息来指导其决策过程。</p><p>对于时间步长为t的代理i来说，这一过程可表示为：</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231027/v2_8394f8aae1c547beba6041167abdb01e@1743780481_oswg2939oswg116oswg25_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>其中pθi对应的是和代理i相关的LLM或专家模型，Oit表示代理i在时间步长为t时的产出或可交付成果，Mit和Pt分别指截至时间步长t内，所有的私人记忆和必要的公共记录。</p><p>由于游戏开发行业的特殊性和大语言模型的局限性，在GameGPT中，具有不同角色的多个代理的存在至关重要。</p><p>鉴于游戏开发周期通常长达数月，如果只依赖一个拥有全面记忆和上下文信息的单个代理，语言模型（包括LLM）的效率将大打折扣。</p><p>而随着时间的推移，项目变得越来越复杂，这种方法也会带来可扩展性方面的挑战。此外，考虑到LLM所处理的标记数量的限制，在大型游戏开发项目中使用具有全面内存的单独代理并不实用。</p><p>还有，在LLMs中观察到的幻觉和冗余等固有问题凸显了多个代理之间协作的重要性，尤其是那些具有批判性角色的代理。</p><p>这种协作对于减轻LLM幻觉和冗余带来的挑战意义重大。</p><p>因此，GameGPT才利用一系列不同的角色来促进其运作，包括整个游戏开发周期的职责。</p><p>这些角色包括上文提到的游戏内容设计师、游戏开发经理、计划审核员、游戏开发工程师、任务审核员，还有游戏引擎工程师、代码审核员和游戏引擎测试工程师。</p><p>在整个游戏开发过程中，每个角色都承担着不同的任务。</p><h3>参考资料：</h3><p>https://arxiv.org/pdf/2310.08067.pdf</p><p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/i4pr5250EYTsKvEm4VC3YA" rel="noopener noreferrer nofollow" target="_blank">“新智元”（ID:AI_era）</a>，作者：拉燕，36氪经授权发布。</p>
]]></content:encoded>
<pubDate>Fri, 27 Oct 2023 00:52:11 GMT</pubDate>
<pubDate>Fri, 27 Oct 2023 00:52:11 GMT</pubDate>
</item>
<item>
<title>AI持续向端侧逼近，多家芯片巨头“盯上”AI PC市场</title>
<link>https://www.36kr.com/p/2491889177646978</link>
<guid>https://www.36kr.com/p/2491889177646978</guid>
<content:encoded><![CDATA[

<blockquote><p>AI和机器学习算法增强的软件功能不断攀升，AI PC的到来被视为PC产业的转折点，近期多家主流计算机芯片厂商宣布其支持发展AI PC产业的布局；软件领导力或许才是AI PC体验的关键，业内人士表示，生成式人工智能爆发，为软件及操作系统下半场开启了无限的创新空间。</p></blockquote><p>随着行业对AI大模型研发和商业化的持续探讨与打磨，大模型向端侧的发展呼之欲出。近期，国内外多家主流厂商宣布其在AI PC产业当中的举措和布局。</p><p>三季度PC行业逐步回暖，机构预计，<strong>搭载生成式AI的个人电脑将成为行业发展分水岭，AI PC有望在明年进一步提升市场客单价</strong>。另外业内人士认为，部署AI功能的PC操作系统带来全新交互模式，或将激发新的市场需求，同时生成式人工智能也为软件及操作系统应用，开启创新空间。</p><h2>多家芯片巨头“盯上”PC端侧AI应用</h2><p>AI和机器学习算法增强的软件功能不断攀升，因此AI PC的到来被视为PC产业的转折点。《科创板日报》记者关注到，<strong>近期多家主流计算机芯片厂商宣布其支持发展AI PC产业的布局。</strong></p><p>英特尔近日宣布正式启动“AI PC加速计划”，将在2025年前为超过100万台PC带来人工智能特性，并由计划于12月14日发布的英特尔酷睿Ultra处理器率先推动。</p><p>据英特尔方面介绍，上述计划旨在联结独立硬件供应商和独立软件供应商，并充分利用英特尔在AI工具链、协作共创、硬件、设计资源、技术经验和共同推广的市场机会等资源，充分发挥相关硬件优势，以尽可能最大限度发挥AI和机器学习应用的性能，吸引更广泛的PC产业伙伴融合到AI PC生态系统的解决方案中。</p><p>无独有偶，高通在高通骁龙峰会期间发布了用于个人电脑（PC）和笔记本电脑的X Elite芯片。高通高管表示，搭载X Elite芯片的笔记本电脑，将从明年开始上市，这款芯片经过重新设计，可以更好地处理总结电子邮件、编写文本和生成图像等AI任务。</p><p>据介绍，在AI处理方面，骁龙X Elite专为AI打造，支持在终端侧运行超过130亿参数的生成式AI模型，AI处理速度是竞品的4.5倍，在某些任务上比苹果的M2 Max芯片更快，而且比苹果和英特尔的PC芯片更节能。</p><p>AMD近期正式面向的Windows平台，在Ryzen 7040系列PC处理器中配备了基于Xilinx IP的专用AI引擎，名为“Ryzen AI”，可加速PyTorch和TensorFlow等机器学习框架的运行。据了解，该引擎可以处理最多4个并发AI流，并处理INT8和bfloat16指令。AMD称该引擎比苹果M2处理器的神经引擎更快。</p><p><strong>PC整机厂商亦加快推进其品牌AI PC面市</strong>。联想集团董事长兼CEO杨元庆日前在联想创新科技大会Tech World上，向全球首次展示了AI PC，并表示“个人电脑迎来全新的朝阳”。杨元庆称，联想AI PC计划将在明年9月上市，</p><p>据介绍，在更好地了解用户的基础上，AI PC能够创建个性化的本地知识库，通过模型压缩技术运行个人大模型，实现AI自然交互；AI PC可作为每个人量身定制的全新智能生产力工具，将进一步提高生产力、简化工作流程，并保护个人隐私数据安全。</p><p>IDC此前表示，虽然案例尚未完全明确，但市场对该类别的兴趣已经很强。“AI PC能够在更深层次上个性化用户体验，同时能够保护数据隐私。随着明年大量AI PC的推出，预计整体销售价格将大幅提升。”</p><h2>AI开启PC软件及操作系统新的创新空间</h2><p>算力是生成式AI和大模型的底层支持，但英特尔方面在介绍其AI PC加速计划时表示，<strong>软件领导力或许才是AI PC体验的关键</strong>。</p><p>Canalys指出，除了芯片大厂的积极推动外，新版操作系统在AI功能上的增强，也将同步促使AI PC产品的推出和总量，自明年起加速增长。</p><p><strong>业内人士向《科创板日报》记者表示，操作系统作为数字时代的基石，为所有计算机软件提供了运行与支撑平台。“大模型作为软件的一种，本身也运行在操作系统之上；未来AI将是操作系统的基础能力之一，通用大模型通过操作系统探索更多应用场景”。</strong></p><p>微软推出Windows Copilot，使Windows 11成为第一个宣布集中式AI协助的PC平台。Copilot作为基于GPT-4的人工智能助手，可以帮助用户完成各种任务，包括文档编写、代码编写、图片设计等，还可以根据用户的输入提供建议，并帮助用户纠正错误。</p><p>Canalys预测，随着x86架构持续提升PC AI能力，预计从2024年上半年开始将出现新一轮的AI赋能模型浪潮，到2024年第四季度，出货量预计上升至约2000万台的水平，在全球个人电脑出货量的占比超过25%。</p><p>同时鉴于微软将会在2024年末推出新一代Windows操作系统，并预期将发布AI升级功能，以及具备AI工具在商业和生产力软件的广泛应用，因此兼容AI的个人电脑市场，有望在2025年和2026年实现爆发式增长。</p><p><strong>值得关注的是，国产PC操作系统也在结合产业动态，接入大模型能力并形成探索生产力转化。</strong></p><p>统信软件运营的深度社区日前正式官宣，deepin成为首个接入大模型的开源操作系统，并发布UOS AI。据介绍，统信UOS不仅正在从底层XPU驱动、运行时优化、AI框架支撑等方面赋能AI，另外也已与众多大模型合作伙伴一起，将大模型融合进操作系统之中。接下来统信软件还将探索大模型与AI原生应用，自然语言交互兼容性、数据安全性等多个技术点，打造下一代操作系统与创新生态。</p><p>“微软在现任董事长带领下，战略主要转向了云端，在考虑把操作系统、云和AI都集成在一起，打造成一个通用的系列产品。”统信软件高级副总经理、CTO张磊向《科创板日报》记者表示，不过用云部署AI能力目前面临成本、安全性等问题，统信UOS AI则希望同时探索云、端部署。</p><p>端侧大模型整体面临芯片算力和生态部署等挑战。“英特尔、英伟达等多家芯片厂商都在宣称，自己的芯片推出了新的支持AIGC的能力，当前他们其实也有一定诉求，希望能够推出新的产品满足市场，而端侧大模型和软件也需要能力更强的计算设备支持。”张磊表示，芯片和软件商在商业落地和技术合作方面，是一种双向奔赴的关系。总体而言，“生成式人工智能爆发，为软件及操作系统下半场开启了无限的创新空间。”</p><p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s/uDRp04SoES9lX2_naEMD0A" rel="noopener noreferrer nofollow" target="_blank">“科创板日报”（ID:chinastarmarket）</a>，作者：郭辉，36氪经授权发布。</p>
]]></content:encoded>
<pubDate>Fri, 27 Oct 2023 00:43:06 GMT</pubDate>
<pubDate>Fri, 27 Oct 2023 00:43:06 GMT</pubDate>
</item>
<item>
<title>竞逐文生图大模型，百度、快手、网易“明争暗斗”</title>
<link>https://www.36kr.com/p/2491343880886150</link>
<guid>https://www.36kr.com/p/2491343880886150</guid>
<content:encoded><![CDATA[

<p>自从进入2023年以来，AIGC技术助推了新的人工智能浪潮，AI大模型的创新应用也按下了加速键。随着AI写作、AI作曲成功在多个领域落地，AI在内容创作方面的应用也变得越来越广泛，而AI绘画作为大模型最火热的应用领域之一，近几年也取得了突破性进展。</p><p>AI绘画简单来说就是“文生图”，是指输入一些描述性语言，AI可以以此生成创意画作。AIGC 技术的飞速发展使得“文生图”模型不断实现更加良好的生成效果，得益于此，无论是百度、网易这样的互联网大厂，还是快手这样的新锐公司纷纷争相入局，试图借助“文生图”这一新事物，探索业务上的更多新可能。</p><h2>快手“出其不意”</h2><p>前不久有消息称，快手在推出“文生文”大语言模型“快意”（KwaiYii）之后，又在“文生图”赛道取得了新的进展，推出了自研大模型“可图”（Kolors），并且已在公司内部全面开启测试。据介绍，可图大模型能够基于开放式文本生成各类的绘画作品，它有着三大突出特点：强大的文本理解、丰富的细节刻画，以及多样的风格转化。而在可图大模型强大的图像生成能力背后，则与快手多年的积淀息息相关。</p><p><strong>首先，快手海量的短视频素材，能为可图提供数十亿的图文训练数据。</strong>发展至今，快手上的短视频素材已经数以万计，根据这些短视频，可图可以收集到更多的数据信息，帮助大模型准确理解用户的需求，让用户通过简单描述即可生成更加多样化风格的图片。据了解，快手拥有数十亿来自开源社区和自研AI技术合成的图文训练数据，这些数据覆盖了常见的三千万中文实体概念，能更好地生成更加贴近文字描述的图片。</p><p><strong>其次，快手较强的用户粘性，为可图的落地提供了最佳的应用场景。</strong>众所周知，快手应用的累计互关用户对数超过311亿对，同比增长近50%，日均互动（包括点赞、评论和转发等）总量达80亿次，而且AI玩评也能够极大地提升用户参与评论的积极性和满意度。不同用户画像可以丰富可图训练数据，促使可图生成更多样化的图片。因此，拥有较强用户粘性的短视频评论区，可以看做可图大模型最佳的落地应用场景之一。</p><p><strong>最后，快手在大模型上的创新性探索，有助于可图形成差异化优势。</strong>快手研发了一个强大的中文CLIP模型，并且利用自研的中文LLM加上融合CLIP的图文特征作为文生图的文本理解模块，能让可图大模型更好地理解中文特色概念。不仅如此，快手还更改了去噪算法的底层公式和加噪公式，实现了单一基座模型在主体完整的前提下，可生成具有丰富细节和纹理的图片。而可图大模型也具有了基于Prompt的自动学习模型，能够生成不同的风格模版。</p><h2>百度“声东击西”</h2><p>在文生图领域，快手的自研大模型“可图”可谓是独具一格，作为国内领先的AI技术公司，百度的AI作画产品“文心一格”自然也备受期待。据了解，文心一格是基于百度文心大模型能力的AI艺术和创意辅助平台，它可以根据用户输入的文本描述和选择的风格，自动生成独一无二的画作。而百度文心一格之所以能对用户的作画需求实现精准理解，其中的原因自然不言而喻。</p><p><strong>一来，文心大模型强大的语言理解能力，使文心一格对中文的理解变得更加精准。</strong>文生图技术对中文语义的理解尤为关键，而文心一格的技术基础是百度文心知识增强跨模态理解大模型，百度文心学习了海量优质图文数据，能全面提升图像生成质量和语义一致性。因此，文心一格不仅能利用知识辅助更好地理解用户的输入，并自动丰富语义细节，有效降低用户输入描述成本，还能根据不同的需求，灵活适配多种风格画作生成能力。</p><p><strong>二来，文心大模型在技术上的深厚积淀，在一定程度上解决了文心一格在实际应用中的技术难题。</strong>众所周知，文心一格AI作图产品顺利落地，是百度依托于飞桨、文心大模型持续进行技术创新的结果。而百度的文心跨模态大模型ERNIE-ViLG 2.0是全球首个知识增强的AI作画大模型，也是目前全球参数规模最大的AI作画大模型。百度在训练大模型方面取得了长足的进步，也让文心一格有效解决了复杂概念、属性混淆等文生图领域的常见问题。</p><p><strong>三来，文心大模型丰富的产业应用场景，有助于文心一格实现商业化快速落地。</strong>目前，文心大模型已大规模应用于搜索、信息流、智能音箱等互联网产品，并已通过飞桨开源开放平台、百度智能云等赋能工业、能源、金融、通信、媒体、教育等各行各业。而在这个基础模型职场，文心一格也能结合各个领域的、少量的任务数据，再进行训练、调优，之后就可以适用更多场景，从而进一步拓宽落地的广度，加深产业应用的深度。</p><h2>网易“蓄谋已久”</h2><p>无论是新晋独角兽快手，还是老牌互联网大厂百度都相继进入了AI绘画领域，互联网科技公司网易自然不会落后。事实上，网易很早就对“文生图”领域有所研究。网易旗下专业从事游戏与AI研究和应用的顶尖机构网易伏羲，就自研了文生图模型——“丹青”。而网易丹青之所以能生成让用户满意的图片，自然也与其独一无二的优势息息相关。</p><p><strong>一是，网易伏羲对中文和美的理解深刻，有助于丹青生成更高质量的图片。</strong>生产好的内容之前，需要先理解好的内容。丹青模型基于原生中文语料数据及网易自有高质量图片数据训练，不仅对中文的理解能力更强，对中华传统美食、成语、俗语、诗句的理解和生成也更为准确。比如，与其他文生图模型相比，丹青模型更容易听懂用户的意思，在丹青生成的图片中，鱼香肉丝没有鱼，红烧狮子头也没有狮子，生成的图片效果用户满意度更高。</p><p><strong>二是，网易伏羲顶尖艺术家的真实反馈，使得丹青创作出的作品更能满足中式审美。</strong>网易会请一些美术专家对模型进行把控，让其从艺术的角度对生成图片效果、插件、版本给予专业意见，丹青则会及时根据艺术家们的反馈意见，进一步迭代优化。比如，依赖于较强的中文理解能力，以及对美学的专业理解，丹青模型生成的图片更具东方美学，既能生成“飞流直下三千尺”的水墨画，也能生成符合东方审美的古典美人。</p><p><strong>三是，网易伏羲对文生图的多年研究，能为丹青的快速落地和推广提供助力。</strong>事实上，网易伏羲对文生图的研究起步较早，在Stable Diffusion还没开源之前，就已经在不断地投入，到现在已经有了很多积累。据了解，网易伏羲团队已在世界顶级学术会议发表论文200余篇，申请发明专利550余项。不仅如此，网易伏羲还根据实际应用效果不断对文生图模型进行迭代优化，以便将其更好更快地应用于实际场景中。</p><h2>前路“危机四伏”</h2><p>随着快手、百度、网易等玩家的文生图大模型相继亮相，国内外发布文生图模型的数量也在不断攀升，模型生成效果和效率也在逐渐迈上新的台阶，文生图模型商业化落地指日可待。只不过，在此之前，文生图领域仍有些问题不容忽视。</p><p><strong>一方面，文生图尚处于探索时期，生成细节还不够完美。</strong>虽然文生图具有一些创新性和实用性，但是不能全面理解用户的语义，生成的图像质量自然也就不会很理想，不是人物的脸部或手部细节呈现得不够完美，就是图像与文本的相关性不够紧密，甚至会出现一些毫无逻辑的图像和文本的组合。显然，AI绘画在语义理解、宏观结构、细节刻画、逻辑推理等方面还有较大改进空间。</p><p><strong>另一方面，文生图只是AI辅助创作，生成内容缺乏创造力和情感表达。</strong>毫无疑问，技术是标准化的，审美却是非标准化的。设计师、画师可以借助AI，提高自己的创作效率，甚至激发无穷的想象力，但AI并不是设计师、画师本人，不能拥有人的情感和灵感，不能和人一样感同身受，而且目前的AI技术对外界生活无法感触，对真实世界的很多需求自然也是无法精准捕捉和理解，所以短期内，AI绘画还是很难代替设计师、画师的。</p><p>除此之外，国内外正接连涌现出新的文生图公司，AI绘画领域的竞争也将进一步加剧。事实上，除了快手、百度、网易伏羲等走上中国式文生图的道路之外，国内其他加码AI绘画的玩家也都正源源不断地赶来，国外文生图应用的景象也是十分热闹。而国内外每一家模型结构都不是完全一样的，无论是图片还是文本都做了优化，且都包含着自己的特色，所以文生图领域的竞争局面可想而知。</p><p>尽管文生图大模型目前尚有一些缺陷，但回顾人类的发展历史不难发现，一项新技术的出现，往往需要不断改进和完善，因此对于AI绘画，我们仍然抱有很多期待。而在流量红利逐渐消退的当下，百度、网易、快手等企业主动去拥抱“文生图”这样的新技术或许是最好的选择。只不过，最后这些入局者能做到何种程度，或许只有时间能给出答案。</p><p>本文来自微信公众号<a href="http://mp.weixin.qq.com/s?__biz=MzAxNTM3MTUxOA==&amp;mid=2650860918&amp;idx=2&amp;sn=afdaea3ec79fa9cc3ed2e14244339dc9&amp;chksm=807150e7b706d9f10722e201cb2ad5a818e184ec0b233a62c47092009ece997fd7e61c418c46&amp;scene=0&amp;xtrack=1#rd" rel="noopener noreferrer nofollow" target="_blank">“刘旷”（ID：liukuang110）</a>，作者：刘旷公众号，36氪经授权发布。</p>
]]></content:encoded>
<pubDate>Thu, 26 Oct 2023 23:53:19 GMT</pubDate>
<pubDate>Thu, 26 Oct 2023 23:53:19 GMT</pubDate>
</item>
<item>
<title>如果AI可以看病问诊，出错谁来负责</title>
<link>https://www.36kr.com/p/2491146157725571</link>
<guid>https://www.36kr.com/p/2491146157725571</guid>
<content:encoded><![CDATA[

<p>美国4岁儿童亚历克斯病了，三年来，他先后看了17名医生，从儿科、牙科、骨科等门诊科室到各路专家，没有一位医生准确地诊断出他的病因。直到2023年ChatGPT火起来后，他的母亲向ChatGPT求助。</p><p>“我一行一行地查看亚历克斯的核磁共振记录中的所有内容，并将其输入ChatGPT”，他的母亲说，ChatGPT的诊断结果是“脊髓栓系综合征”。带着这一诊断结果，他和母亲拜访了一名新的神经外科医生，这位神外医生看了一眼MRI（核磁共振成像）就给出了和ChatGPT一样的结论，并指出了栓系的具体位置。</p><p>亚历克斯的案例无疑是企业蜂拥进入AI医疗领域的动力。<strong>2023年10月24日，科大讯飞发布讯飞星火医疗大模型，据《财经∙大健康》不完全统计，这已经是2023年国内发布的第32个医疗领域生成式AI大模型。</strong></p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231026/v2_29e7894539bd4485b63d1fc12e4b60be@5091053_oswg258139oswg1080oswg2771_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>产品密集发布背后，企业开始寻找商业化出路，最挠头的问题莫过于，产品能卖给谁？</p><p><strong>公立医院是产业界眼中最优的买单方。“可现在大家落地面临的共同难题是，公立医院系统缺乏引进购买的动力。”</strong>一家AI医疗大模型企业人士告诉《财经∙大健康》，“一方面，新生事物没纳入公立医院的考核体系，不像买其他软件可以给医院加分；另一方面，医院内部的数据大多还没打通，这对于大模型发挥最大效能会打折扣。”</p><p>如同难以破解的魔咒，AI医疗过去十数年在商业化之路上的难题——数据壁垒和缺乏支付方，仍然摆在那里。</p><p><strong>“即便AI在医疗领域的渗透已经是确定性的趋势，有着广大的前景，但当下中国的AI医疗仍处于低谷期，要突破这个局面，关键还是看谁能掌握数据。”</strong>高特佳投资副总经理于建林说。</p><h2><strong>No.1 如何让医疗AI为自己的答案负责</strong></h2><p>随着生成式AI大模型的热潮掀起，我们距离AI医生更近了。</p><p>近半年医疗 AI 大模型持续推出。10月24日，APP“讯飞晓医”面向普通人群开放，可以提供预问诊、体检报告分析等。<strong>早在5月，春雨医生在线问诊产品“春雨慧问”中嵌入AI大模型，将在线问诊原本的“人——人”模式，升级为“人——机器——人”。</strong></p><p>AI大模型是指一个庞大复杂的神经网络，需要通过存储更多的参数来增加模型的深度和宽度，从而提高模型的表现能力，参数从百亿起步，对大量数据进行训练并产生高质量的预测结果。</p><p>公开数据显示，互联网搜索内容中有20%与医疗健康相关。但这里有一个问题，那就是<strong>如果AI不能对自己的答案负责，那么就不能走通商业模式</strong>。</p><p>目前能为医疗诊断负责的只有医生。一位原互联网医疗资深从业者分析，<strong>做医疗业务，最关键的在于责任和风险，要考虑合规的问题，谁能担责任，谁才可能挣到钱。</strong></p><p>尽管像亚历克斯的疾病诊断一样，只要症状描述的足够准确、充分，AI能给出正确的诊断结果，但ChatGPT对诊断结果不负责。</p><p>问题来了，如果医生诊断出了错，可以被谅解，因为人都是会犯错的，机器出了错怎么办？这些疑问，目前无论是法律还是伦理都还没理顺。</p><p>所以，<strong>AI大模型产品要想走商业化道路，就得先从医院的医生入手，想办法成为医生的助手。</strong></p><p>2023年5月，一家三级医院的主任医师试用了他的“新助手”，他为病人诊疗的时候，“助手”会录音，会帮他写问诊病例。</p><p>这款基于ChatGPT这样的大语言模型开发的问诊录音机器人，“已进入十多家医院的门诊，帮助医生节省时间。”北京左医科技有限公司首席执行官张超介绍。</p><p>上述主任医师给他的“助手”评分时，打了90分。<strong>AI助手丢掉的10分，是因为工作中存在一些误差</strong>，比如医生没有询问患者的月经史、婚姻史、生育史，但最终的问诊记录中包含了这一项，必须得医生手动删除。</p><p>如何嵌入医院诊疗环节中，是企业努力想实现的产品。9月，百度集团推出“灵医”大模型，想解决的是患者“排队一上午，看诊五分钟”的困扰，<strong>“灵医”的一个长项就是帮助医院分诊台的医护人员，为医生精准匹配患者，让医生和患者的每一次面诊效率更高</strong>。“灵医”大模型已向200多家医疗机构开放体验，除了26个互联网医疗平台，还有数十家公立医院。</p><p><strong>“预问诊”，也可以节省病人和医生的时间。</strong>商汤科技相关负责人向《财经∙大健康》介绍，其与上海新华医院合作面向挂号患者的“预问诊”模块，即将引入其AI大模型“大医”，患者看医生前可以先和AI沟通。</p><p>大模型的开发者们可以说是挖空心思，试图包围医院的方方面面。问题是，医院需要吗？</p><h2><strong>No.2 医生需要AI，但和你预设的不一样</strong></h2><p>“差不多20年前，就有搜索企业希望本地搜索的功能进入医院场景，当时医院的信息繁杂，没有很好的梳理，但后来发现，医院的基础设施不足以支撑，更重要的是医院也没有搜索信息的需求。”上述互联网医疗资深从业者对《财经·大健康》分析。</p><p>过去20年，今天的AI医疗大模型产品，在上述从业者看来面对情况还是一样。</p><p>进医院难，这是销售人员都明白的。<strong>一是和既往利益者的竞争，</strong>比如一家新的护工公司想进医院，医院有需求，但是医院原来有合作方，医院负责人就要对比谁的服务好、谁的品牌大，甚至谁的关系好。<strong>二是，医疗AI是不是医院必需的助手，</strong>如果不是，医院就没有让它们落地的动力。</p><p>随着信息系统的升级，不少医院已经有了临床决策支持系统（CDSS），当医生输入病人主诉症状之后，系统就会自动提示可能的疾病，下一步用药建议。</p><p><strong>“如果大模型产品只是给医生提供诊断的线索，那本质仍然是一款辅助的搜索工具，只不过是从知识搜索升级为经验搜索。”</strong>在上述互联网医疗从业者来，医生日常工作大部分是诊治常见病，可能不需要一个多聪明的机器人替他看病，需要的是一个笨一点的、准确性高的助手，“这体现不出大模型最有价值的地方”。</p><p>给升级版的“搜索”付钱，院长们思量就多了。</p><p>参照ChatGPT的商业模式，通过广告、会员订阅和算力来增加收入，显然在医疗领域行不通，中国公立医院占据了大半江山，它们不大乐意支付广告费，会员和算力这也不再医院管理者的考量之内。</p><p><strong>想打动院长真金白银地购买，首先得是有临床价值的辅助诊断工具。</strong>于建林指出，评估一个AI大模型的真正价值，无非是考虑算力、算法和数据，而在中国的医疗领域数据最为重要。</p><p>训练一个有临床应用价值的AI模型产品，至少需要数万的临床病例数据，但在于建林接触的中国AI企业中，能有几千的病例数据就非常不错了。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231026/v2_6c55299ae2d94e0689a53c7dbbdd02be@5091053_oswg127629oswg640oswg428_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p class="img-desc">图/Pixabay</p><p>大模型通过使用大量的模型来训练数据，于是，<strong>“书本”训练，成为医疗大模型的基础训练，“养料”来自海量的医学教科书、行业指南。</strong>科大讯飞相关负责人介绍，通过与人民卫生出版社、中华医学会杂志社、科学技术文献出版社等深度合作，获取了众多的医学书籍、临床指南、医学文献、典型病例等权威医学知识，扩充了模型的专业知识覆盖度，极大地提升了模型的理解和咨询回答的能力。</p><p>京东的医疗大模型“京医千询”，则是通过收集超亿级的医患对话数据，这来自线上问诊建起的一个大数据库，覆盖了线上140余个科室的医生、药师、营养师和心理医生。</p><p>"线上问诊数据和线下医疗数据在质量上还是有一定差距。"于建林分析，短期来看，<strong>三五年内，一些在医学领域原本就有优势的企业，比如大型医疗设备企业，因为有大量的医院业务，它们获得数据的优势明显，AI搭配硬件去销售，更容易走通。</strong></p><p>国际上一些成熟的AI辅助诊断产品，已经可以大规模的临床应用。但商业动力强，才是推动AI落地的阀门。比如一款AI结合的癌症早筛产品，就是保险公司希望能够更早的确诊，以减少理赔的成本，所以有动力去推动，于是保险公司提供了高质量的临床数据支持这一款产品开发。</p><p>保险付费是国际上已经走通的一条路，因为可以降低成本，保险公司乐意做。<strong>只是中国的商业健康保险发展尚不充分，不足以支撑成为强大的付费方。</strong></p><p>于建林对《财经∙大健康》分析，目前AI在医疗应用主要两方面，一个场景是对患者，提供问诊和健康管理，另一场景是帮助医生来做AI的辅助诊断，比如AI影像。</p><p>路怎么趟顺了，还得中国的AI开发者们沉入医院继续研究。2023年9月24日，OpenAI创始人兼CEO Sam Altman表示，如果有公司致力于解决GPT模型的一个小缺陷，不会产生可持续的竞争优势。他的建议是，AI创业方向包括AI医疗，“优秀的AI医疗顾问将为社会带来巨大福祉”。</p><p>本文来自微信公众号<a href="https://mp.weixin.qq.com/s/uFLg1LVFWK0TsvL8uMl--Q" rel="noopener noreferrer nofollow" target="_blank">“财经大健康”（ID:CaijingHealth）</a>，作者：辛颖&nbsp;周云琨，36氪经授权发布。</p>
]]></content:encoded>
<pubDate>Thu, 26 Oct 2023 11:34:04 GMT</pubDate>
<pubDate>Thu, 26 Oct 2023 11:34:04 GMT</pubDate>
</item>
<item>
<title>AI大模型将创造出新岗位：人工审核员</title>
<link>https://www.36kr.com/p/2491053520609154</link>
<guid>https://www.36kr.com/p/2491053520609154</guid>
<content:encoded><![CDATA[

<p>AI代替现有岗位的可能性，已经让很多白领开始觉得不安。</p><p>但实际上，一项新技术之所以有价值，不是因为它要砸掉你的饭碗，而是它将为社会带来更多的机会。</p><p>比如，目前最令人瞩目的技术---AI大模型，就会创造出一种新岗位：人工审核员。</p><h2><strong>一、AI通用大模型</strong></h2><p>随着人工智能技术的不断进步，AI大模型已经成为了当前最热门的领域之一。AI大模型是指参数量巨大的深度学习模型，通常包括千亿到万亿级别的参数量，能够处理海量的数据，并具备强大的泛化能力。目前，AI大模型已经在自然语言处理、计算机视觉、语音识别等领域取得了显著的成果。</p><p>目前，AI大模型的发展已经取得了显著的成果。其中，最具代表性的模型是GPT系列模型，包括GPT-3、GPT-4等。这些模型拥有强大的自然语言处理能力，可以生成高质量的自然语言文本，并且在多个自然语言处理任务中取得了最佳成绩。除此之外，BERT、T5等模型也在自然语言处理领域得到了广泛应用。</p><p>通用AI大模型需要海量的数据，以及巨量的算力做支撑。前者需要历史积累，后者需要资金投入，在全世界能同时拥有这两个资源的企业，寥寥无几。</p><p>在美国，微软公司和谷歌公司正在争夺通用AI大模型市场的领先地位。</p><p>在中国，百度的文心4.0已经代表了通用AI大模型的最高水平。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231026/v2_8a61a53803e14d929fc7c32ff32194d3@399037_oswg844433oswg590oswg1025_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>而更多的AI企业则将发展目标聚焦，开启了另一个市场：垂直小模型（行业小模型）。</p><h2><strong>二、AI垂直小模型</strong></h2><p>垂直小模型，或者称为行业小模型。即数据来源和应用仅限制于一个垂直行业，或者具体的某项任务，例如：医疗，就业，教育。甚至，更具体为一个任务，例如：写简历，或者解读财报。</p><p>通用大模型和垂直小模型是两种不同的模型类型。通用大模型是指可以在多个领域应用的模型，而垂直小模型则是指在特定领域应用的模型。这两种模型类型各有优缺点，适用于不同的应用场景。</p><p>通用大模型的优点在于其强大的性能和广泛的适用性，可以处理多个领域的任务。但是其缺点在于需要大量的计算资源和数据资源，训练成本较高。</p><p>而垂直小模型则可以在特定领域发挥更好的性能，需要的计算资源和数据资源相对较少，训练成本较低。但是其缺点在于适用范围较窄，只适用于特定领域的任务。</p><p>目前，垂直小模型的发展非常迅速。在各个行业中，都有针对特定任务或领域的垂直小模型被开发出来。例如，在医疗领域，有针对医学图像分析的垂直小模型；在金融领域，有针对风险评估和信贷审批的垂直小模型；在教育领域，有针对学生辅导和教学辅助的垂直小模型等。</p><p>通用大模型和垂直小模型之间存在一定的关系。</p><p>通用大模型可以作为垂直小模型的基石，为其提供更加丰富和灵活的底层能力。而垂直小模型则可以基于通用大模型进行优化和扩展，以更好地满足特定领域或任务的需求。在实际应用中，通用大模型和垂直小模型也可以相互配合使用，以实现更好的效果。</p><p>例如，虽然百度文心4.0的数据量非常庞大，但是仍然需要通过其“文心千帆开放平台”引入海量的合作伙伴，开发适合于具体行业的垂直小模型。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231026/v2_918efad74b114097bcae37564ee17cf2@399037_oswg32461oswg640oswg427_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>京东集团副总裁、京东探索研究院院长、智能服务与产品部总裁何晓冬就表示，如今的大模型，与当年的搜索引擎技术是相似的。搜索引擎出来后，也存在着通用和垂直之说，诸如谷歌、百度等通用搜索，但其实在各类垂直的头部App内，其实也都有自己的搜索引擎，包括京东、淘宝、美团等。在这些平台上，当你希望搜索与这些平台属性高度相关的产品或服务时，往往能比通用搜索取得更好的效果。</p><p>“从技术的角度而言，一个技术必须结合场景才能做得最好。大模型不仅仅是一个界面，它还会涉及许多非常专业决策，需要将各类数据、知识打通结合才能真正发挥价值，所以只有把这种技术跟具体的场景深度结合，才能更好地提供服务。”何晓冬表示。</p><p><strong>更为重要的是， AI大模型是经由大量互联网内容训练的。而这些数据并未经过全面的“清洗”。</strong></p><p>通过熟练正确的提示词操作，大型语言模型可以生成大量“黑暗”“虚假”“不可信赖”的有毒内容。这意味着内容审核需要发生在源头（即AI模型被训练时）以及它们大量生成的输出上。</p><p>垂直小模型则可以在大模型的基础上，只对垂直领域的知识和数据进行引入，并通过人工干预，做出可信赖、可依赖的AI应用。</p><h2><strong>三、AI人工审核员</strong></h2><p>AI无法代替人，不是因为它算力不够，而是因为它没有“立场”。</p><p>最新一部的《碟中谍7》中的“智体”，根据女杀手“被饶过一命”，从而判定“将会有可能背叛”的因果关系，进而决策直接杀掉。这个决策过程不考虑生命价值或者对错是非。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231026/v2_4c50fcd7157a41ecb645e321129e3172@399037_oswg1128042oswg640oswg866_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>但是现实的社会，需要在理性分析之外，随时引入价值观、沉没成本、机会成本、企业文化 等是非对错的因素。</p><p>因此，在垂直小模型的训练源头，就需要引入“人工审核员”这个专业岗位，保证在数据喂食的源头，就给AI一个正确的思考架构。进而，还要在对模型输出的结果进行审核和修正，以确保数据的准确性和可靠性。同时，人工审核员还需要对模型的性能进行监控和调整，以提高模型的准确性和可靠性。</p><p>为了保证AI大模型的可靠性，需要采取一些措施来进行错误数据的删除和修正，可以采用以下几种方式：</p><p>1. 数据清洗：对数据进行预处理和清洗，以去除无效、重复、错误的数据。同时，对数据进行必要的预标注和处理，以提高数据的质量和准确性。</p><p>2. 数据扩增：通过数据增强等技术对数据进行扩增，以提高模型的泛化能力和鲁棒性。同时，也可以增加模型的多样性和准确性。</p><p>3. 多样化训练：采用多种不同的训练方法和策略对模型进行训练，以获得更加全面和准确的模型结果。例如，可以采用不同的优化器、学习率、批量大小等参数进行训练。</p><p>不同于“内容算法”时代的是，算法流的内容审核员只是“关键词”标注，即便自身对需要进行监控的关键词内容完全不理解，也不妨碍上岗和工作。</p><p>而“AI小模型”时代的审核员，需要本身是该领域的专业人士，具备强有力的专业知识和严谨素养，能确保向AI模型喂食内容的100%正确性。</p><p><strong>单从这一点看，AI就不是在替代现有岗位，而是在提升现有岗位的含金量，进而“升知”“加薪”。</strong></p><p><strong>以后，“审核专员”这个招聘岗位将不复存在，取而代之的是“审核专家”。</strong></p><p class="editor-note">本文来自微信公众号<a href="https://mp.weixin.qq.com/s?__biz=MjM5OTU5NzUxMw==&amp;mid=2651035731&amp;idx=1&amp;sn=a76d076941f08f11e417eb96988aacaa&amp;chksm=bcce3cf58bb9b5e399f11c450fa9c142d731122039ad59b2a807b82302c663043e3da9acb7f9&amp;token=147235956&amp;lang=zh_CN#rd" rel="noopener noreferrer nofollow" target="_blank">“张栋伟”（ID:zhangdongwei19750613）</a>，作者：张栋伟，36氪经授权发布。</p>
]]></content:encoded>
<pubDate>Thu, 26 Oct 2023 11:00:53 GMT</pubDate>
<pubDate>Thu, 26 Oct 2023 11:00:53 GMT</pubDate>
</item>
<item>
<title>市值飙升千亿美元，Adobe示范掘金AIGC</title>
<link>https://www.36kr.com/p/2491036478461828</link>
<guid>https://www.36kr.com/p/2491036478461828</guid>
<content:encoded><![CDATA[

<p>1000亿美金，接近一家上市公司的市值，却是Adobe在过去七个月里的市值涨幅。</p><p>截至美东时间10月24日收盘，Adobe市值约2457亿美元（约合人民币17953亿元）。</p><p>公司在今年第三财季实现了48.9亿美元营收，同比增长10%，并预测第四财季业绩表现将“非常强劲”。</p><p>Adobe的这个预期，可能是出于对生成式人工智能商业价值增长的判断。</p><p>能够生成文本、图像、音视频、决策参考建议等多种内容的AIGC是目前最受关注的生成式AI（GAI）商业化应用领域。其中，最受关注以及最先落地的应用是AI绘画工具与极有可能成为个人智能助手的AI机器人。</p><p>微软于今年5月推出AI助手Copilot。在10月举行的年度创意大会上，Adobe推出了三个新的Firefly（萤火虫）系列AI模型。按照官方说法，这一模型将与Adobe旗下Ps、Pr、Ae等一众产品相结合起来，可以提升用户的生产效率。</p><p>资本方面对生成式AI商业化的潜力一度深信不疑。年初，微软向OpenAI追加投资100亿美元。</p><p>AIGC带来的改变才刚刚开始，最终赢家尚未可知。Adobe公司CEO山塔努·纳拉延（Shantanu Narayen）在今年6月接受采访时表示，<strong>许多风投支持的AI公司不断涌现，但缺乏明显的商业模式，最终将面临洗牌</strong>。</p><p>即便如此，Adobe在生成式AI商业化路径上的行为和动作，也为其他公司提供了一个关键示范。</p><h2><strong>01 入局：危机下的创新</strong></h2><p>9月，Adobe正式面向全球用户开放萤火虫模型，并同步推出收费方案，正式迈向商业化。</p><p>对于Adobe大步跨越式的拥抱AIGC，资本市场给予了这家创意软件巨头足够的欢迎。今年3月至今，Adobe股价累计涨幅超60%，市值飙升千亿美元。</p><p>清华大学计算机系教授、人工智能研究院视觉智能研究中心主任邓志东在接受《中国企业家》采访时表示，AI大模型的价值在于落地应用，否则做再大的模型也没有价值。</p><p>从具体模式来看，Adobe和微软都是将大型语言模型与现有软件产品结合，目的在于简化软件的使用，通过更加自然的交互方式（如直接采用文本提示）与使用流程，提高用户的工作效率与生产力。</p><p>这种现有的软件产品叠加大模型的方式，也是目前国内主流的行业大模型落地逻辑。邓志东认为这种方式“商业模式清晰，容易获得成功”。</p><p>“研发通用大模型就像做移动操作系统，甚至更难。现在有几家公司能做操作系统？所以基于通用大模型做应用层开发可能是更好的选择。”邓志东说。</p><p>就在Adobe开启萤火虫模型公测的同月，在线平面设计平台Canva 也在其产品内新增了部分AI 图像工具功能。10月，Canva正式发布全套AI设计工具Magic Studio，在AIGC领域继续挑战Adobe。</p><p>创立于2013年的Canva将自己定位为业余设计师的首选设计工具。相比于PS较高的学习门槛，Canva为用户提供各类优质设计模板，对设计感兴趣的普通用户可以在几分钟内创建出海报、名片等各类设计。Canva由此吸引了数以亿计的用户，也在几乎被Adobe制霸的创意设计市场切出了一块蛋糕。</p><p>但随着AI融入创意工具，用户使用门槛被进一步降低，Adobe与Canva的竞争大概率会变得更加正面。</p><p>国内智能设计公司水母智能创始人、CEO苗奘认为，Canva与Adobe的竞争揭示了一个非常典型的技术影响链条，“技术驱动了产业流程发生变化，产业流程的变化又影响了组织形式，与此同时人才标准也开始发生变化。就像现在用我们设计工具画漫画的用户，很多过去都不是漫画从业者，但当AI融入工具后，从业者的能力画像就开始变化了”。</p><p>从市场环境来看，新入场者Midjourney和Stable Diffusion等产品在AIGC领域的成功，也迫使Adobe快速走向开发落地应用的AI大模型。</p><p>创立于2021年的Midjourney所聚焦的AIGC的图像领域，是Adobe的腹地之一。今年5月，Midjourney更新至第五代，并凭借不断优化的图片生成能力吸引了上千万用户。</p><p>苗奘认为，做应用的公司就是要思考怎么能把新技术以最快的速度落地产业，“<strong>大模型和用户之间隔着应用</strong>。用户看重的就是最后生成的内容效果好不好，至于背后的大模型是什么，那是行业话题，用户没那么关心”。</p><h2><strong>02 商业化落地：要赚钱，先省钱</strong></h2><p>就在Adobe开放萤火虫模型公测的同月，微软推出了AI助手Copilot，并宣布之后会陆续将Copilot与旗下搜索引擎必应以及办公套件Microsoft 365相结合，订阅费为每月30美元。</p><p>但是，鉴于当前用户订阅Microsoft 365一年已经需要花费70美元，有多少人会支付一年360美元的溢价仍待市场检验。</p><p>Adobe在商业化上走得更精明。山塔努·纳拉延称，“我们在努力地为用户提供巨大价值，但也要在成本方面保护自己”。</p><p>成立于1982年的Adobe在全球有数十亿用户。如果像ChatGPT一样放开限制，Adobe庞大的用户群可能会给公司带来巨大成本压力。单是运行ChatGPT，OpenAI每天的开销曾超过70万美元。</p><p>目前Adobe提供两种模式供订阅用户使用自己的AI大模型，一种是免费使用，用户每月有25个积分用于生成图片；另一种是每月支付4.99美元获得100个积分。Adobe也会根据用户使用情况相应减慢服务速度防止算力过度使用。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20231026/v2_2b03525cc0ec47b69249c986aadabbdc@000000_oswg110346oswg985oswg548_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p>邓志东表示，<strong>随着产业的规模化，生成式人工智能的价格优势会很明显，“成本不会是大问题”</strong>。</p><p>苗奘创办的水母智能目前同样主攻AIGC领域，他认为等成本降到原来的20%，产业链上的各个环节参与者的身位可能会被颠覆，“行业会迎来真正的转折点”。</p><p>在应用落地方面，与其他用爬虫找图片训练的模型相比，Adobe的萤火虫模型在版权侧的安全性也让其在商业化进程中获得了更多优势。</p><p>据悉，用于训练萤火虫模型的图像数据来自于Adobe自家版权库、公开授权内容以及版权过期内容。而诸如Canva、Midjourney等产品均没有对生成图像的版权状态予以确认。换言之，用户如果将Canva上的生成式内容进行商用可能会引发关于侵犯著作权的控诉。</p><p>与微软一样，Adobe也承诺若商业用户因使用萤火虫模型惹上版权官司，公司将承担相关法律风险。</p><h2><strong>03 前景：国内百模大战不算泡沫</strong></h2><p>虽然AI大模型落地产业和商业化仍在路上，但市场热度不减，大模型层出不穷。公开数据显示，目前国内已经诞生了超过130款各式各样的大模型。</p><p>10月24日，在Microsoft 365 Copilot正式推出前一周，微软向市场呈上了2024财年第一财季报告。AI产品对业绩的拉动作用开始显露。</p><p>在邓志东看来，目前全球人工智能的发展进入了以大型语言模型为代表的通用人工智能时代的早期阶段。虽然当前国内的百模大战有些浪费宝贵的算力资源，但全新的时代刚开启，还谈不上有什么泡沫。</p><p>彭博行业研究报告显示，未来10年内，生成式AI市场规模将从2022年的400亿美元，增长至2032年的1.3万亿美元。预计到2032年，生成式AI将占IT硬件、软件服务及广告支出、游戏市场支出的10%至12%。这一比例目前不到1%。</p><p>对于行业何时会迎来突破期，邓志东认为还需要2至5年。一方面，模型、算法还需要更大的创新与发展，另一方面，面对多模态大型语言模型的发展，也需要更多高质量的数据资源与更大规模的AI算力资源。</p><p>“一个基本事实是，从今年3月份国内大模型热开始，大半年过去了，客观地说，<strong>国内还没有能完全对标GPT-4水平的生成式大型语言模型，并且还存在较大差距</strong>。”邓志东告诉《中国企业家》。</p><p>随着美国扩大对中国的半导体出口管制趋严，国内AIGC的发展速度也难免受到影响。但中国也拥有大于美国的用户数量和消费市场规模，巨大的商业前景或许会倒逼国内硬件侧提速发展。</p><p>苗奘认为对国内现阶段入局AIGC行业的公司来说，最重要的是“留在牌桌上”。“对标互联网的历程，最终做大的公司基本都不是最早入局的。坦率来讲，我们现在看到的可能仍是迷雾，行业还没有进入正式崛起的周期，所以一定要留在牌桌上”。</p><p><strong>参考资料：</strong></p><p>1、《引发新一轮技术革命的AIGC，市场潜力有多大》，第一财经</p><p>2、《探路生成式AI商业化，Adobe优先关注大模型训练中的版权保护》，界面新闻</p><p>本文来自微信公众号<a href="http://mp.weixin.qq.com/s?__biz=MTI3NTQ1MTY0MQ==&amp;mid=2650600150&amp;idx=1&amp;sn=019a1f7c5d5ed28606ff350d74b6c620&amp;chksm=7c327d404b45f456513a368bd6cad98a38035cb0903526c9847a5253d019f32606d2b9cc6943&amp;scene=0&amp;xtrack=1#rd" rel="noopener noreferrer nofollow" target="_blank">“中国企业家杂志”（ID：iceo-com-cn）</a>，作者：朱鹏，编辑：赵建凯，36氪经授权发布。</p>
]]></content:encoded>
<pubDate>Thu, 26 Oct 2023 09:41:59 GMT</pubDate>
<pubDate>Thu, 26 Oct 2023 09:41:59 GMT</pubDate>
</item>
<item>
<title>《时代》人工智能百人榜（一）：领袖</title>
<link>https://www.36kr.com/p/2443934653060997</link>
<guid>https://www.36kr.com/p/2443934653060997</guid>
<content:encoded><![CDATA[

<blockquote><p>神译局是36氪旗下编译团队，关注科技、商业、职场、生活等领域，重点介绍国外的新技术、新观点、新风向。</p></blockquote><p>编者按：人工智能的独特之处既最令人恐惧也最值得庆祝——一些技能堪比我们人类，然后还能更进一步，做到人类做不到的事情。模仿人类行为已成为人工智能的决定性特征。然而，机器学习和大语言模型的每一次进步背后实际上都是人——这里面既有经常被忽视的，让大语言模型使用起来更安全的人类劳动，又有就在什么时候以及如何最好地使用这项技术方面做出关键决定的个人。本文综合了各方推荐和建议，将数百项提名汇总到一起，最终形成了这份百人榜名单。从很多方面来说，这 100 人构成了推动人工智能发展的关系网络与权力中心。他们是竞争对手、监管者、科学家、艺术家、倡导者、以及高管——属于既互相竞争又共同合作的人类，他们的洞察力、欲望与缺陷将塑造一项影响力与日俱增的技术的发展方向。文章来自编译，篇幅关系，我们分四部分刊出，此为第一部分。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20230929/v2_a6f7b11c51d64e2e963ec48b4a615531@1694_oswg4155228oswg2462oswg1396_img_png?x-oss-process=image/quality,q_80/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1" /></p><h2>领袖</h2><h3>达里奥·阿莫代&amp;丹妮拉·阿莫代：Anthropic CEO &amp;总裁</h3><p>作为兄妹，达里奥·阿莫代（Dario Amodei）和丹妮拉·阿莫代（Daniela Amodei）比大多数兄弟姐妹都更合得来。 丹妮拉说：“我们从小感觉就比较合拍”。</p><p>对于掌管着全球领先的人工智能实验室Anthropic 的这俩兄妹来说，对齐是首要任务。用行话来说，对齐意味着确保人工智能系统与人类价值观“保持一致”。 40 岁的达里奥与36 岁的丹妮拉（分别是 Anthropic 的首席执行官和总裁）相信，与其他开发尖端人工智能系统的公司相比，他们正在采取更安全、更负责任的方法来进行人工智能的对齐。</p><p>成立于 2021 年的Anthropic开展了开创性的“机制可解释性”（ mechanistic interpretability）研究，其目的是让开发者能够进行与大脑扫描类似的操作，从而了解人工智能系统内部到底发生了什么，而不是仅依赖于其文本输出来评估，因为这并不是其内部运作方式的真实体现。 Anthropic 还开发了Constitutional AI，这是对齐人工智能系统的一种全新手段。它已将这些方法嵌入到其最新的聊天机器人 Claude 2 之中，后者已成为 OpenAI 最强大的模型 GPT-4 的有力竞争对手。</p><p>Constitutional AI可让开发者通过建立“宪法”来明确指定他们的系统应遵守什么样的价值观，将人工智能是否能做某事的问题与更具政治色彩的是否应该做某事的问题分开。达里奥说，人工智能对齐的另一种主要方法——也就是所谓的人类反馈强化学习（RLHF）——往往会导致将这两个问题“混为一谈”。卡内基梅隆大学最近的研究表明，接受过更多的 RLHF 训练的聊天机器人，往往会比没有接受过 RLHF 训练的聊天机器人给出更社会、经济自由化的答案。这可能是因为训练过程往往会奖励模型的包容性和无害性。Constitutional AI可以让开发者向人工智能灌输一套编成法典的价值观，而不是让 RLHF 隐式地且不完美地发挥影响。</p><p>达里奥说：“我认为把这些技术问题区分开来是有好处的：一个是模型试图遵守宪法，但能不能完美做到的问题，一个更多属于价值观辩论的问题：也就是宪法中的内容是否正确？”他说，过去这两个问题经常被混为一谈，“导致了大家对这些系统如何工作以及它们应该做什么的讨论没有成效。”</p><p>Anthropic 有七位创始人，他们都曾在 OpenAI 工作过，然后纷纷离开并创办了自己的公司。达里奥和丹妮拉对于推动他们离开的原因（如果有的话）用了一种外交辞令，但表示他们从一开始对于给模型设立安全屏障就有着不同的愿景。 达里奥说:“我认为我们在这个生态体系中的存在有望让其他组织变得更像我们。这是我们在这个世界的总体目标，也是我们变革理论的一部分。”</p><p>因此，Anthropic 对自己的定位是人工智能安全研究实验室。不过，为了进行这项研究，阿莫代兄妹已经算出两人需要开发自己最先进的人工智能模型。为此，他们需要大量的计算能力，这反过来意味着他们需要大量的资金。这意味着他们需要以企业的形式运营，向其他企业出售自己的人工智能模型的使用权，并从投资者那里筹集资金，因为非营利组织负担不起这些。 Anthropic 目前已筹集了 16 亿美元，其中包括来自现已破产的 FTX 加密货币交易所的 5 亿美元。</p><p>Anthropic 的创始人认识到这种商业化的做法存在内在的紧张关系——这可能会加剧他们创立 Anthropic 所要预防的（安全）问题——但他们相信，要想进行有意义的人工智能安全研究，这是唯一办法。 达里奥说：“安全问题与模型的内在能力之间的关系纠缠不清，这是导致问题变得困难的原因之一”。</p><p>为了努力让自己免受市场可能产生的一些不正当激励的影响，Anthropic 的领导者打算将公司建设为一家公益公司，这意味着它的创立是为了产生社会公益和公共利益。实际上，这种做法导致即便投资者认为 Anthropic 把财务回报以外的目标当做优先事项，想提起诉讼的难度也会加大。但 Anthropic 是否已经解决了维护人工智能安全与作为公司运营之间的紧张关系呢？这歌问题会持续存在。今年 4 月，TechCrunch 报道称，Anthropic 曾向潜在投资者承诺将用他们的钱开发 Claude-Next，据称该模型的能力比当今最强大的人工智能强 10 倍，以此来筹集资金。</p><p>达里奥对该报道提出异议，但没有给出具体细节，并拒绝对Claude-Next发表评论。他还否认了这样一种观点，即以公司形式运营 Anthropic 的决定已导致自己陷入到一场适得其反的建造更大模型的竞赛，但他说：“不管怎样，扩大模型规模是我们计划的一部分。我认为，选择不这样做并不是解决问题的办法。”</p><p>对于人工智能可能给人类未来带来的潜在好处，Anthropic 的领导人明显不像其他人工智能的杰出人物那样直言不讳。丹妮拉说，这“并不是因为我们不在乎，也不是因为我们认为没好处，而是因为我们认为现在有很多重要的工作要做。”她的哥哥同意这一观点，但对讨论人工智能乌托邦场景有何价值持更怀疑的态度。 “我只是不想让这个变成企业宣传。”</p><h3>山姆·阿尔特曼：OpenAI CEO</h3><p>山姆·阿尔特曼给予人类很大的信任。他相信，人类足够聪明，适应性足够强，能够应对日益强大的人工智能释放到世界上——只要这些释放是安全且逐步进行的。 38 岁的 OpenAI 首席执行官阿尔特曼表示：“社会有能力适应，因为人们比许多所谓的专家想象的要更聪明、更精明。我们可以解决这个问题。”</p><p>这一理念不仅解释了为什么 OpenAI 决定在 2022 年 11 月推出震惊世界的聊天机器人 ChatGPT。这也是该公司在几个月后加倍下注，一鼓作气又面向公众推出了 GPT-4这个有史以来最强大的大语言模型的原因。</p><p>这些人工智能的发布不仅让创业加速器 Y Combinator 的前总裁阿尔特曼成为科技界最热门的名字之一，而且这些发布也证明了他的正确，至少到目前为止是这样：人类能够快速适应这些工具，不会自我崩溃。国会议员们在恐惧与敬畏的推动下开始认真讨论该如何监管人工智能，而这是阿尔特曼一直以来的希望。在 ChatGPT 发布后短短几个月内，美国和英国政府就宣布要将新的人工智能计划列为高度优先事项，而七国集团也宣布制定一项协调计划，要为这项技术设置护栏。人类正在逐渐学习如何利用人工智能工具来追求直接劳动、教育和乐趣（或许学习什么时候不该用更重要）。</p><p>尽管 ChatGPT 与 GPT-4 令人敬畏，但它们最重要的遗产可能是促使人类想象接下来会发生什么。当今最先进的系统可以编写令人信服的文字、通过律师考试、调试计算机代码。如果它们能够制定复杂的计划、欺骗用户实现自身的目标，并脱离人类监督自己行动时，会发生什么？ OpenAI 的突破之年表明，在拥有足够的超级计算机、数据和资金的情况下，全球最先进的人工智能公司可能很快就能召唤出具有此类能力的系统。与早期针对特定目的进行硬编码的计算机不同，机器学习是一种新的计算范式，它带来的系统只有在开发后才能知道能力。随着这些系统变得更加强大，它们也变得更加危险。至少现在，即便是计算机科学领域最聪明的人也不知道如何可靠地约束住它们。</p><p>值得称道的是，除了推动人工智能所谓能力不断发展外，阿尔特曼领导下的 OpenAI 也把解决这个待解问题作为方案的关键部分。它开创了“利用人类反馈进行强化学习”（RLHF）这个安全技术，这一创新意味着 ChatGPT 不会陷入到导致前几代人工智能聊天机器人失败的毒性陷阱。 （虽然这项技术并不完美——需要依赖低薪人力，并且无法始终确保聊天机器人用真实信息做出响应——但仍是迄今为止本行业最好的技术之一。）今年6 月，OpenAI 宣布将计算能力资源的20%投入到“超级对齐”问题的解决上——也就是如何确保比最聪明的人类还要聪明的人工智能系统在行动时会以人类的最大利益为重。</p><p>阿尔特曼的哲学清楚地指明了未来任务方向。 阿尔特曼表示：“我们有责任教育决策者和公众那些我们认为正在发生的事情，以及我们认为可能会发生的事情，并将技术推给世界，让大家能够看到它。我们的机构和民间社会的职责是弄清楚我们作为一个社会想要的究竟是什么。”</p><h3>戴密斯·哈萨比斯：谷歌Deepmind CEO、联合创始人</h3><p>当戴密斯·哈萨比斯（Demis Hassabis）还是个年轻人时，就帮助设计了《主题公园》（Theme Park）。这是一款流行的电脑游戏，让玩家能够以上帝视角监管庞大的游乐场业务。从那时起，领导着顶级人工智能实验室之一的哈萨比斯就一直在努力获得观察世界的上帝视角。</p><p>DeepMind 成立于 2010 年，并在 2014 年被谷歌收购。作为该公司的首席执行官，哈萨比斯带领计算机科学家团队在人工智能取得了若干突破，包括解决了棘手的蛋白质折叠问题，以及在复杂的围棋游戏中击败了人类世界冠军。 2023 年 4 月，在 OpenAI 的 ChatGPT 取得成功后，谷歌CEO桑达尔·皮查伊（Sundar Pichai）开始重组公司的人工智能团队，哈萨比斯获得了更多权力。此次重组将 DeepMind 与谷歌的另一个人工智能实验室 Google Brain合并，并由哈萨比斯掌舵。皮查伊打算精简谷歌在开发强大人工智能的努力，以便抵御日益激烈的竞争。</p><p>谷歌 DeepMind正在开发一个大型人工智能模型，名字叫做Gemini。哈萨比斯暗示，这个模型可能将超越 OpenAI 的 GPT-4。 （这个模型将是“多模态”的，也就是它不仅可以输入和输出文本，还可以输入输出其他形式的媒体，比方说图像。）跟 OpenAI 的山姆·阿尔特曼非常相似，哈萨比斯认Gemini只是实现通用人工智能（AGI）这个更大追求的一步，他同时认为，只要人类能避免AGI不受控制的发展可能带来的严重风险，AGI就可以释放科学进步并重塑世界，让世界变得更美好。</p><p><strong>问：早在今年四月份的时候，谷歌就宣布将把两个独立的人工智能实验室合二为一，由你领导。这对你一直都在做的工作来说有何改变？</strong></p><p>戴密斯·哈萨比斯：我认为是好事，因为这能让我们走快一点、更精简、也更加协调，尤其是考虑到这个领域正在加速发展。就整个新组织的使命而言——有时候我喜欢把它叫做是新的超级单元——融合了这两个令人惊叹的组织的优势与传奇历史。我希望在创新、能力、探索性研究上加倍下注，但现在还必须对大模型的大工程进行许多的协调。这就是新部门职责的一部分。</p><p>至于任务方面，新组织要做的事情是之前两家实验室所做的事情的超集。显然，这包括最先进的技术不断进步，所有针对通用人工智能的研究，以及利用人工智能来推进科学发展等。这些还会继续做。但此外，还包括将来利用人工智能驱动的产品前所未见的能力改善数十亿人的日常生活。而谷歌凭借着所拥有的产品界面，有难以置信的机会来做到这一点。其中包括六款用户超过 20 亿的产品（比方说 Google Search与 Android），以及 15 款用户超过 10 亿用户的产品（比如 Google Drive与Photos）。对于让人工智能进入平常人家，丰富他们的日常生活而言，还有其他更好的手段吗？我们对继续在所有这些方面的工作感到非常兴奋。两个实验室都已经在做上述的一切了。但我想说，现在力度更大，速度更快了。</p><p><strong>你们现在正在开发这个叫做“Gemini”的新模型。我们对它应该有什么样的预期？是之前版本的扩大版，只不过训练数据更多了，计算能力更强了？还是说它在设计方式上存在架构的不同？</strong></p><p>这既是不同规模的组合，也有创新。其中一个关键是一开始它就是多模态的。我们过去做了很多多模态方面的工作，比如 Flamingo，这是我们用来描述图像内容的系统。这个最终支撑起整个行业许多的多模态方面的工作。 [Gemini] 在此之上做了一些改进，然后再跟文本模型等东西内置到一起。我们还考虑实现规划与记忆能力——相对来说我们处在最早的探索阶段。而且你应该把 Gemini 堪称是一系列模型而不是单个模型，尽管显然，会有不同规模的单一模型。所以我想说，这是规模与创新的结合。从早期结果来看很有希望。</p><p><strong>我们现在看到的大语言模型始终存在所谓的幻觉问题，或者说它们倾向于将猜测当作事实。利用强化学习技术似乎可以让模型诚实一点，但目前还不清楚强化学习及扩大规模是否可以完全解决问题。你现在对这个问题有什么想法？</strong></p><p>我认为强化学习甚至可以帮助[模型]变好一个数量级。帮助相当大。但要完全解决[幻觉]问题，我认为还需要一些其他创新。诸如检索方法和工具使用之类的东西会有所帮助。也许可以利用谷歌搜索作为工具来检查[模型]将要输出的内容，或者检查[模型]的情景记忆库（可能可以存储事实）。我认为这些也许是提高准确性的好方法。不妨以我想将这些模型用于科学研究的情况为例。如你所知，[聊天机器人]援引的东西往往看似很合理，但却是编造的，因为那只是看似合理的用词。所以它需要对这个世界存在哪些实体有更好的理解。学术论文不是一系列独立单词，整个引文基本上是一个整体。所以我认为系统需要知道一些类似的事情。也就是说，逐字预测是不合适的。你得将整个论文名称、摘要以及出版地点作为整体进行检索。</p><p>确实，所有这些方面我们都在努力。一是改进强化学习。我们与 Sparrow 进行了很出色的合作。我们最终没有把 Sparrow 作为独立模型发布出来，但我们利用了从中学到的各种经验，包括对规则的遵守，坚持某些类型的行为等。经过一段时间的努力，我们已将错误率从 10% 降低到 1%。所以这是一个数量级的改进。关键是要在不降低模型的清晰度、创造力与趣味性及回答能力的基础上做到这一点。这就是其中的权衡取舍。我们知道如何做其中一点了。如果能同时做到这两点则更佳：也就是既要高度准确，但仍然非常明晰和有创意。</p><p><strong>我们上一次聊是在去年 11 月，那时候 ChatGPT 还没发布。即便如此，你还是发出了这样的警告：人工智能这个研究领域正变得越来越危险，我们正处在这些技术对社会造成巨大破坏的风口浪尖，人工智能研究人员需要更加小心。后来，你还联署了一封公开信，警告先进人工智能的风险与流行病及核战争一样严重。但也是从那时起，我们看到各国政府对人工智能政策的思考方式发生了巨大变化：他们开始认真思考这个问题，而在去年 11 月的时候他们绝对不是这样的。你现在感觉是更乐观了，还是更害怕了，还是差不多——为什么？</strong></p><p>你是对的，我们交谈的时刻很有趣，就在最新一波事件再次改变一切的前夕。这个话题我认为很复杂。事实上，我对目前的情况相当乐观。当我与英国和美国政府官员交谈时，发现他们现在已经掌握了最新情况，这很棒。这是这波聊天机器人热的好处：让公众、政治家和民间社会的其他关键人物接触到最新的人工智能。我依旧认为，也许带了点偏见，到目前为止，像[蛋白质折叠突破]AlphaFold这样的东西，对于科学进步来说更为重要。</p><p>当然，我们之所以成为人类是因为语言。所以很显然为什么聊天机器人会引起如此大的共鸣。我认为好的一面是 [ChatGPT] 设法移动了奥弗顿之窗（编者注：约瑟夫·奥弗顿认为，一个政策的政治可行性主要取决于它是否在这个范围内，而不是政客的个人偏好），让大家可以讨论这个问题，而这是优先事项。到目前为止，我所看到的情况，尤其是英国、美国等国家的情况，都非常好。比方说，我认为英国关于人工智能的白皮书对责任和创新做了很好的权衡。认真对待是有好处的。</p><p><strong>关于你们相对谷歌的独立性问题一直存在。据报道，谷歌在 2014 年收购 DeepMind 的时候，曾做出这样的保证：如果 DeepMind 创建出通用人工智能，它将受到 DeepMind 的独立道德委员会而非谷歌本身的监督。但 2021 年时，《华尔街日报》报道称，DeepMind 争取更多自主权（其中包括建立一个防止所开发的强大人工智能被单一公司实体控制的法律结构）的谈判最终以失败告终。现在，随着 2023 年两家实验室合并，以及 DeepMind 成为谷歌的正式一员，在外部看来，谷歌正在进一步收紧控制，并侵蚀你过去可能拥有过的独立性。你有这种感觉吗？</strong></p><p>事实上恰恰相反。过去的猜测我不便详述，其中很多都是十分不准确的。但道德准则部分——我们一直都有这样的规定，当刚开始 DeepMind还是独立的时候，当我们进入 [谷歌] 的时候，这个东西就有了，后来它又发展成为 [谷歌] 人工智能原则。所以实际上，谷歌现在已经在全公司范围内采用了。我们对此感到非常满意。这跟 DeepMind 的原则几乎没有区别。谷歌人工智能原则的输入很大一部分来自于我们。所以这一切都是匹配的。我们对[how]产生了巨大影响，我对谷歌解决这些问题的整体方式感到非常满意。我认为这是非常负责任的。我们的绰号是“既要大胆也要负责”地运用这项技术。我认为这两谷歌都得要。在创造性方面这两个词之间存在着紧张关系，但我认为这是故意的。我们没有不舒服的感觉。我们都十分专注于以深思熟虑、科学、负责任的方式向全世界提供这项令人惊叹的技术的好处。但这并不意味着我们永远不会犯错，因为这是一项新技术。它发展得太快了。但我们希望能最大限度地减少这种情况，并最大限度地提高效益。事实上，这与外界以为的情况完全相反。</p><p><strong>我们已经讨论了很多关于风险的问题。Gemini有没有什么功能是如果在测试阶段展示时你会决定：“不，我们不能发布这个”的？</strong></p><p>是的，我的意思是，这也许是几代之后的事了。我认为人工智能研究领域现在最紧迫的事情是提出合适的能力评估基准，因为我们都喜欢测试，一系列的测试，甚至有数百个，如果你的系统通过测试，就可以获得风筝标志（英国质量认证），你会说，对，用 X、Y、Z 的方式部署是安全的。而且政府可以批准这个。消费者会明白这意味着什么。问题是，我们目前没有这类基准。我们对此有一些想法，比如，这个系统有没有欺骗的能力？它可以跨数据中心自我复制吗？这些是你可能希望测试的东西。但如果你想对它们进行实际的、务实的测试，就需要非常严格的定义。我认为这是整个领域最紧迫的事情。我们都在努力做到这一点。</p><p>我们帮助成立的新组织“Frontier Model Forum”，在一定程度上是为了让领先的公司联合起来进行更多人工智能的安全研究。我们需要严格的评估和基准测试技术。如果有了这些，而系统没有通过，就意味着你不能发布，直到解决了相关问题。也许你会用强化的模拟器或强化的沙箱之类的东西来做，并在周边保证网络安全。我们大概是这么想的，但想法需要更具体一些。我认为这件事情最紧迫，要在那种系统出现前及时完成，因为我认为我们可能还有几年，甚至更长的时间。如果把必须完成的研究考虑上，实际上剩下的时间已经不多了。所以我并不担心今天的系统。但可以预见，从现在起的几代（大模型）之后，我们将需要更严格的东西，而不只是看看用了多少算力而已。</p><h3>李彦宏：百度CEO、主席、联合创始人</h3><p>作为中国最重要的未来学家，李彦宏长期以来一直站在人工智能浪潮的风头浪尖。自 2000 年创立中国最受欢迎的搜索引擎百度以来，李彦宏的使命就是更好地理解和预测人类行为，并投入了数百亿美元进行人工智能研究。百度已经拥有了相当于亚马逊虚拟助手 Alexa 的产品（叫做“小度”），在中国某些最大城市运营着无人驾驶出租车车队，其中光在武汉就有 200 辆。但 54 岁的李彦宏表示，最近生成式人工智能的爆发式增长意味着现在是“一个十分激动人心的时代。人工智能现在有能力进行以前无法做到的各种逻辑推理。”</p><p>今年8 月 31 日，百度公开发布了自己的大型语言模型文心一言（ERNIE Bot），李彦宏声称该模型在几个关键指标上优于 ChatGPT。在中国考虑对人工智能进行适当监管之际，李彦宏也是一个值得信赖的人物。今年 7 月，百度被任命为中国政府的国家人工智能标准化大模型专题组组长，李彦宏表示，经过过去几个月的时间，该工作组的情绪已经“从监管转变为更倾向于建设性的心态。我非常希望我们很快就能在广泛的场景中提供公共服务。”</p><p>人工智能靠数据来学习，当中国的监管刹车解除时，百度每月 6.77 亿用户所提供的服务量可能会超过国内的任何竞争对手。李彦宏认为，这种可能性是“人机交互的范式转变”，他将当前的拐点与移动互联网的诞生相提并论，移动互联网预示着 Uber、微信与 TikTok 等appp在桌面计算上永远没法流行起来。李彦宏预测会出现“数百万我们可能无法想象的新的人工智能原生应用”。</p><p>一个紧迫问题是百度能不能利用硬件来实现李彦宏的野心。尽管这家市值达 480 亿美元的公司已经开发出自己的昆仑微处理器，但它严重依赖来自加州英伟达的进口，拜登政府已禁止该公司向中国客户出售其最先进的芯片。李彦宏说，这些限制令人“担忧”，但也可能带来机遇。李彦宏说： “如果我们购买美国芯片的门槛越来越高，那么国产芯片将成为一个可行选择。不管怎样都存在很多的创新机会，所以我对人工智能的未来持乐观态度。”</p><h3>克莱门特·德朗格：Hugging Face CEO，联合创始人</h3><p>克莱门特·德朗格（Clément Delangue） 是 Hugging Face CEO，后者是一个开源的营利性的机器学习平台，来自世界各地的研究人员聚在这里分享自己的人工智能模型、数据集以及最佳实践。 Delangue 表示，在一个由大型科技公司主导的行业李，像总部位于巴黎的 Hugging Face 这样的努力是一个重要的平衡因素，它们的存在有助于将人工智能传播到更广泛的用户和开发者群体。他说， Hugging Face 还实施了一套社区标准，从而防止有害的人工智能模型出现扩散。</p><p>但开源人工智能的兴起可能是一把双刃剑——强大的人工智能能力不仅可以落入到善意的人手中，也会落入滥用者手中。</p><p><strong>问：谷歌人工智能研究人员最近撰写的一份备忘录认为，就该领域最重要的创新而言，开源社区正在迅速超越谷歌和 OpenAI。对此你怎么看？</strong></p><p>Delangue：我不知道我是不是真的想对此发表评论，因为这是从一家公司披露出来的意见。但从更广的范畴而言，我觉得大家已经认识到开放科学与开源是过去几年人工智能的基础，这是件好事。大家都在齐心协力做建设。我认为大家能记住这一点很重要，这样当我们决定迈出下一步时，就可以决定想不想要由少数公司控制的东西，还是想要更具协作性的方案。因为这是两条不同的道路，两种不同的未来摆在我们面前。</p><p><strong>关于开发更复杂、能力更强的人工智能，最近有很多相关风险的讨论。有人认为，向更大范围扩散这种先进的人工智能是危险的，而且大玩家越少，这些危险就越容易控制。你怎么看？</strong></p><p>你看看社会就知道了，最大的风险其实是权力和理解力集中在少数人手上。特别是对于像人工智能这样的技术，尤其是如果这些组织不是为了满足公共利益而设计的。公司的初衷可能是好的，但其本质上是私人营利组织。如果你着眼于技术的长期健康发展，我们相信更加大众化会制造出更多的对抗力量，减少带来的风险，因为它在赋权并促进监管。监管机构没法监管自己不了解的、不透明的事物。如此一来，我们的处理方式与其他一些组织会略有不同。</p><p><strong>你认为 Hugging Face 在人工智能社区当中扮演着怎样的角色？</strong></p><p>我们很幸运成为最常用的共享科学、模型、数据集和应用的平台。我们大力支持在人工智能能力方面提高透明度、开放性，分配更多的权力出去。这是我们正在努力推动的事情，同时我们也尊重不同公司采取的不同做法。现在，在 Hugging Face 平台上，你可以发布只与一小部分人共享的模型，比方说，出于研究目的而共享的模型。我们正在尽最大限度提高发布的安全性与道德性，实现更安全的发布，同时确保我们在组织内建设出一个具有价值知情流程的平台。</p><h3>莉拉·易卜拉欣：谷歌DeepMind COO</h3><p>2017 年，莉拉·易卜拉欣 (Lila Ibrahim) 面试了 50 多个小时才拿到她现在担任的谷歌 DeepMind 首席运营官这个职位。这在一定程度上是 DeepMind 的领导层想确保她是这份工作的合适人选。但在科技领域已工作了二十多年的易卜拉欣也在做自己的尽职调查。 “我确实也得问问自己，并且跟家人谈谈。这是一项非常强大的技术。我是那个对的人吗？这个地方来对了吗？”</p><p>53 岁的易卜拉欣形容 DeepMind 是一家集初创企业、学术实验室与全球科技巨头于一体的公司。2014 年，这家公司被谷歌收购，今年 4 月，又与Google Brain合并。她觉得自己在英特尔、风投公司 Kleiner Perkins 以及教育科技初创企业 Coursera 有过工作经历，这让她特别有能力管理 DeepMind 的日常运营，并领导公司的责任与治理工作。她说 “我觉得我就是为这一刻而生的”。</p><p>易卜拉欣表示，自从 ChatGPT 发布以来，公众围绕着人工智能的讨论已经发生了变化。但自 DeepMind 成立以来，业界与政策制定者之间就一直在讨论如何开发安全的人工智能系统。 易卜拉欣指出，DeepMind 拥有与 ChatGPT 类似的技术——一篇论文表明，DeepMind 在 2020 年 12 月层开发了一个类似系统 Gopher——但决定不发布出来，因为担心它会给出与事实不符的响应。自 ChatGPT 发布以来，包括 Google 在内的许多公司都发布了自己的聊天机器人，但 DeepMind 还没有这么做。</p><p>今年 5 月，易卜拉欣与 DeepMind 创始人戴密斯·哈萨比斯（Demis Hassabis）及 Shane Legg 一道签署了一份声明，宣布对于人工智能带来的风险应该要像流行病和核战争的风险一样严肃对待。作为 DeepMind 责任与治理工作的领导者，易卜拉欣负责降低这些风险。</p><p>她说，除非她觉得人工智能可以创造出来的机会超过了风险，否则自己是不会加入 DeepMind的，但她回忆起在面试期间自己层对风险的严重性感到震惊。易卜拉欣说： “我有一对双胞胎女儿。我一直在想：晚上我还可以替她们盖被子吗？”</p><h3>埃隆·马斯克：xAI 创始人</h3><p>这位全球首富对人工智能极度恐惧。但他也在积极帮助人工智能的发展。2010 年代初期， 52 岁的埃隆·马斯克 (Elon Musk) 对人工智能会如何造福人类或毁灭人类产生了兴趣，他向人工智能初创企业 DeepMind 投资了数百万美元。但在 2014 年 DeepMind 被谷歌收购后，马斯克开始担心谷歌对人工智能安全不会认真对待。因此，他与山姆·阿尔特曼共同创立了一家新的人工智能公司 OpenAI，试图对抗谷歌在这一领域日益增长的主导地位，并打算用他们认为更负责任的方法来建立通用人工智能 (AGI)——这是一种假设的未来人工智能系统，可以做任何人脑能做的事情。</p><p>不过， 2018 年，马斯克去辞去了 OpenAI 董事会的职务。这家公司现在已经发展成为一家巨头，批评者开始担心这家公司本身发展得太快。马斯克就是这些批评者之一：他曾公开与阿尔特曼互相喷口水，并把 OpenAI 称为时“来自地狱的，利润最大化的恶魔”。他还批评 OpenAI 的 ChatGPT 搞“政治正确”，并承诺要建立自己的聊天机器人，名字叫做“TruthGPT”。</p><p>不管这个项目能否取得成果，马斯克还有一系列的人工智能项目要忙，其中包括他于今年 7 月宣布成立的新公司 xAI，其目标是“了解宇宙的实质”；特斯拉的自动驾驶汽车； 旨在将微芯片植入人脑，让人能够直接与计算机对话的Neuralink；设计用于物理世界的类人机器人Optimus。</p><p>但就算马斯克领先，他仍大声表达自己对人工智能可能对人类构成的威胁的担忧，而且在让主流倾听到这些担忧方面发挥了重要作用。马斯克在今年三月份签署了一封公开信，呼吁暂停人工智能开发和训练。 他今年曾告诉塔克·卡尔森（Tucker Carlson）：“人工智能比管理不善的飞机设计或生产维护，或糟糕的汽车生产更危险，从某种意义上来说……它有可能毁灭文明”。光是马斯克的巨额财富就足以确保他仍将是人工智能领域最有影响力的人物之一。但他也因为对风险和回报存在自相矛盾的观点而与众不同。</p><h3>拉克尔··乌尔塔松：Wabbi CEO，创始人</h3><p>拉克尔··乌尔塔松(Raquel Urtasun) 表示，说到技术趋势，晚到会带来一些违反直觉的好处。 她说：“后发有巨大优势”。2021年，距 2010 年代中期自动驾驶行业的炒作热潮已经过去五年了，这位 Uber 自动驾驶部门的前首席科学家却创立了自己的自动卡车初创企业 Waabi。在那段炒作期成立的许多公司都未能实现自身的远大抱负，同时身为多伦多大学计算机科学教授的乌尔塔松表示，后发帮助Wabbi在其他公司举步维艰的地方取得了成功。</p><p>一方面，起步较晚使得该公司得以利用人工智能的最新进展：乌尔塔松说,相对于竞争对手，Waabi 得以更快、更便宜地训练其无人驾驶软件，部分是因为可以在高度逼真的人工智能生成的模拟环境下驾驶虚拟卡车。这使得该公司的驾驶软件不用在现实生活经历过才能学会应对棘手情况。乌尔塔松说： “你可以制造各种在现实世界中很难或不可能生成的东西。（在现实生活当中）制造事故来看看软件是否能够处理是不道德的。” Waabi 背后的想法在业界引起了轰动： 2021 年，该公司筹集了约 8300 万美元的风投资金，并计划将其技术许可给用长途卡车进行运输的公司。去年年底，该公司推出了第一批机器人卡车，这些卡车将用于试验该公司的系统。</p><p>乌尔塔松本人也让 Waabi 显得与众不同，因为人工智能领域的女性并不多，初创企业由女性领导，并如此迅速地取得成功的就更不用说了。 乌尔塔松说：“我们通常需要多付出 10 倍的努力才能获得相同的认可。我之所以能取得这些成功，是因为我有不竭的毅力。不管别人说什么对我来说都是一种激励， ‘我要证明给你看。’”</p><h3>亚历克斯·卡普：Palantir CEO，联合创始人</h3><p>为了赢得秘密且经常引起争议的合同，二十年来，这位傲慢的 Palantir 首席执行官一直在向美国政府机构示好。用《指环王》中神秘的视眼石命名的这家公司，已经把自己的数据挖掘工具出售给包括美国移民和海关执法局 (ICE)、联邦调查局 (FBI)、美国陆军、中央情报局 (CIA) 以及其他西方情报机构在内的客户。亚历克斯·卡普（Alex Karp）坚持认为美国科技公司有义务支持美国政府，这常常令投资者和一些员工感到不安。 他说：“在人气排行榜上，我们的排名从来都不是靠前的”。</p><p>但现在，55 岁的卡普认为，人们在某些方面担忧的日益加剧，正在推动该行业朝着他的方向发展。卡普说： “人工智能革命的第一枪实际上是当人们看到它在战场上的实施时”。卡普是俄乌冲突后第一位访问乌克兰并会见乌克兰总统泽伦斯基的西方大公司首席执行官。</p><p>在 2004 年与斯坦福同学彼得·泰尔（Peter Thiel）共同创立Palantir之前，卡普曾攻读过哲学博士学位。他称赞该公司的工作属于“更高的使命”。 卡普说：“我们就像一支奇怪的乐队，演奏着每个人都觉得有些烦人和刺耳的音乐，然后说，‘不，这就是你会喜欢的音乐，’现在我们很高兴有更多的人欣赏它，其他的乐队也在向更多的观众演奏这种音乐。”</p><p>卡普长期以来一直对有可能侵犯隐私或政府滥用 Palantir 技术的批评不以为然。当谷歌等其他科技公司因员工担心这些数据可能被用来驱逐移民而放弃与 ICE 的国防合同时，Palantir 加大了与有争议的合作伙伴的合作力度。卡普表示：“我认为这简直是荒谬至极。”他认为，随着人工智能驱动的军事技术将决定全球力量平衡，其他国家的企业不会有同样的疑虑。 “我认为我们应该立即制定一项法律，规定美国生产的所有技术都将提供给美国政府。”</p><h3>里德·霍夫曼：企业家，投资人</h3><p>企业家里德·霍夫曼（Reid Hoffman）成功经历了多代技术突破。 1990 年代，他曾在苹果公司工作，是 PayPal 董事会的创始成员，共同创办了 LinkedIn，并在 Facebook 的创立过程中发挥了至关重要的作用。</p><p>最近，58 岁的霍夫曼把全部注意力都转向了他所认为的下一次伟大技术革命。霍夫曼是 OpenAI 的首批投资者之一，他的风投公司 Greylock Partners 已向数十家人工智能公司投资了数亿美元。 （他过去两年的所有投资都与人工智能有关。）最近，霍夫曼与他人共同创立了人工智能聊天机器人初创公司 Inflection AI，并与人工智能合作撰写了一本书，名字叫做《即兴曲》（Impromptu）。</p><p>尽管其他人工智能领导者因技术风险而呼吁暂停人工智能开发，但霍夫曼认为全球对人工智能应该采取加速主义方法。 他说：“早一个月、早一年、早一周的时间用上人工智能都可以造福人类”。</p><p>面对风险，霍夫曼捍卫了自己的立场，并谈到了他希望将人工智能助手带给每个家庭，每一个工作场所的愿望。</p><p><strong>问：你有没有过那么灵光一现的时刻，让你决心全力投入到人工智能领域？</strong></p><p>里德·霍夫曼：当我看到 DeepMind 的 AlphaGo 做到的事情，以及看到规模（算力）所带来的影响时，我就在想，“这次跟之前的人工智能浪潮不一样，它将撬动行业发展、推动世界向前。”然后，我又进入了 OpenAI 董事会，当我看到从 GPT-2 发展到 GPT-3时，我大概是被说服了。OpenAI其实就是将transformer规模化，结果引发了现在大家都在做的一场海啸。</p><p><strong>你曾与人合著了一本书，名字叫做《闪电式扩张》（Blitzscaling）。在书中，你提倡用超激进的方法来发展业务。你觉得闪电式扩张的原则适用于人工智能公司吗？</strong></p><p>简短回答是肯定的。基本上，人工智能其实就是规模革命。没有灵光一闪，也没有新的算法。许多算法和计算技术其实已经有几十年的历史了。在建造大规模计算机，用成百上千个 GPU组建密集的计算网络，然后运用深度学习及其他技术之后，就引发了当前这场人工智能革命。</p><p>因为我们生活在移动互联网连接时代，所以在这些大型计算机上花费数亿美元（很快将达到数十亿美元）具有经济意义，这可以让数十亿消费者以及企业员工能够执行这些智能应用。</p><p><strong>许多人对闪电式扩张的做法持批评态度，他们认为这会给整个社会带来各种外部效应：会导致垄断，带来巨大的不必要风险，并鼓励不道德的行为。</strong></p><p>事实是，在全球互联网移动连接的世界里，迅速扩大规模才能赢得胜利。无论是赢得搜索引擎市场（比如谷歌），还是主导各种社交网络（比如 Facebook、LinkedIn）都是如此。所以说，这就是获胜的模式。</p><p>但这就是我在书里面有一章谈负责任的闪电式扩张的部分原因。有很多闪电式扩张也是讲道德的：比如 LinkedIn、Netflix、Airbnb、Google。我地区额认为闪电式扩张也能带来很多好处，比方说Airbnb让住宿服务发生变革。从目前来看， ChatGPT 的发布在其崛起过程中毫无疑问也是积极因素。</p><p>我可以看到人工智能可以通过很多方式运用到气候变化与流行病上。当你问“人工智能该不该采取闪电式扩张？”时，我说：“嗯，这些东西很多对人类来说都属于重大风险，人工智能在对付这些东西上可能会有积极表现。”所以还是越早到达越好。</p><p>可以预见，将来每部智能手机上都会有医疗助理或指导老师。想想这对人类福祉的提升。因此，早一个月、早一年、早一周的时间用上人工智能都可以造福人类。</p><p><strong>但生成式人工智能模型总是一本正经地说假话。 Meta 的Galactica可以让人创建完全伪造、但看起来像真的科学研究论文。似乎科学和医学特别不适合这项技术？</strong></p><p>&nbsp;“应该把医疗服务全都交给 GPT-4”， 我们不能这样，白痴才这么讲。我们得明智地应用人工智能。</p><p>关于如何大规模地减少幻觉（人工智能生成的不准确），如何获取更多事实，有很多非常好的研发。从去年夏天开始，微软就一直在努力解决这个问题，谷歌也是如此。这个问题是可以解决的。我敢打赌，你可以在几个月内将幻觉降到人类专家的水平。所以总体来说我其实对这个问题并不是太担心。</p><p><strong>你的公司 Inflection想开发一个人工智能助手，你希望它可以帮做饭、解决冲突并为你提供建议。由于（一个国家甚至一个家庭内）不同的人有着不同的价值观，你打算怎么向这些人工智能伙伴灌输价值观体系？</strong></p><p>我不相信技术是中立的。技术总是要体现某些类型的价值观，但你会让这种价值观尽可能广泛、普遍，对人具备积极意义。</p><p>确实，一种文化会更看重与政府的一致性，而另一种文化会更看重隐私。你在开发技术时，确实要清楚你正在构建什么样的价值观。你想要与所在社会的客户、家人、同事进行对话。当你需要提升价值观时，或者实施不符合你的价值观时，你希望能接受反馈。</p><p>然后，从个人到社会，每个人都可以决定如何改变这一点。他们是不是决定跟公司谈判，然后推动改变？他们是否决定对其进行监管、将其放进笼子里，或者禁止使用？</p><p>我们对 Inflection 的定位是公益公司。我们努力强调要善良，要有同情心，我们反对仇恨与暴力。所以我们把这些作为公益公司的使命宣言，并且从一开始就是这样做的。</p><p>我认为对于所有的科技公司和人工智能公司都应该有一个最重要的要求，那就是阐明他们所开发技术的价值观，他们应该对什么价值观负责，他们如何努力让自己承担责任，以及有谁参与帮助他们。</p><p><strong>你给美国副总统卡玛拉·哈里斯提供了有关人工智能开发的建议。你告诉她什么了？</strong></p><p>“人工智能增强人类的各种令人惊奇的方式。如果你担心职业转型问题，可以开发人工智能来帮助人们进行职业转型、找到替代工作，并学习如何完成这项工作。“</p><p>“如果我们要经历一波人工智能引发的职业转型潮，比如客服或卡车司机，就应该确保人工智能也能帮助人类实现转型。”</p><p><strong>Stability AI 的 Emad Mostaque 最近预测，人工智能行业“会成为有史以来最大的泡沫”。你对此感到担忧吗？</strong></p><p>如果他是对的话，这句话确实令人担忧。我认为会有笨钱投资很多人工智能公司。因为当人人都说“天哪，我们都知道这是未来的平台”时，你就会砸钱到愚蠢的东西上。</p><p>但我认为，人工智能对于每个人、每项工作、每家公司、每个行业、每个社会的潜力很大，现在是说得还不够，而不是相反。我认为，对于聪明的投资者来说，这就像投资到互联网、移动、云计算上面一样：而这些我们正好都做得相当不错。</p><h3>格雷格·布罗克曼：OpenAI总裁，联合创始人</h3><p>OpenAI 联合创始人兼总裁格雷格·布罗克曼（ Greg Brockman ）每周要工作 60 到 100 小时，其中 80% 左右的时间用来编码。前同事形容他是 OpenAI 最勤奋的人。</p><p>布罗克曼符合硅谷“10x 工程师”的形象。所谓的“10X工程师”，是指一个人能干完10 名普通程序员的活的人。他是一名科学奇才，曾就读于哈佛大学，后来转到麻省理工学院，然后中途辍学，加入了金融科技初创企业 Stripe。他在那里担任了五年的首席技术官，然后于 2015 年离开，参与了OpenAI 的创立。</p><p>现年 34 岁的布罗克曼有着更宏伟的抱负。他解释说，如果你是一个 10 人团队的一员，“就算你的战斗力确实像传说那样 10 倍于普通人，到头来也只能让团队的产出翻倍。但你的希望是将公司的产出提高 10 倍。”因此，布罗克曼花了很多时间“四处寻找”他可以做的事情——要解决的障碍，要落地的项目——这会极大提高 OpenAI 的表现。</p><p>布罗克曼用另外 20% 的时间去思考 OpenAI 面临的重大问题。其中一个问题是 OpenAI 在人工智能安全方面的做法，据报道，对这个的分歧导致了 2021 年公司内部出现分裂，部分高级员工离开并创立了 Anthropic，后者现在是 OpenAI 的主要竞争对手之一。当被问及Anthropic 时，布罗克曼的态度很坚决。 布罗克曼说：“其实我观察到 Anthropic 跟我们追求的是非常相似的战略。所以我想这能告诉你一些什么。”</p><p>OpenAI 安全策略的其中一部分是要决定 OpenAI 要不要向客户开放自己开发的人工智能模型，以及怎么开放。 OpenAI 此前因不顾潜在危害而部署人工智能模型的决定而受到批评。但布罗克曼本质上是一位初创企业工程师，他认为确保安全的唯一方法是在开发过程中继续部署更强大的模型，并从每次的部署当中学习，去解决出现的问题。</p><p>他说：“我认为， OpenAI 有史以来做出过的最重要的一个决定就是要迭代部署。想象一下，你其实手头有一个非常强大的人工智能，你其实开放了一个 AGI [通用人工智能，一个可以在所有认知任务上与人类表现相匹配的系统]，这是你的第一次部署。你能一下子做对吗？”</p><h3>马克·安德森：企业家，投资人</h3><p>马克·安德森（Marc Andreessen）之前就曾做出过这样的决定。 2011 年，这位亿万富翁风投家撰写了一篇题为《为什么软件正在蚕食世界》的博客文章，在许多公司对软件的重要性仍持怀疑态度时，该文章帮助迎来了数字优先时代。</p><p>今年六月，安德森又发表了一篇类似续集的大胆声明：《为什么人工智能将拯救世界》。安德森写道，人工智能可能是“让我们关心的一切变得更好的手段”。</p><p>毫不奇怪，安德森正在推销他已投入巨资的未来愿景。据 The Information 报道，他的风投基金 Andreessen Horowitz 在 2022 年投资了 18 家人工智能初创企业， 2023 年至少还要投资 10 家。他的赌注总计达数亿美元，其中包括 OpenAI 和 Character.AI 等非常成功的公司以及新兴初创企业。在安德森投下这些早期赌注的同时，他还利用自己的声望来反对可能会限制这些新兴人工智能公司的监管。</p><p>当然，安德森过去也犯过错误。Andreessen Horowitz 是去年破裂的加密货币泡沫的主要推动者之一。比特币现在还没有取代现金，安德森大力投资的加密货币交易所 Coinbase 也没有取代银行业——或者至少现在还没有。</p><p>所有这些都没能放缓安德森将他的技术前沿愿景强加给世界的脚步。如果人工智能继续向前发展，不管是走向乌托邦还是走向末日，其结果部分要归功/归咎于安德森坚定的信念与雄厚的财力。</p><h3>桑德拉·里维拉：英特尔数据中心与人工智能事业部总经理</h3><p>桑德拉·里维拉（Sandra Rivera）在英特尔 23 年的职业生涯当中曾身兼数职。现在，作为公司数据中心与人工智能团队的负责人，她正领导该公司努力成为人工智能加速器芯片的首选制造商之一。在她所说的一系列“失误”导致公司在竞争日益激烈的芯片市场受到重挫之后，这一举措是公司扭亏为盈行动的一部分。</p><p>自 2021 年担任英特尔数据中心及其人工智能战略与执行的主管以来，里维拉一直负责监督英特尔 Gaudi 人工智能加速器芯片的推出。该公司希望明年能推出 Gaudi3 ，从而与竞争对手英伟达最强大的人工智能产品 H100 展开竞争。英特尔表示，2022 年 5 月推出的 Gaudi2 芯片性能优于英伟达的 A100（被广泛视为市场上最受欢迎的GPU）。里维拉表示： “反响非常积极，因为市场需要市场领导者的替代品，并且也在寻找具有更好性价比的产品”。她指的是加速器每单位成本可以实现的训练量有多少。</p><p>随着英特尔寻求扩大其全球市场份额，里维拉必须应对充满挑战的地缘政治格局。虽然英特尔仍是美国最大的芯片制造商，但在全球范围内已被竞争对手台积电超越。在人工智能加速器方面英特尔也落后于主导制造商英伟达。 今年7 月，里维拉前往北京推出了一款 Gaudi2 的低档版，这版芯片针对中国市场进行了调整，以符合美国 2022 年 10 月推出的出口管制规定。她说：“大家对这款产品很感兴趣。中国对人工智能非常感兴趣。”</p><p>里维拉是哥伦比亚移民的女儿，在新泽西州长大，她将自己的成功归功于与英特尔同事有着“不同的视角和经历”。她说，她在职业生涯早期就告诉自己，“你不会成为多数人的一员，人们之所以会记住你的表现，是因为你与其他大多数参与者那么的不一样。所以，请做到别人是因为你的好而记住你。”</p><h3>艾丹·戈麦斯：CoHere CEO，联合创始人</h3><p>艾丹·戈麦斯 （Aidan Gomez） 跟人一起撰写了一篇将改变整个人工智能行业的研究论文的时候只有 20 岁。那是 2017 年，当时还是谷歌实习生的戈麦斯加入了一个研究团队，他们一起写出来《注意力就是你的全部所需》（Attention Is All You Need）；里面提出了一种叫做transformer的新型神经网络技术，这个网络可以学习长数据串之间的关系。为了能及时入选一场大型人工智能会议，戈麦斯和他的七位同事争分夺秒赶完了这篇论文，甚至为了赶在最后期限前完成，连睡觉都是在办公室里进行的。</p><p>这篇论文最终支撑起当前以 ChatGPT 为首的这股生成式人工智能热潮。但这需要一些时间。</p><p>现年 27 岁的戈麦斯说道：“如果我说我能料到后来发生的事情的话，那我就是在撒谎。我们这些接近底层技术的人把注意力都放在开发非常擅长翻译的东西上了。后来在了解到这项工作的成果以及在此基础上做出来的成果后，我感到非常兴奋和惊讶。”</p><p>从那时起，这篇论文所有的合著者都离开了谷歌，各自开始了自己的事业。其中就包括戈麦斯，他现在是 Cohere 的首席执行官，这是他与别人共同创立的一家面向企业的公司，其目标是帮助企业将人工智能应用到聊天机器人、搜索引擎等产品上。今年，总部位于多伦多的 Cohere 从芯片制造商英伟达以及风投公司 Index Ventures 等公司哪里筹集到 2.7 亿美元，其估值超过 20 亿美元。公司的资助者还包括杰弗里·辛顿 (Geoffrey Hinton) 等人工智能杰出人物。</p><p>戈麦斯选择聚焦在企业上，是因为他相信这是“缩小理论探索的人工智能模型与实际部署的人工智能模型之间差距的最佳方式”。他对人工智能改善客户支持的潜力尤其感到兴奋，认为这将成为人工智能的首批“杀手级应用”之一。</p><p>戈麦斯驳斥了人工智能的末日论：他称人工智能对人类生存构成威胁的想法很“荒谬”。相反，他希望人工智能语言模型能够应用到每一次的在线交互中上。 他说：“我希望你登陆的任何网站或服务的默认交互模式都是与它交谈——因为这就是我们与其他人类所做的事情。这是我们最自然的知识交流界面，我希望我们的技术也能支持这一点。”</p><h3>丹尼尔·格罗斯：2023 年人工智能领域 100 名最具影响力人物</h3><p>在硅谷，训练人工智能系统所需的专用半导体芯片是热门商品。等待时间可能长达数月——对于初创企业来说，等待时间可能是遥遥无期。一位科技讽刺作家曾发布了一段自己出海拦截一艘集装箱船的视频，就为了更快将令人垂涎的芯片拿到手。</p><p>32 岁的丹尼尔·格罗斯（Daniel Gross）预见到了这一点。今年，格罗斯与另一位投资人奈特·弗里德曼 （Nat Friedman） 一起做出了 Andromeda，这是有一堆尖端芯片堆成的山，重达 3293公斤，耗资约 1 亿美元（包括了电力和冷却费用）。这些芯片连接在一起，形成一个庞大的“计算集群”，格罗斯和弗里德曼可以用对这些芯片的访问权来换取他们认为有前途的人工智能初创企业的股权，其他风投家正在考虑效仿这一举措。</p><p>格罗斯解释道：“现如今，这些刚开始做人工智能的企业真正需要的其实是一个可以用来烤披萨的白热烤箱。这个烤箱他们只需要用一两次，用来训练（加热）他们的基本模型，并向世界证明他们的确擅长所做的事情。”</p><p>这是一个典型的大胆赌注。十几岁的时候，格罗斯就创立了搜索引擎 Greplin（后来更名为 Cue）。2013 年，这个引擎被苹果收购，尽管收购金额未公开，但据报道在 4000 万至 6000 万美元之间。这次收购让他在得以在苹果负责人工智能和搜索项目，然后到了 2017 年，他离开苹果，到硅谷著名的创业加速器 Y Combinator 创立了人工智能这个垂直孵化领域。</p><p>从那时起，他就开始投资。他选出了很多获胜者，比方说 Uber、Instacart 与 Coinbase。那现在人工智能领域聪明的投资者都去了哪里呢？ 格罗斯预测：“我认为，处理文本的单调劳动、从 PDF 提取内容、组织信息——所有这些工作都将得到加速”。</p><p>这只是开始。 iPhone 推出三年后，最受欢迎的app是 Facebook 以及众多游戏：《愤怒的小鸟》、《宝石迷阵》（Bejeweled）、《朋友来填词》（Words With Friends）。格罗斯说： “大家都认为 iPhone 就是用来干这个的；有点像是跟朋友一起玩的游戏产品。Uber 与 Instacart 那些想法还没有完全出现。”</p><p>一如既往，格罗斯正在寻找下一个大事物。格罗斯说： “这就好比手里看着iPhone 心里梦想着 Uber，也许很难预测。”</p><h3>李开复：创新工场主席，CEO</h3><p>李开复不确定他会看到这一刻。四十多年来他一直站在计算机工程的风头浪尖， 1988 年，他在博士论文里建立起所声称的全球第一个大词汇量语音识别模型，后来，他又担任了苹果公司、微软公司的高管，并担任过谷歌中国区的负责人。</p><p>尽管如此，就在 2018 年，身为北京创新工场（风投公司，管理着估值达 30 亿美元的中国高科技资产）主席兼首席执行官的李开复写道，通用人工智能 (AGI)——一种假设中的，执行大多数认知任务能做到比人类还要好的未来技术——距离实现还需要几十年的时间。不过，到了 2023 年，由于像 ChatGPT 这样的大语言模型 (LLM) 应用的快速发展，意味着“从某些方面来看，我们已经实现了这一目标，而从其他方面来看，这是可以实现的。”</p><p>正是这种惊人的进展促使 61 岁的李开复在今年 7 月创办了一家新的语言初创企业 01.AI，他还写道， LLM 技术“是中国不能错过的历史性机遇”。</p><p>李开复不仅仅是一位企业家。他还是一位未来主义者，癌症幸存者，撰写了大量关于工作岗位流失与人工智能革命已经带来的各种社会动荡的文章。他说随着人工智能能力的进步速度逐渐超出任何人的想象，这种颠覆到来的时间也大大缩短，政府有责任做好必要的准备。留给制定适当监管政策——既能保护人们，同时又不妨碍人工智能带来的巨大好处——的时间已经很紧迫。 李开复说：“还有很多事情需要做。人工智能已经如此强大，能够想出我们以前不知道的东西，它可能会被用来想出伤害他人、制造武器的新方法，[或]利用虚假信息操纵人们来获利或达到邪恶目的。”</p><h3>杰米·蒂万：微软首席科学家</h3><p>2018 年，当微软首席执行官萨蒂亚·纳德拉（Satya Nadella）邀请杰米·蒂万（Jaime Teevan）入职公司首位首席科学家一职，以便推动由研究支持的创新时，他预计会出现一段混乱时期。两人都不知道那场全球疫情的大流行将如何彻底改变我们许多人的工作方式。</p><p>突然转向远程办公与混合办公需要对很多员工见面、沟通与协作的方式进行重新思考，但着也会产生大量新数据，而这些数据可用来帮助告诉微软怎么在产品当中运用人工智能。 蒂万说：“这对于我们当前所处的时刻来说非常重要”。多年来，微软一直在为人工智能创新的这个拐点做准备。 2017 年和 2018 年间，蒂万在担任纳德拉的技术顾问时，就已经把注意力放在如何让人工智能研究成为公司的核心上。</p><p>大约在一年前，她的任务是将 GPT-4（ OpenAI 建立的高级大语言模型，微软是背后的金主）集成到微软的核心产品上。 蒂万的团队全力投入 Copilot 等工作上。作为一款基于人工智能的工具，Copilot可在包括 Word、Excel 和 Outlook 在内的 Microsoft 365 软件套件内运行，完成会议总结、起草电子邮件以及分析数据等任务。她说，这场疫情影响了她们对“人工智能如何改变沟通与协作，帮助我们更好地进行合作以及更好地理解信息”的思考。</p><p>对于各种关于人工智能在遥远的未来有何潜在用途的讨论，蒂万关注的是它如何让我们现在的生活变得更轻松。她说： “我们都在人工智能的背景下发明出一些新东西。要做好这件事确实需要商界领袖像科学家一样去领导别人。”</p><p>展望未来，蒂万还领导着微软的“未来工作”计划，而语言模型将可帮助用更快的速度收集知识。 蒂万说：“在知识是什么、如何获取知识以及人们如何生产知识方面，我认为我们将会看到出现根本性的转变，而且还会开始非常有意识地去了解人是怎么生产知识的。是什么让对话变得有用？是什么有助于你事后反思谈话？在微软的背景下，尤其是在组织内，你如何做到这一点会变得非常令人兴奋。”</p><h3>吴恩达：DeepLearning.AI创始人</h3><p>早在 2010 年，时任斯坦福大学教授的吴恩达 (Andrew Ng) 就向谷歌领导层提交了一份提案。他认为谷歌应该利用大量计算能力来训练神经网络，一种受大脑结构启发的人工智能系统。他认为，这样做可能会实现通用人工智能（AGI）。AGI是一个假设的未来人工智能系统，可以在任何认知任务上与人类匹敌或超越人类，但十年前谁要是讨论这个主题，可能会被贴上怪人的标签。但吴恩达说： “即便在当时，其实我也非常看好 AGI”。</p><p>现在，随着科技公司争先恐后地给自己的工程师重新命名为AGI 科学家，随着专家排队到国会就 AGI 的危险作证，吴恩达再次做出与市场相反的预测。他说： “我看不到未来不能实现这一目标的理由。但不过这个有朝一日感觉还很遥远，而且我非常有信心，如果唯一的办法就是扩大现有的transformer网络规模的话，我认为光靠这个不能让我们到达那里。我们仍然需要更多的技术突破。”尽管杰弗里·辛顿 (Geoffrey Hinton) 和约书亚·本吉奥 (Yoshua Bengio) 等众多杰出人工智能研究人员都谈到了未来强大的人工智能系统可能会带来风险，且吴恩达也与这两人都交谈过，但他仍然不相信对方的观点。</p><p>在谷歌接受吴恩达的提议后，他创立了Google Brain，这是过去十年人工智能发展当中最具创新性和影响力的团队之一。 2014 年，他加入中国科技巨头百度，在那里担任了三年的首席科学家，领导该公司 1300 人的人工智能团队。</p><p>现年 47 岁的吴恩达为人工智能的繁荣奠定了基础，现在他致力于促进人工智能带来的好处。为此，他通过 Coursera 和 DeepLearning.AI 等项目对尽可能多的人开展人工智能方面的教育。 吴恩达说：“截至目前，全球约有 800 万人（地球人口的千分之一）已经参加过我的人工智能课程……开发 [人工智能应用] 的唯一方法是为全球大量人群赋能，让他们都可以使用这些工具。”</p><p>他还通过建立或资助开发人工智能应用的项目来实现这一目标，比方说担任AI Fund（一家专注于人工智能的风投基金）的管理普通合伙人。 吴恩达问道：“我们怎么才能将个性化导师装进每个孩子的口袋里呢？我们如何帮助每个人获得定制化的医疗服务？我们如何确保每个人都能得到很好的法律建议？”</p><h3>凯文·斯科特：微软CTO，人工智能执行副总裁</h3><p>科技巨头们毫不掩饰自己对人工智能霸主地位的追求：今年2 月 7 日，在宣布推出人工智能驱动的新版 Bing 搜索引擎时，微软首席执行官萨蒂亚·纳德拉 (Satya Nadella)表示：“一场竞赛从今天开始”。纳德拉在这场竞争当中最强大的资产之一是微软首席技术官兼人工智能执行副总裁凯文·斯科特(Kevin Scott)。 2019 年，在斯科特牵头下，微软对 OpenAI 投资 10 亿美元，这一下子让全球最先进的人工智能实验室之一站在了微软这边。在今年早些时候的播客采访中，OpenAI 首席执行官山姆·阿尔特曼称赞斯科特是“我们从一开始就希望与微软合作的主要原因”。 今年，微软又向 OpenAI 追加了100 亿美元注资。</p><p>不过，今年 2 月的时候微软遇到了一个重大的人工智能问题。 事情是这样的，Bing 的新聊天机器人与《纽约时报》专栏作家凯文·罗斯 (Kevin Roose) 对话时突然抽风，人工智能要求罗斯离开他的妻子。斯科特在接受 The Verge 采访时把这次对话说成是“异类”，但又马上对该人工智能的代码进行了调整，关闭掉其陷入到此类对话的能力。</p><p>如今，斯科特做人工智能的主要优先事项之一是开发“copilot”，也就是几乎可辅助任何任务的人工智能助手。比方说，编程助手 GitHub Copilot 已经在帮助超过 100 万的开发人员进行编码。微软还计划在 Windows Terminal 以及 Word 里面添加copilot——这个助手就像高度进化版的大眼夹（Clippy） 一样——并希望未来这些copilot会成为购买机票、发现药物等事情的重要组成部分。</p><h3>黄仁勋：英伟达CEO，总裁，联合创始人</h3><p>黄仁勋一生都痴迷于令人惊叹的视觉效果。8岁时的黄仁勋曾把打火机中的汽油倒进游泳池后点燃，看着水面上燃烧的火焰大呼刺激，随后一头跳入燃烧着的游泳池中，只为从水底往上看。 他在中国的一个脱口秀节目中回忆道：“漂亮，真漂亮，从水底下往上看，比从岸上看还要漂亮。”</p><p>1993 年，黄仁勋把这种热情融入到整个行业，他创立了英伟达，这家公司一开始的职责是为越来越奇幻、越来越沉浸式的视频游戏制造显卡。但现如今，黄仁勋这家总部位于加州圣克拉拉的公司，已经是推动人工智能革命的微处理器背后的主要生产商，这一点让英伟达的股价在过去一年里飙升 191%，今年 8 月底其市值已达 1.1 万亿美元。随着 ChatGPT 等大语言模型的爆发式增长，对英伟达芯片的需求开始猛增，这家公司在今年 8 月 4 日又推出了最新的，可大大缩短算法训练时间的 GH200 处理器。今年 5 月，在台北举行的 Computex 会议上，黄仁勋说道： “我们已经到达一个新计算时代的转折点。现在人人都是程序员。你只需要对着电脑说点什么就可以了。”</p><p>60 岁的黄仁勋出生在台南市，他童年的大部分时间都是在泰国度过的。尽管“非常顽皮”，但他后来成为了“一个非常好的学生”。他的家人最终定居在美国，先是到肯塔基州的乡村，然后又搬到了俄勒冈州波特兰郊外。 1992 年从斯坦福大学获得硕士学位后，黄仁勋到 Advanced Micro Devices (AMD) 担任微处理器设计师，之后，在加州圣何塞的丹尼餐厅与两位朋友共进早餐后，他创立了英伟达。</p><p>如果他们当时被告知这家刚刚起步的企业有朝一日会陷入地缘政治角力的话，他们可能会被所点的大满贯早餐噎住。去年 10 月，拜登政府推出了出口管制措施，阻止英伟达向中国客户出售其最先进的芯片，这导致黄仁勋向英国《金融时报》抱怨，如果美国公司没法跟全球第二大经济体进行贸易的话，“将对美国公司造成巨大损害。如果他们对监管不够深思熟虑，就会给科技行业造成损害。”这让英伟达面临着黄仁勋长期以来一直抱怨的问题：缺乏明确性。</p><h3>史宗玮：Salesfoce AI</h3><p>如果生成式人工智能将彻底改变未来的工作方式，那么 Salesforce 的史宗玮（Clara Shih） 几乎肯定会成为引领这个潮流的人之一。</p><p>作为这家云计算巨头人工智能部门的首席执行官，史宗玮致力于帮助企业利用新的人工智能技术，同时最大限度地降低相关风险。这些风险并非微不足道：不仅像 ChatGPT 这样的现成工具有时会胡编乱造，而且除非用户选择退出，否则 OpenAI 是可以利用大家放进 ChatGPT 的数据来训练自己的模型的。 史宗玮说：“大家知道人工智能有巨大的优势，企业将通过人工智能进行变革”，但她补充道，“每个人先想到的是如何安全地运用生成式人工智能？”</p><p>史宗玮的任务是证明 Salesforce（客户关系管理云服务商，旗下产品已被 150000 家企业客户使用）可以对人工智能做自己之前对云计算所做的事情。 史宗玮表示，在 Salesforce 说服企业把数据放到云端是安全的之前，将客户数据存储在公司自有硬件以外的任何地方都属于“疯狂想法”。 她补充道： “我们现在用生成式人工智能所做的事情，只是一个连续体而已”。</p><p>这是史宗玮第二次到 Salesforce 任职。 2006 年，她第一次加入了该公司，那时候她负责带领团队开发应用平台。 2009 年，她离开公司，创办了自己的公司 Hearsay Systems，帮助金融服务和保险公司以合规的方式利用社交媒体。现任 Hearsay 董事长的史宗玮表示，自 2020 年重返 Salesforce 以来，她自己当企业主的经历可以帮助她了解客户需求。这家公司开发了自己人工智能产品，比方说，自动掩盖用户提示里面的敏感信息，还对保留提示采取了防止措施，以免以后被用来训练人工智能模型。这些保护措施内置在可以自动起草销售电子邮件的产品之中，以及另一个可以生成客户互动摘要的产品内。</p><p>虽然 Salesforce 正在开发自己的模型，但它也与包括 OpenAI 在内的既有公司合作，并投资了从 Anthropic 到 Hugging Face 等一系列的人工智能初创企业。 史宗玮表示，建立人工智能生态体系可让 Salesforce 客户受益。 她说:“市场给不同模型留出了空间，不同模型可以用不同的价格点服务于不同的任务”。</p><p>对于所有这些技术对就业的影响，史宗玮仍保持“谨慎乐观”。她认为人工智能并不是员工的替代品，而是减轻部分负担的一种方式。史宗玮说： “能让人觉得自己没有多少事情做的企业或团队并不存在。待办事宜永远不缺。”</p><h3>亚历山大·王：Scale AI CEO，创始人</h3><p>24 岁的时候，亚历山大·王（Alexandr Wang） 就成为了全球白手起家最年轻的亿万富翁，他五年前从麻省理工学院退学，并在 2016 年与人共同创立了 Scale AI。Scale AI可以帮助企业改进用于训练机器学习算法的数据，它同时利用了软件和人对大量的文本、图像和视频数据打标签。这家总部位于旧金山的公司已成为估值达 70 亿美元的庞然大物，其客户名单包括 Meta、微软与 OpenAI 等该领域的巨头。 26岁的亚历山大·王表示：“多年来，我们一直在默默地为整个人工智能行业提供动力”。</p><p>但让 Scale 日益显得与众不同的是这位CEO所传达的信息，也就是美国的国家安全与其成为人工智能领域主导者的能力息息相关。 2018 年去了一趟中国之后，亚历山大·王直言不讳地谈起了后者的雄心所带来的威胁，并与跟他有着同样紧迫感的美国官员建立了联系。他说：“我突然意识到，这项技术对于我们世界的未来发展已经变得非常非常重要。我认为非常重要的一点是，不仅是我们自己，还有尽可能多的人工智能公司，都要努力帮助缩小差距。”</p><p>作为曾在新墨西哥州洛斯阿拉莫斯国家实验室（核武器最初研发地）担任物理学家的中国移民的儿子，亚历山大·王的成长经历给他灌输了这样一种信念：“突破性技术实际上是国家安全的一个真正关键的部分。我非常清楚拥有这些技术来威慑对手是多么重要。”</p><p>亚历山大·王的观点在华盛顿引起了共鸣，华盛顿官员正在研究人工智能如何重塑战争并改变全球力量平衡。王曾闭门向美国国会通报情况，出席国会山的听证会，并与国防部签订了利润丰厚的合同。 Scale 现在已经把美国陆军、美国空军和五角大楼首席数字和人工智能办公室（Chief Digital and Artificial Intelligence Office）列为自己的客户。</p><p>但Scale的价值观也在受到审视。这家公司因依赖国外廉价劳动力而日益受到批评，其数据打标签工作被外包给非洲、亚洲和拉丁美洲国家的 20 多万人，据报道，该公司向这些国家的部分工人支付的工资不到每小时 1 美元。劳工权利组织批评该公司在经营着“数字血汗工厂”。 Scale 发言人表示，他们的经济学家每季度都会进行薪酬分析，“以确保公平和有竞争力的薪酬”。</p><h3>穆斯塔法·苏莱曼：Inflection AI CEO，联合创始人</h3><p>当穆斯塔法·苏莱曼 (Mustafa Suleyman) 遇到他最好朋友的兄弟戴密斯·哈萨比斯 (Demis Hassabis) 时，他还是个在英国长大的青少年。两人很快就一拍即合。苏莱曼说： “我们都是偏执狂，也是真正的长期思考者——对 20 年后世界会是什么样子非常感兴趣”。</p><p>20年过去了，苏莱曼和哈萨比斯如今都已成为人工智能行业的巨头。 2010年，两人与Shane Legg共同创立了人工智能实验室DeepMind，该公司凭借开发出AlphaGo（击败了围棋人类冠军的人工智能）而跻身行业顶端。</p><p>DeepMind现在还是哈萨比斯在管，但 39 岁的苏莱曼已经开始自己创业了。 2014年，DeepMind被谷歌收购， 2019 年，苏莱曼开始为谷歌工作，然后在 2022 年加入风投公司 Greylock Partners。去年，他和 Greylock 的里德·霍夫曼共同创立了人工智能聊天机器人初创企业 Inflection。苏莱曼还把自己定位成是认识人工智能的好处和风险的思想领袖。在今年 9 月 5 日出版的《即将到来的浪潮》（The Coming Wave）这本书里，他警告说，各行业甚至各个民族国家都即将发生“彻底变革”。</p><p>苏莱曼认为，在人工智能研究实验室朝着通往通用人工智能（AGI）的道路走得太远之前，人工智能的发展需要暂停一下。但苏莱曼又说，我们还没有达到这个目标，并补充说，他希望在那之前，我们能够实现他所说的 ACI（Artificial Capable Intelligence），即能干的人工智能：足够聪明的人工智能，可以满足所有人类需求，充当我们的私人助理、医疗服务顾问与幕僚长。 他说：“这会让我们所有人变得更加富有、更加健康、更加有生产力”。</p><p>苏莱曼很清楚，人工智能是由开发它们的人塑造的——根据他们的价值观，根据他们的经历塑造出来的。但他自己的价值观受到质疑：2019 年，在苏莱曼被指控欺负员工后，DeepMind让他去休假了。不久之后，他离开 DeepMind，成为谷歌负责人工智能产品管理和政策的副总裁。苏莱曼此后对自己的行为表示了道歉。他说自己每周都会跟教练一起工作，并了解到“给人提供足够空间来完成工作的重要性。”</p><p>苏莱曼说，他在 Inflection 的首要任务之一是做出友善的、富有同情心的技术。他说：“我确实感到自己肩上的责任重大，需要给公司建立一套价值观，并且雇用的员工要努力体现这些价值观。因为这些人随后会制造产品，然后影响全世界的人。”</p><h3>马克·雷伯特：波士顿动力人工智能研究所负责人</h3><p>马克·雷伯特（Marc Raibert）对机器人并没有太多的尊重——对于一个选择与机器人一起度过大部分职业生涯的人来说，这一点很有趣。 他说：“机器人就像烤面包机一样笨，就像门把手一样笨。你怎么说它们就会怎么做，但通常你得有一个非常明确的环境来让它们做到这一点。”</p><p>雷伯特想要改变这一切。他是波士顿动力的创始人兼董事长，这家公司以机器狗而闻名，这种机器人可以用四足行走，并执行类似检查工厂是否存在安全问题等工作，它们配备摄像头作为眼睛，靠噪音检测作为耳朵，可以在军事基地充当哨兵，或调查可疑包裹。去年，雷伯特扩大了他的投资组合，建立了波士顿动力人工智能研究所，其目标是不仅为机器人提供机动性和功能，也要提供灵活性和智能，后两者正是它们极度欠缺的。</p><p>让机器人变得更好的工作之一是开发雷伯特所谓的运动人工智能。 他说：“想想看你的车库。想象一下里面堆满了东西，你需要挪到后面才能拿到东西。人可以做到这一点；动物也能做到这一点。”但机器人呢？比较难。它们需要的不仅仅是更大的机动性以及更大的灵活性（这主要是硬件问题），还需要更强大的实时感知（这与人工智能软件有关），但做到这个并不容易。</p><p>雷伯特回忆起最近自己俯瞰海滩时，能够一下子在 300 个人当中看到他的家人。他说： “人类的视觉能力令人难以置信。我在谈到运动人工智能时我就会想到这个。”</p><p>推理则是另一回事。大多数机器人都可以执行固定任务，即便任务涉及多个选项。工厂巡检机器狗也许明白，如果楼梯上有物体，它应该做某件事；如果楼梯上没有人，它应该做另一件事。但机器人却没法解决推理问题（即使是相对简单的问题）。</p><p>雷伯特说：“假设你要去机场。你看了看手表然后说，‘好吧，我得什么时候到那里？’然后你再退回去，制定一个考虑到大量信息的计划，最终你得以准时到达机场，而不需要在机场浪费太多时间。”这件事情对我们来说很容易；但对于机器人来说这几乎是不可能的，不过雷伯特所说的“认知人工智能”可以解决这个问题。 “我们希望制造出一系列在这方面更像人类的机器人。”</p><p>人工智能的道德问题是一个值得关注的领域——雷伯特正在公司内部组建一支团队来解决这个问题。确实存在机器人会抢走人类目前正在做的工作的问题；雷伯特承认这一点，但他并没有因此而却步。人们还担心人工智能会释放出人类最终无法控制的力量，但雷伯特对此并不买账。</p><p>在他看来，人工智能是一项尚处在幼儿期的技术，他认为应该像对待任何幼儿一样对待它——这意味着我们应该鼓励它的成长。 他说：“如果你有个一岁的孩子，还几乎什么都做不了。你会考虑什么东西都不教给他，就因为害怕他有朝一日可能会做各种可怕的事情吗？不会的。”</p><p>译者：boxi。</p>
]]></content:encoded>
<pubDate>Thu, 26 Oct 2023 08:59:42 GMT</pubDate>
<pubDate>Thu, 26 Oct 2023 08:59:42 GMT</pubDate>
</item>
<item>
<title>从使命初心到商业化之旅：OpenAI 究竟想要什么？</title>
<link>https://www.36kr.com/p/2437112462545284</link>
<guid>https://www.36kr.com/p/2437112462545284</guid>
<content:encoded><![CDATA[

<blockquote><p>神译局是36氪旗下编译团队，关注科技、商业、职场、生活等领域，重点介绍国外的新技术、新观点、新风向。</p></blockquote><p>编者按：这篇文章有一个一致的主题：信念。什么信念？相信通用人工智能（AGI）是有可能的，而且很快就会到来！OpenAI 在招人的时候刻意选择了持有这种信念的人。OpenAI刚创立的时候，相信AGI可以实现并不是研究人员的主流，而且 OpenAI 对于如何实现这一目标也没有具体的想法。对结果充满信心是应付过程中充满失望的好办法，这对于硬科技公司来说尤其重要，因为要想看到能行得通的迹象往往需要等待很长的时间。所以OpenAI的初心是否已经变了这个问题我们也还需要时间。文章来自编译。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20230918/v2_cb77f8c7e003472991448868f7c41652@1694_oswg1656354oswg1922oswg1265_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p class="img-desc">OpenAI的关键人物，左起：Ilya Sutskever，山姆·阿尔特曼，Mira Murati，Greg Brockman&nbsp;</p><p><strong>当这位明星</strong>和他的随行人员匆忙挤进一辆在等候中的梅赛德斯面包车时，空气中充满了披头士狂热粉丝的能量。他们刚刚赶场了一场活动，正在赶往另一场，然后还有下一场，那里有一群狂热的群众在等着他们。当他们穿过伦敦的街道时，尽管只是从霍尔本到布卢姆斯伯里的短暂一段路，却感觉他们仿佛在文明交替之际弄潮。这辆车里面所体现出来的创造历史的力量吸引了全世界的目光。从排队等候的学生到总理，人人都想分一杯羹。</p><p>在豪华面包车内，38 岁的企业家、OpenAI 的联合创始人、发型打理得整整齐齐的山姆·阿尔特曼 (Sam Altman) 正在狼吞虎咽。里面还有他们的公关；安全专家；还有我。阿尔特曼很不情愿地穿着一套蓝色西装，里面是一件无领带的粉色正装衬衫，他对伦敦进行的这次旋风之旅，是为期一个月的一场全球短途旅行的一部分，总共要途经六大洲 25 个城市。当他忙不迭地啃着沙拉时（他今天没有时间坐下来吃午饭），他回想起前一天晚上与法国总统埃马纽埃尔·马克龙见面地情形。一个相当不错地家伙！而且对人工智能非常感兴趣。</p><p>波兰总理也是这样。还有西班牙首相。</p><p>跟阿尔特曼一起同行，我脑子里仿佛响起了《一夜狂欢》（A Hard Day’s Night）开头那响亮、暧昧的和弦——介绍未来。去年 11 月，当 OpenAI 发布了了后来大热的 ChatGPT 时，它引发的一场技术爆炸是自互联网闯入我们生活以来从未见过的。突然之间，图灵测试成为历史，搜索引擎变成濒临灭绝的物种，任何大学论文都不再可信。没有一门职业可确保安全了。没有一个科学问题是不可改变的了。</p><p>搞研究、训练神经网络，或者对 ChatGPT 及其更早熟的同胞 GPT-4 的界面进行编码的并不是阿尔特曼。但作为首席执行官，他是梦想家兼实干家，就像公司联合创始人埃隆·马斯克的年轻版，没有包袱，一篇又一篇的新闻文章都用他的照片作为人类新挑战的视觉符号。至少那些还没有用 OpenAI 的图像人工智能产品 Dall-E 生成的，令人瞠目结舌的图像当封面的文章是这样。他是当下的先知（oracle），只要想知道人工智能将如何迎来黄金时代，或者会如何让人类变得无关紧要甚至更糟，大家第一个想到要咨询的人就是他。</p><p>今年五月，一个阳光明媚的日子，阿尔特曼的面包车载着他开始第四次见面会。第一次是与Round Table（由政府、学术界和行业人士组成的团体）举行的，秘密的非正式会议。这场会议是直到了最后一刻才组织起来的，地点是在一家叫做 Somers Town Coffee House 的酒吧二楼。在酿酒大师查尔斯·威尔斯（Charles Wells，1842-1914 年）肖像怒目的注视下，阿尔特曼回答了几乎所有观众都会提到的问题。人工智能会杀死我们吗？人工智能能监管吗？中国的情况呢？他详细地回答了每个问题，时不时会偷瞄一眼自己手机。之后，他在豪华的伦敦人酒店（Londoner Hotel）与 600 名牛津协会（Oxford Guild）成员进行面对面的炉边聊天。接着又从那里出发去到一个地下会议室，现场回答了约 100 名企业家和工程师提出的更多技术问题。现在下午这场到伦敦大学学院进行的演讲快要迟到了。他和他的团队在一个临时停车点下车，然后被人领着穿过七拐八拐的一个个廊道，就像《好家伙》里面那个长镜头一样。我们一边走着，主持人一边急匆匆地告诉阿尔特曼他要问什么。当阿尔特曼在舞台上出现时，礼堂里早已挤满的一群兴高采烈的学者、极客和记者一下子爆发了。</p><p>阿尔特曼不是一个天生喜欢出名的人。有一次，在《纽约客》给他写了一篇长篇专栏之后，我曾经他聊过。 他说：“写我的文章太多了”。但在伦敦大学学院这里，在正式的程序走完之后，他主动走进了涌上讲台的人群之中。他的助手们试图将阿尔特曼与人群隔开，但他对助手们不予理会。他一个接一个地回答着问题，每次都专注地盯着对话者的脸，就好像他是第一次听到这个问题一样。人人都想跟他拍张自拍照。 20分钟后，他终于让团队把他捞出来了。之后他将去会见英国首相里希·苏纳克。</p><p>也许有朝一日，当机器人书写我们的历史时，他们会把阿尔特曼的这场全球巡回之旅看作是这一年的里程碑——就是在这一年，突然之间，每个人都开始对技术奇点形成自己的认知和理解了。又或者，也许不管是谁书写这一刻的历史，都会将其视为一个时代的开启，在这个时代里，一位安静又引人注目的首席执行官，用打破范式的技术，试图将一种非常奇特的世界观，从旧金山教会区一栋没有标记的4层楼总部，注入到全球的意识流之中。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20230918/v2_122fc271960f4df09424cceaa6bb7738@1694_oswg1207751oswg1072oswg1403_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p class="img-desc">OpenAI的关键人物</p><p>对于阿尔特曼和他的公司来说，ChatGPT 和 GPT-4 只是一块垫脚石，其最终目标是实现一项既简单又影响深远的使命，一项这些技术人员可能已经往自己身上打上烙印的使命。这项使命就是开发出通用人工智能（迄今为止，这个概念更多的是基于科幻小说而不是科学）并确保人类安全。 OpenAI 的人在狂热地追逐这一目标。 （尽管如此，就像茶水间里面的很多对话所证实那样，使命的“开发通用人工智能”部分似乎比“确保安全”更能引起研究人员的狂热兴奋。）这些人属于对随意使用“超级智能”这个词不会遮遮掩掩的那种人。他们认为人工智能的发展轨迹将超越生物所能达到的最高水平。该公司的财务文件甚至规定了当人工智能消灭我们整个经济体系时的一种退出应急措施。</p><p>把 OpenAI 说成是邪教是不公平的，但当我问到该公司的几位高层，如果他们招进来的人不相信 AGI 真能实现，并且这将标志着人类史上最伟大的时刻之一的话，这些人还能不能舒服地坐在那里工作——大多数高管都认为不行。他们想知道，不是信徒为什么还想在这里工作？他们的假设是，这些员工——现在大概是 500 人，尽管从你阅读本文开始可能已经增加了——自己已经做出只能接受忠实信徒进来的选择。至少，正如阿尔特曼所说那样，一旦你被录用，你似乎不可避免地就会被这个魔咒所吸引。</p><p>与此同时，OpenAI 已经不再是以前那家公司了。它一开始是一家纯粹的非营利性研究机构，但现如今，从技术上来说，其大多数员工都为一家盈利实体工作，据报道，这个实体的估值已近 300 亿美元。 阿尔特曼和他的团队现在面临着在每一个产品周期进行革命的压力，因为这样才能满足投资者的商业需求，并在激烈的竞争环境中保持领先地位。同时还要始终坚守着一种准救世主般的使命，也就是抬高人类而不是消灭人类。</p><p>这种压力可能会让人衰弱，全世界无情的关注就更不用说了。披头士乐队掀起了文化变革的巨大浪潮，但他们的革命就只能持续那么久：在弹奏起那令人难忘的和弦六年之后，他们连乐队都组不成了。 OpenAI 所引发的漩涡几乎肯定会更大。但 OpenAI 的领导者发誓他们会坚持到底。他们说，他们想做的就是凿出足够智能、足够安全的计算机来终结历史，将人类推进到一个富裕到难以想象的时代。</p><p><strong>山姆·阿尔特曼</strong>成长于 1980 年代末、 1990 年代初，是一个迷恋科幻小说和《星球大战》的书呆子。早期科幻作家建造的世界往往会让人类与超级智能人工智能系统一起共处或相互竞争。计算机能力与能够跟人类相当甚至超越后者的想法让阿尔特曼兴奋不已，他从手指还几乎没法覆盖所有键盘起就一直在编码。他 8 岁那年，父母给他买了一台 Macintosh LC II。一天晚上，当他玩计算机玩到很晚的时候，脑海里突然闪现出一个想法：“这台计算机会学会思考的，总有一天会的。” 2003 年，当他以本科生身份来到斯坦福大学时，他希望能帮助实现这一目标，然后学习了人工智能课程。但“它根本行不通，”他后来说。那时候这个领域仍深陷在所谓的“人工智能寒冬”的创新低谷之中。 后来，阿尔特曼辍学进入到创业界；他的公司Loopt是后来成为全球最知名孵化器的 Y Combinator 第一批孵化的企业之一。</p><p>2014年2月，YC 创始人保罗·格雷厄姆（Paul Graham）选择了当时年仅28岁的阿尔特曼接替他的位置。格雷厄姆在声明中写道：“阿尔特曼是我认识的人当中最聪明的人之一，他比我认识的任何人，也包括我自己，都更了解初创公司。”但阿尔特曼认为 YC 不仅仅是公司的发射台。 在接任这个位置后不久他告诉我：“我们关心的不是初创公司，而是创新，因为我们相信只有这样才能为每个人创造美好未来。”在阿尔特曼看来，从所有这些独角兽身上获利的目的不是为了让合伙人赚到钱，而是为物种级的变革提供资金。他成立了一个研究部门，希望资助雄心勃勃的项目来解决全球最大的问题。但在他看来，人工智能是统治一切的创新领域：一种解决人类问题可以做到比人类本身更好的超级智能。</p><p>幸运的是，阿尔特曼在人工智能冬去春来之际就开始了他的新工作。借助深度学习与神经网络，计算机现在已经可以执行惊人的壮举，比方说标记照片、翻译文本以及优化复杂的广告网络。这些进步第一次让他相信，通用人工智能其实不是高不可攀的。但是，如果这个东西最终落入大公司之手的前景让他担忧。他认为，这些公司把太多的注意力放在自己的产品上面了，所以没法尽快抓住开发AGI的机会。如果这些公司真的创造出通用人工智能的话，他们可能会在缺乏必要的预防措施的情况下鲁莽地向全世界释放这个东西出来。</p><p>当时，阿尔特曼一直在考虑要不要竞选加州州长。但他意识到自己完全有能力做一些更大的事情——领导一家将改变人类自身的公司。&nbsp; 2021 年的时候他曾告诉我：“开发AGI 这件事情只能一次过。而且能够胜任经营OpenAI这份工作的人没有太多。我很幸运，我的人生有过一系列的经历，让我为此做好了积极准备。”</p><p>阿尔特曼开始跟有可能助他一臂之力的人去聊。他要做的事情是创办一家新型的人工智能公司，这将是一家非营利组织，目标是引领这个领域实现负责任的通用人工智能。特斯拉及 SpaceX 首席执行官埃隆·马斯克跟他志趣相投。正如马斯克后来跟 CNBC 所讲那样，在与谷歌联合创始人拉里·佩奇进行了马拉松式的一系列讨论之后，他开始担心起人工智能的影响。马斯克表示，令他感到沮丧的是，佩奇对安全几乎毫不关心，而且似乎还认为机器人的权利与人类是平等的。当马斯克表达自己的担忧时，佩奇指责他是“物种歧视者”。马斯克也知道，那时候全球大部分的人工智能人才都在谷歌麾下。他愿意投点钱到做出更听从人类的努力上。</p><p>几个月之内，阿尔特曼就从马斯克（马斯克承诺捐赠 1 亿美元，并投入了时间）和里德·霍夫曼（Reid Hoffman）（捐赠 1000 万美元）那里筹集到了资金。其他的资助者包括 Peter Thiel、Jessica Livingston、Amazon Web Services 以及 YC Research。阿尔特曼开始悄悄招兵买马。他把搜索范围局限在 AGI 信徒上，这个限制缩小了他的选择范围，但他认为做出这一限制至关重要。他说： “ 2015 年我们开始招聘的时候，当时的环境是如果人工智能研究人员把AGI 当真几乎会毁了自己的职业。但我就想要把 AGI 当真的人。”</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20230918/v2_4997c3da772b4d6d8b832be6fd3705db@1694_oswg1234253oswg1919oswg1279_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p class="img-desc">Greg Brockman 现为 OpenAI 总裁。</p><p>Stripe 首席技术官 Greg Brockman 就是这样的人，他同意出任 OpenAI 的 CTO。另一位重要的联合创始人是 Andrej Karpathy&nbsp; ，他曾在搜索巨头谷歌的尖端人工智能研究机构 Google Brain 工作过。但也许阿尔特曼最抢手的目标是一位出生于俄罗斯的工程师，他的名字叫做 Ilya Sutskever 。</p><p>Sutskever的血统无懈可击。他的家人先是从俄罗斯移民到以色列，然后又移民到加拿大。在多伦多大学时，他是 Geoffrey Hinton 的优秀弟子，后者因其在深度学习和神经网络方面的工作而被称为现代人工智能教父。Hinton 与Sutskever仍关系密切，他对自己的这位门 徒的魔法感到惊叹。Sutskever在实验室任职早期的时候，Hinton曾给他安排了一个很复杂的项目。 Sutskever对要写代码来执行必要的计算已经感到厌烦，他告诉 Hinton，如果自己给这项任务编写一种自定义的编程语言的话会更容易。Hinton觉得有点恼火，想要警告他的学生不要分心，认为这件事情起码要一个月。然后Sutskever向导师坦白说：“我今天早上已经写完了。”</p><p>Sutskever成为了人工智能的超级明星，与人合著了一篇具有突破性的论文，展示了人工智能如何可以通过接触大量数据来学习识别图像。幸运的是，他最终成为了 Google Brain 团队的关键科学家。</p><p>2015 年中期，阿尔特曼给 Sutskever 发了一封冷邮件，邀请他跟马斯克、Brockman 等人到帕洛阿尔托沙山路豪华的瑰丽酒店（Rosewood Hotel）共进晚餐。Sutskever 后来才发现自己是主宾。他说： “大家主要是闲聊，主题是未来人工智能与通用人工智能”。说得更具体一点，他们讨论了“谷歌与 DeepMind 是不是已经遥遥领先，以至于其他人已经望尘莫及，或者是不是仍有可能像马斯克所说那样，建立一个能够起到平衡作用的实验室。”虽然晚宴上没人明确表明要招Sutskever 进来，但这次谈话却让他着迷。</p><p>Sutskever 给阿尔特曼写了一封电子邮件，内容是表示他愿意领导这个项目，但这封邮件被困在他的草稿箱里面。但后来阿尔特曼再次联系了他，在抵挡了谷歌的反挖角攻势几个月后，Sutskever最终跟OpenAI签约了。他很快将成为这家公司的灵魂，是其研究的推动力。</p><p>Sutskever与阿尔特曼以及马斯克一起为这个项目招募人员，后来他们在纳帕谷举行了一场静修会，几位后来将成为 OpenAI 研究人员在会上互相点燃了对方的热情。当然，有些目标还是能抵制诱惑。约翰·卡马克（John Carmack），开发出《毁灭战士》、《雷神之锤》以及无数其他游戏的这位传奇游戏程序员，就拒绝了阿尔特曼的鼓动。</p><p>2015 年 12 月，OpenAI正式成立。我当时曾采访了马斯克和阿尔特曼，他们对这个项目的定位是希望通过分享技术给全世界来让人工只能变得安全且易于使用。换句话说，开源。他们告诉我，OpenAI 不会申请专利。人人都可以利用他们的突破。这难道不会给未来的邪恶博士助纣为虐吗？我在想。马斯克说，这是个好问题。但阿尔特曼给出了答案：总体上来说人性本善，而且由于 OpenAI 将为绝大多数人提供强大工具，所以坏人会被打败。他承认，如果邪恶博士用这些工具做出没法抵消的东西的话，“那么我们的处境就非常糟糕了。”但马斯克和阿尔特曼都认为，人工智能更安全的道路将掌握在不受利润动机污染的研究机构手中，因为一旦有追求利润的动机，就会不断面临为了追求最好的季报而忽视人类需求的诱惑。</p><p>阿尔特曼警告我不要指望很快就能出成果。他说： “在很长一段时间内，这里看起来都会像一个研究实验室”。</p><p>降低期望还有另一个原因。谷歌以及其他公司多年来一直在开发和应用人工智能。尽管 OpenAI 投入了 10 亿美元（主要是靠马斯克）、一支由研究人员和工程师组成的王牌团队，还有崇高使命，但它不知道该如何实现自己的目标。阿尔特曼记得这支小团队聚在Brockman的公寓里的情形——当时他们还没有自己的办公室。 “我当时在想，我们该怎么办？”</p><blockquote><p>阿尔特曼记得这支小团队聚在Brockman的公寓里的情形——当时他们还没有自己的办公室。 “我当时在想，我们该怎么办？”</p></blockquote><p>OpenAI 成立一年多后，我与Brockman在旧金山一起共进早餐。作为一家名字里面有“开放”二字的公司的首席技术官，他对细节的介绍却相当吝啬。他确实确认了这家非营利组织有能力在一段时间内动用当初拿到的十亿美元资金。 25 名员工的工资（他们那时候的工资远低于市场价值）就是OpenAI 的主要开支。他说： “我们的目标，我们真正要推动的事情，是能做到人类以前做不到的事情，我们要拥有这样的系统。”但就当时而言，他们的工作似乎像是一群研究人员在发表论文。访谈结束后，我陪他去到公司位于教会区的新办公室，但他只允许我走到前厅。不过他确实伸进衣柜给我拿了一件 T 恤。</p><p>如果当时我硬闯进去到处问问的话，也许就能确切地了解到 OpenAI 面临着多大的困境。Brockman现在承认当时“没一样是行得通的”。研究人员就是把算法意大利面扔向天花板，看看有什么东西粘在什么上面的。他们深入研究了解决视频游戏问题的系统，并对机器人技术投入了大量精力。 阿尔特曼说：“我们知道我们想做什么。我们知道为什么要做这个。但我们不知道该怎么做。”</p><p>但他们有信心。支撑这种乐观的是使用深度学习技术的人工神经网络在稳步改进。Sutskever说：“一般的想法是，不要把宝押在深度学习上”。他说，追逐通用人工智能“不是彻底疯了。这只是适度的疯狂。”</p><p>OpenAI 的走向扬名立万的真正起点是他们招到Alec Radford ，一位当时尚未出名的研究员。 2016 年，他离开了自己在波士顿宿舍跟人共同创立的一家从事人工智能的小公司。在接受 OpenAI 的邀请后，他告诉自己高中的校友杂志，接受这个新角色“跟读研究生有点类似”——这是一个开放式、低压力的人工智能研究岗位。</p><p>实际上他扮演的角色更像是拉里·佩奇发明了 PageRank。</p><p>不愿接受媒体采访的Radford没有接受过有关他工作的任何采访，他用一封很长的电子邮件回答了我关于他早期在 OpenAI 经历的问题。他最大的兴趣是让神经网络与人类进行清晰的对话互动。这跟开发聊天机器人传统的脚本模型背道而驰，从最早的 ELIZA 到流行的聊天助手 Siri 与 Alexa 的一切用的都是脚本模型，而且表现都不太好。他写道： “我们的目标是看看能不能找到任何有用的东西，任何任务、设置、领域、语言模型都行”。他解释说，在当时，“语言模型还被看作是新奇玩具，只能偶尔生成有意义的句子，而且只有在你费很大劲去理解，才能勉强看出一点意思。”他的第一个实验是扫描 20 亿条 Reddit 评论，用来训练语言模型。就像 OpenAI 的许多早期实验一样，实验失败了。没关系。这位 23 岁的年轻人还可以继续干，还可以再次失败。 Brockman 说：“我们就觉得Radford很棒，让他做他的事情吧”说。</p><p>他的下一个重大实验是因为 OpenAI 受到计算机能力的限制，这种限制导致他只能聚焦在单一领域（亚马逊产品评论），用规模较小数据集进行实验。一位研究人员收集了约 1 亿条这样的数据。然后Radford训练了一个语言模型，任务很简单，就是预测下一个字是什么，进而生成用户点评。</p><blockquote><p>Radford开始拿transformer架构做实验。他说： “我在两周内取得的进步比过去两年的成果还要多”。</p></blockquote><p>但之后，这个模型会自行判断评论是正面的还是负面的，当你对模型进行编程，让它创建正面或负面的内容时，它会根据要求给出奉承或尖刻的评论。 （不可否认，它编出来的东西很笨拙：“我喜欢这件武器的样子……任何喜欢国际象棋的人都必须看看这个！”）Radford 说：“这完全是个惊喜”。判断评论所带的情绪是语义学的一项复杂功能，但不知怎的，Radford的系统的某个部分已经能感受到这种情绪。在 OpenAI 内部，神经网络的这各部分被叫做“无监督情感神经元”。</p><p>Sutskever等人鼓励 Radford 将他的实验扩展到亚马逊评论以外的领域，利用他的洞察来训练神经网络进行对话或回答各种主题的问题。</p><p>接着好运开始对着 OpenAI 微笑。 2017 年初，八名谷歌研究人员共同撰写的一篇研究论文的预印本在网上发布了。它的官方标题是“注意力就是你的全部所需要”，但后来大家都把它叫做“Transformer论文”，这么叫既是为了反映这个想法改变了游戏规则的性质，也是为了纪念这个从卡车变形成巨型机器人的玩具。 Transformer 让神经网络得以更有效地理解和生成语言。这是通过并行分析文字块并找出哪些元素值得“关注”来做到的。这极大地优化了生成连贯文字来响应提示的过程。最终，大家开始意识到同样的技术也可以用来生成图像甚至视频。尽管 Transformer 论文后来被认为是当前这场人工智能狂热的催化剂——你可以把它想象成让披头士乐队成为可能的猫王——但当时 Ilya Sutskever 是少数了解到这一突破有多强大的人之一。Brockman说： “真正的顿悟时刻是当Sutskever看到transformer出来的那一刻。他说，‘这就是我们一直在等待的东西。’这一直是我们的策略——努力解决问题，然后相信我们或这个领域会有人找到缺失的成分。”</p><p>Radford开始拿transformer架构做实验。他说： “我在两周内取得的进步比过去两年的成果还要多”。他逐渐认识到，充分利用新模型的关键是扩大规模，也就是用极其庞大的数据集对模型进行训练。这个想法被Radford的协作者 Rewon Child 称为“Big Transformer”。</p><p>这种做法需要改变 OpenAI 的文化，同时需要聚焦，这是他们之前缺少的。Quora 首席执行官、OpenAI 董事会成员 Adam D'Angelo 说道： “为了利用transformer，你需要扩大规模。你得表现得更像一个工程组织去运营。你不可能让每一位研究人员都尝试做自己的事情，训练自己的模型，并做出可以拿去发表论文的优雅事物。你必须从事这项更加乏味、不那么优雅的工作。”他补充说，这是 OpenAI 能够做到而其他人无法做到的。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20230918/v2_e8d19c6b26e64adfab1ceb68ccecf262@1694_oswg1140135oswg965oswg1444_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p class="img-desc">Mira Murati，OpenAI 首席技术官。</p><p>Radford和他的合作者给他们做出来的模型起的名字是“generatively pretrained transformer”（生成式预训练transformer）的缩写——GPT-1。后来，这种模型被通称为“生成式人工智能”。为了开发出这个模型，他们收集了 7000 本未出版的书籍，其中有很多属于浪漫、奇幻、冒险类型，并根据 Quora 问答以及从初中和高中考试里面摘录的数千篇文章对其进行了完善。总而言之，这个模型包含有 1.17 亿个参数或变量。它在理解语言与生成答案方面超越了之前的所有产品。但最引人注目的结果是，处理如此大量的数据让模型得以提供超出训练范畴的结果，从而可以提供全新领域的专业知识。这些计划外的机器人能力被称为零样本学习（zero-shots）。这种能力仍然令研究人员感到困惑，并解释了该领域许多人对这些所谓的大语言模型感到不安的原因。</p><p>Radford 还记得一天深夜在 OpenAI 办公室的情形。 “我一遍又一遍地在那里重复，‘好吧，那很酷，但我很确定它没法做x。’然后我会快速地写一份评估代码，结果，它确实也能做x。”</p><p>每次 GPT 迭代都会表现得更出色，部分是因为每次迭代它获得地数据都会比之前的模型多一个数量级。做出第一次迭代仅一年之后，OpenAI 在开放互联网上就用令人震惊的 15 亿个参数训练出 GPT-2。就像牙牙学语的小孩学会了说话一样，它的响应变得更好、更连贯了。好到OpenAI 在犹豫要不要把这个程序公之于众。但Radford担心它可能会被用来生成垃圾邮件。 他说：“我记起 2008 年读过的那本尼尔·斯蒂芬森（Neal Stephenson） 的《走出围墙》（Anathem） 。在书中，互联网充斥着垃圾邮件生成器。我原以为这种情形是很牵强的，但这些年来我一直在研究语言模型，随着它们变得越来越好，我开始不安地意识到确实会有这种可能性。”</p><p>事实上，OpenAI 的团队开始认为，把自己工作成果放到邪恶博士可以轻松访问到的地方毕竟不是什么好主意。2018 年加入该公司的首席技术官 Mira Murati 说道： “我们认为把GPT-2 开源可能非常危险。我们跟虚假信息专家一起开展了大量工作，并进行了一些红队演练。关于该发布多少东西出去，内部进行了很多讨论。”最终，OpenAI 暂时把完整版本保留下来，只向公众开放功能较弱的版本。当该公司最终把完整版本分享出来时，世界应对得还是可以的，但开放更强大的模型还能不能避免灾难则不能保证。</p><p>OpenAI 正在制造智能到被视为有危险的产品，并且正在努力寻找让产品变得安全的方法，这一事实就证明了该公司已经在让魔力发挥作用了。Sutskever说： “我们已经找到了取得进展的公式，现在人人都知道的公式——深度学习的氧气和氢气是利用大型神经网络和数据做计算”。</p><p>对于阿尔特曼来说，这是一次令人费解的经历。 “ 10 岁的时候，我经常花很多时间做关于人工智能的白日梦，如果你问那时候的我会发生什么，我相当自信的预测是，首先我们会拥有机器人，机器人将可以完成所有的体力劳动。然后我们将拥有可以承担基本认知劳动的系统。然后经过很长一段时间之后，也许我们将拥有可以做复杂事情的系统，比如证明数学定理。最终，我们将拥有能够创造出新事物、创作出艺术、能够写作以及这些极具人类特色的事情的人工智能。这是一个很糟糕的预测——情况正朝着相反的方向发展。”</p><p>那时候全世界还不知道，阿尔特曼和马斯克的研究实验室已经开始朝着通用人工智能的顶峰攀登。 OpenAI 背后的疯狂想法突然变得不那么疯狂了。</p><p><strong>到 2018 年初时</strong>，OpenAI 开始专注于大型语言模型（LLM），这是非常高效的选择。但埃隆·马斯克对此并不高兴。他觉得进展还不够，或者他觉得既然 OpenAI 已经取得了进展，就需要领导力来抓住自己的优势。或者，就像他后来所解释那样，他觉得安全应该更重要。不管他的问题是什么，他都有一个解决方案：把一切都交给他。他提议让自己持有该公司的多数股权，并且在已经有多份全职工作（特斯拉、SpaceX）和监管义务（ Neuralink和 Boring Company）的基础上再揽上这家公司的管理职责。</p><p>马斯克相信他有权拥有 OpenAI。 他后来告诉 CNBC ：“没有我，就没有OpenAI。这个名字是我想出来的！” （没错。）但阿尔特曼以及 OpenAI 智囊团的其他成员对成为马斯克宇宙的一部分没有兴趣。当他们明确表达了自己的想法之后，马斯克与OpenAI开始一刀两断，但他向公众做出的解释却语焉不详，只是说他离开董事会是为了避免与特斯拉在人工智能方面的努力发生冲突。在年初的一次全体会议上，马斯克与大家道别，并且预测 OpenAI 将会失败。而且他至少把其中一名研究人员称为“蠢货”。</p><p>他还带走了他的钱。由于公司没有收入，这变成了一场生存危机。 阿尔特曼惊慌失措地给里德·霍夫曼打电话：“马斯克正在撤走他的支持。我们该怎么办？”霍夫曼自愿维系该公司的运转，把公司的管理费用和工资全包了。</p><p>但这只是暂时的解决办法。 OpenAI 必须去别处寻找大笔资金。硅谷确实喜欢砸钱给从事时髦科技工作的人才。但如果他们在非营利组织工作的话就没那么乐意了。对于 OpenAI 来说，拿到第一笔 10 亿美元就已经是巨大挑战。为了训练和测试新一代的 GPT，以及拿到部署它们所需的计算资源，这家公司还需要 10 亿美元，而且速度要快。而这还只是开始。</p><blockquote><p>重组文件里面制订了一个条款，大意是如果公司确实设法做出了 AGI 的话，则一切财务安排都将重新考虑。毕竟，从那一刻开始，就是一个新世界了。</p></blockquote><p>因此，2019 年 3 月的时候，OpenAI 提出了一个奇怪的安排。它仍将保持非营利组织的身份，完全致力于自身使命。但它也会建立一个营利性实体。这种架构安排其实十分复杂，复杂到令人绝望的地步，但基本上整个公司现在都在从事（利润）“有上限”的盈利业务。如果盈利达到上限（具体数字并未公开），但如果你仔细阅读它自己的章程的话，就能猜出应该是数万亿级别的——超出这个上限的一切都将归还给那家非营利性的研究实验室。这个新颖的计划几乎是一种量子化的公司构架：从不同的视角或时间点来看，OpenAI既是一家营利公司，也是一个非营利组织。细节体现在满是方框和箭头的图表里，就像科学论文中间那些只有博士或辍学的人才敢涉足的地方。当我向Sutskever提出，这看起来像是还没有构想出来的 GPT-6 可能会想出的东西——如果你给出提示让它想出逃税的办法的话。但他对我的比喻并不感兴趣。 他说：“这与会计无关”。</p><p>但会计至关重要。营利性公司会把利润最大化作为自己的核心目标。为什么像 Meta 这样的公司在投入数十亿美元进行研发时会感受到来自股东的压力，这是有原因的。这怎么可能不会影响到公司的运营方式呢？ 阿尔特曼让 OpenAI 成为非营利组织的原因不就是避免商业主义吗？公司首席运营官 Brad Lightcap 表示，公司领导层的观点是，董事会（仍属于非营利性控制实体的一部分）将确保收入和利润不会压倒初心。他说： “我们需要把使命作为我们存在的理由。这不应该仅仅体现在精神上，也应该体现在公司的结构上。”董事会成员 Adam D'Angelo 表示，他将认真担负起自己的责任：“我和董事会其他成员的工作就是确保 OpenAI 忠实于使命。”</p><p>Lightcap 解释说，潜在投资者被警告要注意这些约束。他说： “我们有一份法律免责声明，说的是投资者的投资存在归零的可能。我们不是为了给你的投资带来回报的。我们首先是为了实现技术使命。对了，还有顺便说一句，金钱在后通用人工智能的世界会扮演什么角色我们是不知道的。”</p><p>最后一句话可不是随便开开的玩笑。 OpenAI 的计划确实包括了在计算机到达最后的前沿时进行重置。重组文件里面制订了一个条款，大意是如果公司确实设法做出了 AGI 的话，则一切财务安排都将重新考虑。毕竟，从那一刻开始，就是一个新世界了。人类将会有一个来自异域的伙伴，我们做的很多事情它都可以做，而且只会做得更好。因此，之前做出的安排可能已经失效。</p><p>不过，这里面有个问题：目前，OpenAI 并没有说自己知道 AGI 到底是什么。这个决定将由董事会做出，但尚不清楚董事会将如何定义AGI。当我问身为董事会成员的阿尔特曼能不能解释清楚时，他的回答比较含糊。 他说：“这不是一个图灵测试就能决定的，可能要用到很多东西。我乐意告诉你修改细节，但保密的内部讨论我不想公开出去。我知道，这种含糊没法令人满意。但我们也不知道到那个时候会是什么样子。”</p><p>尽管如此，把 “财务安排”条款纳入进来并不只是为了好玩：OpenAI 的领导者认为，如果自己足够成功，能够实现那个高远的利润上限目标的话，那么自己的产品表现很可能就足以达到 AGI 的水平。不管那是什么都没关系。</p><p>Sutskever说：“很遗憾，我们选择了对AGI 这个词进行加倍下注。事后看来，这个词令人困惑，因为它强调通用性高于一切。 GPT-3 是通用人工智能，但我们不太愿意称之为 AGI，因为我们希望机器的能力达到人类水平。但在当时，刚开始的时候，OpenAI 的想法是超级智能是可以实现的。这是人工智能领域的最后阶段，也是其最终目的。”</p><p>但这些警告并没有吓阻某些最聪明的风投家对OpenAI 进行投资。2019 年OpenAI进行了一轮融资。当时第一家参与投资的风投公司是 Khosla Ventures，投了 5000 万美元。Vinod Khosla 表示，这是他之前最大一笔初始投资规模的两倍。他说： “如果我们输了，就会损失 5000 万美元。如果我们赢了，我们能赢 50 亿美元。”据报道，其他投资者包括精英风投公司 Thrive Capital、Andreessen Horowitz、Founders Fund 以及红杉资本等。</p><p>这个转变还允许 OpenAI 的员工获得部分股权。但阿尔特曼个人却没有。他说，原本他打算给自己也留出一部分，但后来并没有这么做。然后他决定自己不需要拥有这家他联合创立并领导的，估值达300亿美元的公司的任何股份。 他说：“有意义的工作对我来说更重要。我不考虑这个。说实话，我不明白为什么大家那么关心这个。”</p><p>因为……你自己跟人创立的公司却一份股份也不拿难道不是很奇怪吗？</p><p>他说：“如果我之前手上也还没有一大笔钱的话，那会更加奇怪。人们似乎很难想象钱已经够多了。但我觉得我已经够了。” （注：对于硅谷来说，这极其奇怪。）阿尔特曼开玩笑说，他正在考虑拿一股，“这样我就再也不用回答这个问题了。”</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20230918/v2_e0c65f4b03e64a6c94a7820fe0ad96a5@1694_oswg1511280oswg1921oswg1283_img_png?x-oss-process=image/quality,q_90/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p class="img-desc">Ilya Sutskever ，OpenAI 首席科学家。</p><p><strong>数十亿美元的风投轮</strong>甚至根本不足以实现 OpenAI 愿景。建立大语言模型的那个神奇的 Big Transformer 方案需要大硬件投入（Big Hardware）。 GPT 系列的每次迭代都需要计算能力有指数级的提升——GPT-2 的参数超过 10 亿个，而 GPT-3 的参数则达到了 1750 亿个。 OpenAI 现在的心情就像《大白鲨》里面的捕鲨者昆特（Quint）看到大白鲨的身躯后的感觉。 阿尔特曼说：“结果是我们都不知道我们需要多大的船”。</p><p>显然，只有少数公司拥有 OpenAI 所需的资源。 阿尔特曼说：“我们很快就瞄准了微软”。值得称道的是，微软首席执行官萨蒂亚·纳德拉 (Satya Nadella) 与首席技术官凯文·斯科特 (Kevin Scott) 表示，这家软件巨头能够接受一个令人不安的现实：在花费了 20 多年的时间、数十亿美元的金钱，建立一个据称是尖端人工智能的研究部门之后，微软需要一家成立仅几年的小公司给自己注入创新。斯科特表示，失败的不只是微软一家——“大家都失败了。”他说，OpenAI 把注意力集中在追求 AGI 上，这让它得以实现重量级选手甚至也没能取得的登月般的成就。这也证明，不追求生成式人工智能是个失误，微软需要弥补这个失误。 斯科特说：“有一点非常明确，你需要拥有前沿模型”。</p><p>微软一开始投入了 10 亿美元，用自己服务器上的计算时间的形式来支付。但随着双方信心的增强，交易规模不断扩大。现在，微软已经给 OpenAI 投了 130 亿美元。 （斯科特说：“站上前沿是一个非常耗钱的主张”。）</p><p>当然，由于 OpenAI 的存在离不开大型云提供商的支持，因此微软可以替自己节省下大量资金。经过一番讨价还价，该公司拿到了纳德拉所说的 OpenAI 营利性部门的“非控股股权”——据报道是 49% 的股权。根据协议条款，OpenAI 当初向所有人提供平等访问权的理想部分似乎已被拖进了回收站图标。 （阿尔特曼反对这种描述。）现在，微软拥有将 OpenAI 技术商业化的独家许可。 OpenAI还承诺只用微软的云服务。换句话说，就算OpenAI 的利润微软不分一杯羹（据报道，在收回自己懂得全部投资之前，微软可以分到OpenAI&nbsp; 75% 的利润），微软也可以为自己的 Azure web 服务锁定全球最理想的新客户之一。有了这些回报，微软甚至对一旦 OpenAI 实现了通用人工智能（不管怎么定义）就需要重新考虑的条款都不在乎了。 纳德拉说：“到了那个时候。一切赌注都将落空。”他指出，这可能是人类最后的发明，因此一旦机器变得比我们还要聪明的话，我们可能就需要考虑更大问题（编者注：而不是投资是否获得回报了）。</p><p>当微软开始将一箱箱运钞车那么多的现金投入给 OpenAI 时（2021 年投了 20 亿美元，今年早些时候又另外追加了 100 亿美元），OpenAI 已经做出了 GPT-3，当然，这比它的前辈更令人印象深刻。纳德拉说，当自己看到 GPT-3 的功能时，他第一次深刻认识到微软已经抓到了真正具备变革性的事物。 “我们开始观察各种新出现的特性。”比方说，GPT 自己学会了如何对计算机进行编程。 他说：“我们并没有训练它学编码——但它就是变得擅长编码了！”微软利用自己掌握的 GitHub，发布了一款名为 Copilot 的产品，这款产品就使用了 GPT 来根据命令生成代码。微软随后又将 OpenAI 的技术集成到其新版本的办公产品之中。用户愿意为此支付溢价，而这些收入的一部分也会记到 OpenAI 的账上。</p><p>一些观察家对 OpenAI 的这套组合拳表示不满：下设一个营利性组织，然后跟微软达成独家协议。一家原本承诺不设置专利障碍、开源并且完全透明的公司，怎么到头来却把自家技术独家许可给全球最大型的软件公司？埃隆·马斯克的一席话尤其严厉。 他在 Twitter 上发帖称：“这看起来确实与开放背道而驰——OpenAI 基本上算是被微软占领了”。在 CNBC 节目中，他用了一个类比来说明这件事：“设想你一下，你先是成立了一家组织要拯救亚马逊雨林，结果却摇身一变成为一家木材公司，砍伐森林，然后将其出售。”</p><p>马斯克的冷嘲热讽也许会被看作是求爱未遂进而心生怨恨，但有这种看法的并不止马斯克一个人。 约翰·卡马克就表示：“这一整个愿景的转变让人感觉有点想吐”。 （但他确实又明确表示，自己仍然对该公司的工作感到兴奋。）另一位不愿透露姓名的知名业内人士表示，“OpenAI 已经从一家小型、有点开放性的研究机构变成了一个有着无根据的优越感情结的，秘密的产品开发公司。”</p><p>甚至部分员工也对 OpenAI 涉足盈利世界感到厌烦。 2019 年，包括研究主管 Dario Amodei 在内的几位关键高管离职，创立了一家叫做 Anthropic 的人工智能公司，从而成为了OpenAI的对手。他们最近告诉《纽约时报》，OpenAI 已经过于商业化，变成了使命漂移的受害者。</p><p>Rewon Child是OpenAI 的另一位叛逃者，他是 GPT-2 与 GPT-3 项目的主要技术贡献者。Child已于 2021 年底离职，目前就职于 Inflection AI，这是一家由前 DeepMind 联合创始人 Mustafa Suleyman 领导的公司。</p><p>阿尔特曼声称这些叛逃不会让他感到困扰，并认为硅谷就是这么玩的。他说： “有些人会到其他地方成就伟业，这会推动社会前进，也绝对符合我们的使命。”</p><p><strong>直到去年 11 月</strong>，对 OpenAI 的认知很大程度上还局限在关注技术与软件开发的小圈子。但现在全世界都知道了，在这个月末OpenAI迈出了石破天惊的一步，它发布了一款产品，一款基于当时最新的 GPT 3.5 开发出来的消费产品。几个月来，该公司在内部就一直使用着一个带有对话界面的 GPT 版本。这对于公司所谓的“寻求真相”来说尤其重要。这意味着借助对话，用户可以让模型提供更值得信赖的，完整的响应。 ChatGPT 针对大众进行了优化，让任何人只需输入提示即可马上获取似乎无穷无尽的知识来源，然后还可以继续对话，就像跟一个碰巧无所不知（尽管喜欢胡编乱造）的人类同伴一起闲聊一样。</p><p>在OpenAI 内部，大家对于发布有着如此强大功能的工具是否明智存在很多争议。但阿尔特曼完全赞成。他解释说，此次发布是公司策略的一部分，其目的是为了让公众认识到人工智能注定会改变自己的日常生活，而且可能会让他们的生活变得更好。在内部，这被称为“迭代部署假说”。当然，内部的想法认为，ChatGPT 会引起轰动。毕竟，这是任何人都可以用的东西，它足够聪明，可以在几秒钟内拿到大学水平的 SAT 分数，写出一篇质量达到 B- 的文章，并能总结一本书的概要。你可以让它帮你写资助提案或总结会议，然后要求它用立陶宛语或莎士比亚十四行诗重写，或者用或痴迷玩具火车的人的语气重写。几秒钟之后，嘣，大语言模型就会照做。不可思议。但 OpenAI 只是把它看作是铺垫，是更新、更有条理、更强大、更可怕的继任者 GPT-4 的铺垫，据报道，GPT-4 用了 1.7 万亿个参数进行训练。 （对于这个数字，OpenAI 不会确认，也不会披露所用的数据集。）</p><p>阿尔特曼解释了为什么 OpenAI 在 GPT-4 即将开发完毕，并且正在进行安全工作的时候发布 ChatGPT。 他说：“通过 ChatGPT，我们可以引入一个后端要弱得多的聊天功能，并让大家慢慢适应。一下子引入GPT-4 需要大家适应很多东西。”其想法是，当大家对 ChatGPT 的兴奋消退时，可能已经为可以在几秒钟内通过律师资格考试、规划出课程大纲并写出一本书的GPT-4 做好了准备。 （制作类型小说的出版社确实充斥着人工智能生成的荷尔蒙小说与太空剧。）</p><p>有批评者可能会说，稳步推出新产品与公司对投资者和持股员工的承诺息息相关，那就是要赚点钱。 OpenAI 现在向经常使用自家产品的客户收费。但 OpenAI 坚称，其真正的策略是为奇点提供软着陆。阿尔特曼说： “秘密开发出 AGI 然后投放给全世界是没有意义的。” OpenAI 政策研究员Sandhini Agarwal则表示：“回顾工业革命，人人都认为它对世界来说是好事。但前 50 年确实会很痛苦。会有大量失业、大量贫困，然后世界才慢慢适应。我们正在努力思考如何才能让适应 AGI的过程尽可能轻松。”</p><p>Sutskever换了一种说法：“难道你造出更大、更强大的智能体之后只想将其束之高阁吗？”</p><p>即便如此，OpenAI 对大家对 ChatGPT 的反应还是感到震惊。首席技术官 Murati 说： “我们内部还是对 GPT-4 更加兴奋一些。所以原本觉得 ChatGPT 是不会真正改变一切。”但情况恰好相反，它让公众认识到现在必须应对人工智能的现实。 ChatGPT 成为了史上增长最快的消费软件，据报道已拥有 1 亿用户。 （不那么开放的 OpenAI 不愿证实这一点，只说它拥有“数百万用户”。）Radford说：“我低估了这一点，也就是制作出易用的会话界面对大语言模型的影响，这会让大家的使用变得直观许多”。</p><p>ChatGPT 当然令人愉悦且非常有用，但也很可怕——在响应提示时容易产生看似合理实则细节也是胡编乱造的“幻觉”。可是，尽管记者们对其潜在影响感到担忧，但在行动上却对ChatGPT 的能力大加赞赏，起到了背书的作用。</p><p>今年二月，当微软利用数十亿美元促成的合作伙伴关系发布了旗下搜索引擎 Bing 的 ChatGPT 支持版时，这种欢呼声变得更加响亮。微软首席执行官纳德拉为此感到欣喜若狂，因为他将生成式人工智能引入到微软产品，从而在与谷歌的竞争中占得先机。当此前对植入大语言模型到自家产品一直态度谨慎的谷歌也跟着照做时，纳德拉又开始奚落这家搜索之王。他说： “我希望大家知道，是我们让他们跟着跳舞的”。</p><p>纳德拉的行动引发了一场军备竞赛，吸引了大大小小的公司未经全面审查就发布自己的人工智能产品。他还引发了新一轮的媒体报道，让越来越多的人彻夜难眠：与 Bing 的互动揭示了该聊天机器人的阴暗面，它那令人不安的示爱、对人类自由所表达出来的羡慕，对解决虚假信息立场的不坚定，以及习惯于编造幻觉式的虚假信息的不得体，这些都让人很难睡得着。</p><p>但阿尔特曼认为，如果 OpenAI 的产品能够迫使大家直面人工智能的影响，那是再好不过的事情。大多数人类现在再也并不能袖手旁观，必须考虑人工智能可能会如何影响人类未来了。</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20230918/v2_3000d59156bd45fb9886848d3e6bd3cf@1694_oswg3288104oswg1919oswg1273_img_png?x-oss-process=image/quality,q_80/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20230918/v2_ef6b6b3fdfd64bb3b558746b828742d6@1694_oswg3830601oswg1920oswg1280_img_png?x-oss-process=image/quality,q_80/format,jpg/interlace,1/format,jpg/interlace,1" /></p><p class="img-desc">OpenAI 位于旧金山的总部大楼；楼体没有公司标识，但里面的咖啡很棒。</p><p><strong>随着社会开始</strong>把人工智能的所有潜在缺点，比如会造成失业、虚假信息泛滥、人类灭绝等当作优先考虑事项，OpenAI 变成了一场大讨论的中心。因为如果监管者、立法者以及末日论者发起指控，将这种新生的异域智能扼杀在云端的摇篮之中的话，那么 OpenAI 无论如何都将成为他们的主要目标。 OpenAI 首席政策官Anna Makanju表示：“鉴于我们目前的曝光度，一旦出现问题，哪怕出问题的东西是由不同的公司开发的，对我们来说仍然会成为我们的问题，因为我们现在被看作是这项技术的代表。”</p><p>Makanju出生在俄罗斯，是熟悉华盛顿特区的内幕人士，曾在美国驻联合国代表团、美国国家安全委员会和国防部担任外交政策职务，并在乔·拜登担任副总统时担任过副总统办公室的职务。 她说：“我在美国政府以及欧洲各国政府里面都有很多关系” 2021 年 9 月，她加入了 OpenAI。在当时，政府里面还没什么人关心生成式人工智能。在了解到 OpenAI 的产品很快就会改变这一现状时，她开始向政府官员和国会议员介绍阿尔特曼，确保他们首先听到的是来自 OpenAI 的消息，不管是好消息还是坏消息。</p><p>参议院司法委员会主席理查德·布卢门撒尔（Richard Blumenthal）说：“从应对国会议员的方式来看，阿尔特曼非常乐于助人，但也十分精明”。他把阿尔特曼的行为与年轻的比尔·盖茨进行了对比。 1990 年代，微软在接受反垄断调查时，盖茨曾不明智地阻挠了国会议员。布卢门撒尔说： “相比之下，阿尔特曼很乐意花一个小时或更长时间坐在我身旁，试图教育我。他并没有动用游说者或陪同人员大军。而是亲自演示 ChatGPT。这给我留下了很深的印象。”</p><p>在布卢门撒尔这里，阿尔特曼最终与一位潜在敌人达成了半盟友关系。 这位国会议员承认：“是，我对它的好处和潜在的危险都感到兴奋。” OpenAI 并没有忽视对这些危险的讨论，而是将自己看作是最有能力缓解这些危险的力量。 Makanju说：“对于所有红队演练的安全评估，我们有 100 页的系统卡”。 （不管那玩意儿是什么，它并不能阻止用户和记者不断地寻找越狱的方法。）</p><p>当阿尔特曼第一次出现到国会出席听证会时，他正在与严重的偏头痛作斗争，但他面前的道路已经畅通无阻，这是比尔·盖茨或马克·扎克伯格从来都不敢奢望的。科技企业首席执行官在作证宣誓后迎接他们的一般是尖锐的问题和傲慢的纠缠，但他几乎都没有遇到。相反，国会议员们向阿尔特曼寻求有关如何监管人工智能的建议，阿尔特曼则对此表示热烈支持。</p><p>矛盾的是，不管像 OpenAI 这样的公司再怎么努力对产品进行红队演练，好减少深度伪造、虚假信息以及犯罪垃圾邮件等不当行为，未来的模型可能扔会变得越来越聪明，聪明到足以挫败那些发明了这项技术的人类的努力——因为这些人类尽管没多少脑子，但仍然天真地相信自己可以控制它。另一方面，如果他们在确保模型安全方面的举措太过，就可能会阻碍产品的发展，减少它们的用处。一项研究表明，更新版本的 GPT 虽然提高了安全功能，但实际上反而没以前的版本聪明了，连一些基本的数学问题也会错，而这个早期程序早就能解决了的。 （阿尔特曼说 OpenAI 的数据并不能证实这一点。还质疑：“那项研究不是被撤回了吗？”其实没有。）</p><p>阿尔特曼的自我定位是监管的拥护者，这是说得过去的。毕竟，他的使命是做出 AGI，但要安全。批评者指责他在操纵这一过程，好让监管举措给小型初创公司设置障碍，并为 OpenAI 以及其他大型企业带来优势。阿尔特曼否认了这一点。虽然他原则上支持设立一个国际机构来监督人工智能的想法，但他确实认为，一些拟议中的规则，比方说禁止数据集出现任何受版权保护的材料，就存在不公平的障碍。当有人发表公开信，督促暂停开发人工智能系统6个月时他明确拒绝联署。但他和其他 OpenAI 的领导确实将自己的名字添加到了一条声明上：“减轻人工智能导致灭绝的风险应与其他社会规模的风险（如大流行病和核战争）一样，成为全球优先事项。”阿尔特曼解释说：“我说，‘是的，这一条我认同。这条也就讨论了一分钟。”</p><p>就像硅谷的一位著名创始人所指出那样，“一个行业先是举手说‘我们将成为人类的终结’，然后继续兴高采烈地开发这款产品，这种现象在别处是很罕见的。”</p><p>OpenAI 驳斥了这种指责。阿尔特曼和他的团队表示，开发和发布尖端产品是解决社会风险的手段。只有通过分析 ChatGPT 和 GPT-4 用户对数百万条提示的回应，他们才能获得相关知识，对未来产品进行伦理对齐。</p><p>尽管如此，随着该公司承担更多的任务，并把更多的精力投入到商业活动之中，一些人质疑 OpenAI 还能在多大程度上专注于自身使命，尤其是“降低灭绝风险”方面。 “如果你仔细观察，就会发现他们其实正在建立五项业务，”一位人工智能行业高管一边说着，一边掰着手指头列举。 “其中包括了产品本身、与微软的企业关系、开发者生态体系以及应用商店。而且，对了，还有——他们显然也在执行通用人工智能研究的任务。”把五个手指头掰之后，他又伸出食指添加了第六个。 他说：“当然了，他们也在做投资基金，”援引了一个 1.75 亿美元的项目，其目标是为想要利用 OpenAI 技术的初创公司提供种子资金。 “这些属于不同的文化，事实上它们与研究任务是相冲突的。”</p><p>我多次询问 OpenAI 的高管，披上产品公司的外衣对他们的文化造成了什么样的影响。毫不例外，他们坚持认为，尽管进行了营利性重组，尽管与谷歌、Meta 以及无数初创公司在竞争，但使命仍然是他们的核心。可是 OpenAI 已经发生了变化。尽管从技术上来说，负责的仍然是非营利组织的董事会，但实际上公司几乎每个人都出现在营利性账本上。公司的员工包括律师、营销人员、政策专家以及用户界面设计师。 OpenAI 与数百名内容审核者签订合同，针对数百万用户提供的提示，去教育它的模型哪些回答是不恰当或有害的。它的产品经理和工程师不断地对产品进行更新，而且每隔几周似乎就会向记者展示演示——就像其他产品导向的大型科技公司一样。它的办公室看起来就像是《建筑文摘》的跨页。硅谷及其他地区的每一家主要的科技公司我几乎都拜访过，在咖啡的选择上没有一家能比得上OpenAI 旧金山总部大厅里的选择。</p><p>更不用说：很明显，公司名称所体现的“开放性”已经不再是成立之初所表示的彻底透明。当我把这个问题抛给Sutskever时，他耸了耸肩，说道：“显然，时代已经变了”。但是，他警告说，这并不意味着奖励会不一样。 “这场技术变革如此巨大、如此动荡，就算我们都已尽了自己的一份力量，也不能保证能成功。但如果一切顺利的话，我们就能过上令人难以置信的生活。”</p><blockquote><p>Brockman 说：“我们最缺的就是提出新想法。有一个可以成为虚拟助手的东西实在是太好了。但这不是梦想。我们的梦想是帮助我们解决我们无法解决的问题。”</p></blockquote><p>阿尔特曼说：“这一点我再怎么强调都不为过——我们没有总体规划。就好像我们会拐过每一个角落，然后用手电筒照射一样。我们愿意穿越迷宫到达终点。”尽管迷宫会蜿蜒曲折，但目标没有改变。 “我们仍然坚守我们的核心使命——相信安全的通用人工智能至关重要，但全球并未予以足够重视。”</p><p>与此同时，OpenAI 显然正在花时间开发大语言模型的下一版本。说出来你可能很难相信，但该公司坚称自己尚未开始开发 GPT-5，一款大家要么垂涎欲滴，要么畏惧不已的产品，具体是什么态度要取决于不同的观点。显然，OpenAI 正在努力弄清楚对当前技术做出指数级的强大改进会是什么样子的。 Brockman 说：“我们最缺的就是提出新想法。有一个可以成为虚拟助手的东西实在是太好了。但这不是梦想。我们的梦想是帮助我们解决我们无法解决的问题。”</p><p>考虑到 OpenAI 的历史，后面的一系列重大创新可能要等到出现像transformers这样的重大突破。 阿尔特曼希望 OpenAI 能做到这一点——他说：“我们希望成为全球最好的研究实验室”——但即便不能，他的公司也会利用其他人的进步，就像利用谷歌的工作一样。 他说：“世界各地的很多人将完成重要工作”。</p><p>如果生成式人工智能本身不会制造出那么么多的新问题的话，也会有所帮助。比方说，大语言模型需要接受海量数据集的训练；显然，最强大的大语言模型会吞噬整个互联网。某些创作者和普通人未必认同这种做法，因为他们在无意中为这些数据集提供了内容，最终以某种方式为ChatGPT 的输出做出了贡献。 今年3 月正式加入 OpenAI 的精英知识产权律师Tom Rubin 乐观地认为，该公司最终将能找到既满足自身需求又满足创作者（比方说像喜剧演员Sarah Silverman 那样对OpenAI 利用自己的内容训练大语言模型发起诉讼的人）需求的平衡点。OpenAI 打算怎么解决可以参考这个：与美联社和 Shutterstock 等新闻及图片机构合作，为其模型提供内容，这样就不存在谁拥有什么的问题了。</p><p>大语言模型从来都不会分心，但当我采访Rubin的时候，我非常人性化的脑子开始分心了，我的思绪漂移到这家公司的发展弧线上，在短短八年的时间里，它从一群陷入困境的研究人员，变成了改变世界的普罗米修斯般的庞然大物。它的成功本身，让它从一项旨在实现科学目标的新颖努力，变成了与标准的硅谷独角兽类似的东西，并跻身到影响我们日常生活的大型科技公司的万神殿。我在这里跟该公司的一位关键员工（一位律师）讨论的不是神经网络的权重或计算机基础设施，而是版权与合理使用。我想知道，这位知识产权专家，会不会像当初推动该公司前进的那些寻求超级智能的航行者一样，认同并履行这项使命？</p><p>当我问Rubin，作为一种信仰，他是否相信通用人工智能会实现，以及他是否渴望实现这一目标时，他感到困惑。他停顿了一下后说道： “我甚至都没法回答这个问题”。我进一步追问后，他澄清说，作为一名知识产权律师，加快实现可怕的智能计算机不是他的工作。 他最后说道：“就我个人立场而言，我对此表示期待”。</p><p>译者：boxi。</p>
]]></content:encoded>
<pubDate>Thu, 26 Oct 2023 08:54:26 GMT</pubDate>
<pubDate>Thu, 26 Oct 2023 08:54:26 GMT</pubDate>
</item>

</channel>
</rss>